Transform: AWS::Serverless-2016-10-31
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:

    - Label:
        default: Pipeline Configuration
      Parameters:
      - AutoUpdateHelmChart

    # - Label:
    #     default: Pipeline Resource Configuration
    #   Parameters:
    #   - MaxCpuToUse
    #   - MaxRamToUse
    #   - SchemaResourcePercentage
    #   - AggregatorResourcePercentage
    #   - ClusterModelResourcePercentage
    #   - FlowModelResourcePercentage
    #   - ClusterOutResourcePercentage
    #   - FlowOutResourcePercentage

    # - Label:
    #     default: Aggregator Configuration
    #   Parameters:
    #   - AggNumWorkers
    #   - AggMemoryLimitInGb
    #   - AggBlockSize
    #   - ChunkSize

    # - Label:
    #     default: Pipeline Configuration
    #   Parameters:
    #   - MaxAlertsForUiUpload
    #   - SequencerContextWindowSize

Parameters:

  AutoUpdateHelmChart:
    Type: String
    Default: 'true'
    Description: Whether to auto update the helm chart
    AllowedValues:
    - 'true'
    - 'false'

  # MaxCpuToUse:
  #   Description: Max CPU to use for the pipeline tasks.
  #   Type: Number
  #   Default: 80
  #   MinValue: 80

  # MaxRamToUse:
  #   Description: Max RAM to use for the pipeline tasks.
  #   Type: Number
  #   Default: 480
  #   MinValue: 480

  # SchemaResourcePercentage:
  #   Description: Schema resource percentage.
  #   Type: Number
  #   Default: 25
  #   MinValue: 10

  # AggregatorResourcePercentage:
  #   Description: Aggregator resource percentage.
  #   Type: Number
  #   Default: 30
  #   MinValue: 30

  # ClusterModelResourcePercentage:
  #   Description: Cluster model resource percentage.
  #   Type: Number
  #   Default: 40
  #   MinValue: 40

  # FlowModelResourcePercentage:
  #   Description: Flow model resource percentage.
  #   Type: Number
  #   Default: 40
  #   MinValue: 40

  # ClusterOutResourcePercentage:
  #   Description: Cluster out resource percentage.
  #   Type: Number
  #   Default: 10
  #   MinValue: 10

  # FlowOutResourcePercentage:
  #   Description: Flow out resource percentage.
  #   Type: Number
  #   Default: 10
  #   MinValue: 10

  # MaxAlertsForUiUpload:
  #   Description: Max alerts for UI upload. Any batch with more than this number of
  #     alerts will not be uploaded to the UI.
  #   Type: Number
  #   Default: 250000
  #   MinValue: 1

  # AggNumWorkers:
  #   Description: Number of workers for the aggregator.
  #   Type: Number
  #   Default: 2
  #   MinValue: 2

  # AggMemoryLimitInGb:
  #   Description: Memory limit for the aggregator in GB per worker.
  #   Type: Number
  #   Default: 3
  #   MinValue: 3

  # AggBlockSize:
  #   Description: Block size for the aggregator to read file in smaller chunks.
  #   Type: Number
  #   Default: 32
  #   MinValue: 10

  # ChunkSize:
  #   Description: Chunk size for the aggregator to split single batch into smaller
  #     chunks.
  #   Type: Number
  #   Default: 32
  #   MinValue: 10

  # SequencerContextWindowSize:
    Description: Context window size for the sequencer.
    Type: Number
    Default: 5000
    MinValue: 500

Mappings:
  Images:
    LambdaContainerImage:
      ImageUri: >-
        697204775395.dkr.ecr.us-east-1.amazonaws.com/map-dedupe
      Version: >-
        v0.10.2.1-simple-test
    LambdaAggContainerImage:
      ImageUri: >-
        582441423537.dkr.ecr.us-east-1.amazonaws.com/lambda-function-agg
      Version: >-
        v0.10.1.4-test
    AirflowContainerImage:
      ImageUri: >-
        697204775395.dkr.ecr.us-east-1.amazonaws.com/seshat-airflow
      Version: >-
        v0.10.2.1-simple-test
    ClusterModelPart1ContainerImage:
      ImageUri: >-
        582441423537.dkr.ecr.us-east-1.amazonaws.com/seshat-embedding-ecs
      Version: >-
        v0.10.1.5
    FlowModelContainerImage:
      ImageUri: >-
        582441423537.dkr.ecr.us-east-1.amazonaws.com/seshat-flow-ecs
      Version: >-
        v0.10.1.3
    WebContainerImage:
      ImageUri: >-
        582441423537.dkr.ecr.us-east-2.amazonaws.com/cytech
      Version: >-
        v0.10.1.6-test
    NginxContainerImage:
      ImageUri: >-
        582441423537.dkr.ecr.us-east-2.amazonaws.com/cytech-nginx
      Version: >-
        v0.10.1.2-test
    LogCollectionContainerImage:
      ImageUri: >-
        582441423537.dkr.ecr.us-east-1.amazonaws.com/seshat-log-collection
      Version: >-
        v0.10.2

  UI:
    ServiceWorkerUsername:
      Value: >-
        service_worker
    ServiceWorkerPassword:
      Value: >-
        service_worker_admin

    OTEL-EXPORTER-OTLP-ENDPOINT:
      Value: >-
        http://localhost:4317

  Lambda:
    UI-ENTITIES-TABLE-FILE-PATH:
      Value: >-
        scratch/user_node_feature.json

    NODE-FEATURE-FILE:
      Value: >-
        node_feature/node_feature.json

    CLUSTER-CONFIG-PREFIX:
      Value: >-
        clustering_agent/

    VOLUME-SERVICE-PLATFORM:
      Value: >-
        AWS

    ATTRIBUTE-WEIGHTS-FILE:
      Value: >-
        scratch/attribute_weights.json

    AIRFLOW-USERNAME:
      Value: >-
        cypienta

    AIRFLOW-PASSWORD:
      Value: >-
        cypienta

    CYPIENTA-NS:
      Value: >-
        simple


  LogCollection:
    WINDOW-NEXT-LOGS-SECONDS:
      Value: 120

    WINDOW-PREVIOUS-LOGS:
      Value: 3

    API-URL:
      Value: >-
        https://fxx7sy5frafew3zbzhkm2hrogy0pxxqh.lambda-url.us-east-1.on.aws

    OTEL-EXPORTER-OTLP-ENDPOINT:
      Value: >-
        http://localhost:5000

  ServicePrincipalPartitionMap:
    aws:
      EC2: ec2.amazonaws.com
      EKS: eks.amazonaws.com
      EKSFargatePods: eks-fargate-pods.amazonaws.com
      IRA: rolesanywhere.amazonaws.com
      SSM: ssm.amazonaws.com
    aws-cn:
      EC2: ec2.amazonaws.com.cn
      EKS: eks.amazonaws.com
      EKSFargatePods: eks-fargate-pods.amazonaws.com
    aws-iso:
      EC2: ec2.c2s.ic.gov
      EKS: eks.amazonaws.com
      EKSFargatePods: eks-fargate-pods.amazonaws.com
    aws-iso-b:
      EC2: ec2.sc2s.sgov.gov
      EKS: eks.amazonaws.com
      EKSFargatePods: eks-fargate-pods.amazonaws.com
    aws-iso-e:
      EC2: ec2.amazonaws.com
      EKS: eks.amazonaws.com
      EKSFargatePods: eks-fargate-pods.amazonaws.com
    aws-iso-f:
      EC2: ec2.amazonaws.com
      EKS: eks.amazonaws.com
      EKSFargatePods: eks-fargate-pods.amazonaws.com
    aws-us-gov:
      EC2: ec2.amazonaws.com
      EKS: eks.amazonaws.com
      EKSFargatePods: eks-fargate-pods.amazonaws.com
      IRA: rolesanywhere.amazonaws.com
      SSM: ssm.amazonaws.com

  RegionMap:
    af-south-1:
      lambdaLayer: arn:aws:lambda:af-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-3:336392948345:layer:AWSSDKPandas-Python311:12
    ap-south-1:
      lambdaLayer: arn:aws:lambda:ap-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ca-central-1:
      lambdaLayer: arn:aws:lambda:ca-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-central-1:
      lambdaLayer: arn:aws:lambda:eu-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-north-1:
      lambdaLayer: arn:aws:lambda:eu-north-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-1:
      lambdaLayer: arn:aws:lambda:eu-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-2:
      lambdaLayer: arn:aws:lambda:eu-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-3:
      lambdaLayer: arn:aws:lambda:eu-west-3:336392948345:layer:AWSSDKPandas-Python311:12
    sa-east-1:
      lambdaLayer: arn:aws:lambda:sa-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-1:
      lambdaLayer: arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-2:
      lambdaLayer: arn:aws:lambda:us-east-2:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-1:
      lambdaLayer: arn:aws:lambda:us-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-2:
      lambdaLayer: arn:aws:lambda:us-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-east-1:
      lambdaLayer: arn:aws:lambda:ap-east-1:839552336658:layer:AWSSDKPandas-Python311:14
    ap-south-2:
      lambdaLayer: arn:aws:lambda:ap-south-2:246107603503:layer:AWSSDKPandas-Python311:13
    ap-southeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-3:258944054355:layer:AWSSDKPandas-Python311:14
    ap-southeast-4:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-4:945386623051:layer:AWSSDKPandas-Python311:13
    eu-central-2:
      lambdaLayer: arn:aws:lambda:eu-central-2:956415814219:layer:AWSSDKPandas-Python311:13
    eu-south-1:
      lambdaLayer: arn:aws:lambda:eu-south-1:774444163449:layer:AWSSDKPandas-Python311:14
    eu-south-2:
      lambdaLayer: arn:aws:lambda:eu-south-2:982086096842:layer:AWSSDKPandas-Python311:13
    il-central-1:
      lambdaLayer: arn:aws:lambda:il-central-1:263840725265:layer:AWSSDKPandas-Python311:12
    me-central-1:
      lambdaLayer: arn:aws:lambda:me-central-1:593833071574:layer:AWSSDKPandas-Python311:12
    me-south-1:
      lambdaLayer: arn:aws:lambda:me-south-1:938046470361:layer:AWSSDKPandas-Python311:14
    cn-north-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-north-1:406640652441:layer:AWSSDKPandas-Python311:10
    cn-northwest-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-northwest-1:406640652441:layer:AWSSDKPandas-Python311:10

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub
      - cypienta-${StackSuffix}
      - StackSuffix:
          Fn::Select:
          - 2
          - Fn::Split:
            - /
            - !Ref AWS::StackId

  # Roles

  S3AccessPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action:
          - s3:ListBucket
          Resource: !GetAtt Bucket.Arn
        - Effect: Allow
          Action:
          - s3:GetObject
          - s3:PutObject
          - s3:DeleteObject
          Resource: !Sub "${Bucket.Arn}/*"
        - Effect: Allow
          Action:
          - elasticloadbalancing:DescribeLoadBalancers
          - aws-marketplace:RegisterUsage
          Resource: '*'

  EKSClusterRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName:
        Fn::Sub:
        - ${StackPrefix}-EKSClusterRole
        - StackPrefix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - Fn::FindInMap:
              - ServicePrincipalPartitionMap
              - Ref: AWS::Partition
              - EKS
          Action:
          - sts:AssumeRole
          - sts:TagSession
      ManagedPolicyArns:
      - Fn::Sub: arn:${AWS::Partition}:iam::aws:policy/AmazonEKSClusterPolicy
      - Fn::Sub: arn:${AWS::Partition}:iam::aws:policy/AmazonEKSVPCResourceController
      - Fn::Sub: arn:${AWS::Partition}:iam::aws:policy/AmazonEKSComputePolicy
      - Fn::Sub: arn:${AWS::Partition}:iam::aws:policy/AmazonEKSBlockStoragePolicy
      - Fn::Sub: arn:${AWS::Partition}:iam::aws:policy/AmazonEKSLoadBalancingPolicy
      - Fn::Sub: arn:${AWS::Partition}:iam::aws:policy/AmazonEKSNetworkingPolicy
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/ClusterRole

  EKSNodeRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName:
        Fn::Sub:
        - ${StackPrefix}-EKSNodeRole
        - StackPrefix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - Fn::FindInMap:
              - ServicePrincipalPartitionMap
              - Ref: AWS::Partition
              - EC2
          Action:
          - sts:AssumeRole
      ManagedPolicyArns:
      - Fn::Sub: arn:${AWS::Partition}:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly
      - Fn::Sub: arn:${AWS::Partition}:iam::aws:policy/AmazonEKSWorkerNodeMinimalPolicy

  EKSPodRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName:
        Fn::Sub:
        - ${StackPrefix}-EKSPodRole
        - StackPrefix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: pods.eks.amazonaws.com
          Action:
          - sts:AssumeRole
          - sts:TagSession
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonEFSCSIDriverPolicy
      - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      - !Ref S3AccessPolicy

  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName:
        Fn::Sub:
        - ${StackPrefix}-lambdaRole
        - StackPrefix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName:
          Fn::Sub:
          - ${StackPrefix}-AllowECS
          - StackPrefix:
              Fn::Select:
              - 0
              - Fn::Split:
                - '-'
                - Fn::Select:
                  - 2
                  - Fn::Split:
                    - /
                    - !Ref AWS::StackId
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - eks:*
            - cloudformation:*
            - elasticloadbalancing:DescribeLoadBalancers
            - ecr:*
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonS3FullAccess
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  # lambda function autoUpdateStack

  autoUpdateStack:
    Type: AWS::Serverless::Function
    DependsOn:
    - ControlPlane
    Properties:
      FunctionName: !Sub
      - ${StackPrefix}_auto_update_stack
      - StackPrefix:
          Fn::Select:
          - 0
          - Fn::Split:
            - '-'
            - Fn::Select:
              - 2
              - Fn::Split:
                - /
                - !Ref AWS::StackId
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: autoUpdateStack
      InlineCode: |
        import subprocess
        import sys
        import os
        import json
        import time
        import asyncio
        import secrets
        import tarfile
        import base64
        import urllib.request
        import hashlib
        import shutil

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        os.makedirs("/tmp/nltk_download", exist_ok=True)
        subprocess.call('pip install pyyaml ruamel.yaml eks-token boto3 botocore pyhelm3 kubernetes -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import boto3
        from botocore.exceptions import ClientError
        import requests
        import requests.adapters
        from requests.auth import HTTPBasicAuth

        import yaml
        from ruamel.yaml import YAML
        from eks_token import get_token
        from pyhelm3 import Client
        from kubernetes import client, config


        def download_helm_script(output_path):
            url = "https://get.helm.sh/helm-v3.17.3-linux-amd64.tar.gz"
            # url = "https://get.helm.sh/helm-v3.17.3-linux-386.tar.gz"
            with urllib.request.urlopen(url) as response, open(output_path, 'wb') as out_file:
                out_file.write(response.read())
            print(f"Downloaded Helm install script to {output_path}")


        # Usage
        helm_tar_path = "/tmp/helm-v3.17.3-linux-amd64.tar.gz"
        # helm_tar_path = "/tmp/helm-v3.17.3-linux-386.tar.gz"
        download_helm_script(helm_tar_path)

        HELM_INSTALL_DIR = "/tmp/helm"
        os.makedirs(HELM_INSTALL_DIR, exist_ok=True)
        env = os.environ.copy()
        env["HELM_INSTALL_DIR"] = HELM_INSTALL_DIR

        with tarfile.open(helm_tar_path, 'r:*') as tar:
            tar.extractall(path=HELM_INSTALL_DIR)

        HELM_EXECUTABLE = os.path.join(HELM_INSTALL_DIR, "linux-amd64/helm")


        class S3_service():
            '''
            S3 service class
            '''
            def __init__(self):
                '''
                Initialize the S3 service
                '''
                self.s3 = boto3.client('s3')
                self.retry_limit = 3
                self.retry_delay = 10

            def check_file_exists(self, bucket, key):
                '''
                Check if the file exists in the S3 bucket
                Parameters:
                    bucket (str): The name of the S3 bucket
                    key (str): The key of the file in the S3 bucket
                Returns:
                    bool: True if the file exists, False otherwise
                '''
                # check if the function parameters are valid
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(key, str):
                    raise ValueError("key must be a string")

                failure = False
                exception = None
                file_exists = False

                for i in range(self.retry_limit):
                    try:
                        try:
                            self.s3.head_object(Bucket=bucket, Key=key)
                            file_exists = True
                        except ClientError as e:
                            if e.response['Error']['Code'] != '404':
                                raise e

                            print(f"File does not exist: {key}")
                            file_exists = False

                        failure = False
                        break
                    except Exception as e:
                        print(f"Error downloading file from S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

                if file_exists:
                    return True

            def upload_file(self, file_path, bucket, key):
                '''
                Upload a file to the S3 bucket
                Parameters:
                    file_path (str): The path to the file to upload
                    bucket (str): The name of the S3 bucket
                    key (str): The key of the file in the S3 bucket
                '''
                # check if the function parameters are valid
                if not isinstance(file_path, str):
                    raise ValueError("file_path must be a string")
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(key, str):
                    raise ValueError("key must be a string")

                failure = False
                exception = None

                print(f"Uploading file to S3: {file_path} to {bucket}/{key}")

                for i in range(self.retry_limit):
                    try:
                        self.s3.upload_file(file_path, bucket, key)
                        failure = False
                        break
                    except Exception as e:
                        print(f"Error uploading file to S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

            def download_file(self, bucket, key, file_path):
                '''
                Download a file from the S3 bucket
                Parameters:
                    bucket (str): The name of the S3 bucket
                    key (str): The key of the file in the S3 bucket
                    file_path (str): The path to the file to download
                '''
                # check if the function parameters are valid
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(key, str):
                    raise ValueError("key must be a string")
                if not isinstance(file_path, str):
                    raise ValueError("file_path must be a string")

                failure = False
                exception = None

                print(f"Downloading file from S3: {bucket}/{key} to {file_path}")

                for i in range(self.retry_limit):
                    try:
                        self.s3.download_file(bucket, key, file_path)
                        failure = False
                        break
                    except Exception as e:
                        print(f"Error downloading file from S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

            def delete_file(self, Bucket, Key):
                '''
                Delete a file from the S3 bucket
                Parameters:
                    Bucket (str): The name of the S3 bucket
                    Key (str): The key of the file in the S3 bucket
                '''
                # check if the function parameters are valid
                if not isinstance(Bucket, str):
                    raise ValueError("Bucket must be a string")
                if not isinstance(Key, str):
                    raise ValueError("Key must be a string")

                failure = False
                exception = None

                for i in range(self.retry_limit):
                    try:
                        self.s3.delete_object(Bucket=Bucket, Key=Key)
                        failure = False
                        break
                    except Exception as e:
                        print(f"Error downloading file from S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

            def list_files(self, bucket, prefix):
                '''
                List files in the S3 bucket
                Parameters:
                    bucket (str): The name of the S3 bucket
                    prefix (str): The prefix of the files in the S3 bucket
                '''
                # check if the function parameters are valid
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(prefix, str):
                    raise ValueError("prefix must be a string")

                failure = False
                exception = None

                for i in range(self.retry_limit):
                    try:
                        response = self.s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
                        failure = False
                        break
                    except Exception as e:
                        print(f"Error downloading file from S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

                return response

            def clean_objects_with_prefix(self, bucket, prefix):
                '''
                Clean up the S3 bucket
                Parameters:
                    bucket (str): The name of the S3 bucket
                    prefix (str): The prefix of the files in the S3 bucket
                '''
                # check if the function parameters are valid
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(prefix, str):
                    raise ValueError("prefix must be a string")

                func = "clean_s3_prefix"

                print(f"{func}: Clean up folder: {prefix}")

                try:

                    response = self.list_files(bucket, prefix)

                    if 'Contents' in response:
                        for item in response['Contents']:
                            file_key = item['Key']

                            print(f"{func}: Deleting file: {file_key}")

                            if file_key[-1] == '/':
                                continue

                            self.delete_file(bucket, file_key)
                except Exception as e:
                    print(f"{func}: Failed to clean up scratch folder: {e}")
                    return


        ENV = os.getenv("ENV")

        AUTO_UPDATE_HELM_CHART = os.getenv("AUTO_UPDATE_HELM_CHART")

        # auto update helm chart git repo
        OWNER = os.getenv("OWNER")
        REPO = os.getenv("REPO")
        BRANCH = os.getenv("BRANCH")
        BUCKET = os.getenv("BUCKET")
        HELM_GITHUB_REPO = os.getenv("HELM_GITHUB_REPO")

        # required to turn off the airflow while updating
        AIRFLOW_LB_NAME = os.getenv('AIRFLOW_LB_NAME')
        AIRFLOW_USERNAME = os.getenv('AIRFLOW_USERNAME')
        AIRFLOW_PASSWORD = os.getenv('AIRFLOW_PASSWORD')

        # Replace with your target repository details
        STACK_ID = os.getenv("STACK_ID")
        STACK_NAME = os.getenv("STACK_NAME")
        CLUSTER_NAME = os.getenv("CLUSTER_NAME")
        LAMBDA_ROLE_ARN = os.getenv("LAMBDA_ROLE_ARN")

        CYPIENTA_NS = os.getenv("CYPIENTA_NS")
        EFS_FILE_SYSTEM_ID = os.getenv("EFS_FILE_SYSTEM_ID")
        AWS_REGION = os.getenv("AWS_REGION")

        LAMBDA_IMAGE_REPO = os.getenv("LAMBDA_IMAGE_REPO")
        LAMBDA_IMAGE_TAG = os.getenv("LAMBDA_IMAGE_TAG")
        # LAMBDA_AGG_IMAGE_REPO = os.getenv("LAMBDA_AGG_IMAGE_REPO")
        # LAMBDA_AGG_IMAGE_TAG = os.getenv("LAMBDA_AGG_IMAGE_TAG")
        # CLUSTER_1_MODEL_IMAGE_REPO = os.getenv("CLUSTER_1_MODEL_IMAGE_REPO")
        # CLUSTER_1_MODEL_IMAGE_TAG = os.getenv("CLUSTER_1_MODEL_IMAGE_TAG")
        # FLOW_MODEL_IMAGE_REPO = os.getenv("FLOW_MODEL_IMAGE_REPO")
        # FLOW_MODEL_IMAGE_TAG = os.getenv("FLOW_MODEL_IMAGE_TAG")

        # UI_IMAGE_REPO = os.getenv("UI_IMAGE_REPO")
        # UI_IMAGE_TAG = os.getenv("UI_IMAGE_TAG")
        # UI_NGINX_IMAGE_REPO = os.getenv("UI_NGINX_IMAGE_REPO")
        # UI_NGINX_IMAGE_TAG = os.getenv("UI_NGINX_IMAGE_TAG")

        AIRFLOW_IMAGE_REPO = os.getenv("AIRFLOW_IMAGE_REPO")
        AIRFLOW_IMAGE_TAG = os.getenv("AIRFLOW_IMAGE_TAG")

        # LOG_COLLECTION_IMAGE_REPO = os.getenv("LOG_COLLECTION_IMAGE_REPO")
        # LOG_COLLECTION_IMAGE_TAG = os.getenv("LOG_COLLECTION_IMAGE_TAG")

        # MAX_ALERTS_FOR_UI_UPLOAD = os.getenv("MAX_ALERTS_FOR_UI_UPLOAD")

        # AGG_NUM_WORKERS = os.getenv("AGG_NUM_WORKERS")
        # AGG_MEMORY_LIMIT_IN_GB = os.getenv("AGG_MEMORY_LIMIT_IN_GB")
        # AGG_BLOCKSIZE = os.getenv("AGG_BLOCKSIZE")
        # CHUNK_SIZE = os.getenv("CHUNK_SIZE")

        # SEQUENCER_CONTEXT_WINDOW_SIZE = os.getenv("SEQUENCER_CONTEXT_WINDOW_SIZE")

        LAMBDA_CPU_LIMIT = os.getenv("LAMBDA_CPU_LIMIT")
        LAMBDA_MEMORY_LIMIT = os.getenv("LAMBDA_MEMORY_LIMIT")

        # AGGREGATOR_CPU_LIMIT = os.getenv("AGGREGATOR_CPU_LIMIT")
        # AGGREGATOR_MEMORY_LIMIT = os.getenv("AGGREGATOR_MEMORY_LIMIT")

        # CLUSTER_MODEL_CPU_LIMIT = os.getenv("CLUSTER_MODEL_CPU_LIMIT")
        # CLUSTER_MODEL_MEMORY_LIMIT = os.getenv("CLUSTER_MODEL_MEMORY_LIMIT")

        # FLOW_MODEL_CPU_LIMIT = os.getenv("FLOW_MODEL_CPU_LIMIT")
        # FLOW_MODEL_MEMORY_LIMIT = os.getenv("FLOW_MODEL_MEMORY_LIMIT")

        # node pool counts

        # MAX_CPU_TO_USE = os.getenv("MAX_CPU_TO_USE")
        # MAX_RAM_TO_USE = os.getenv("MAX_RAM_TO_USE")

        # LAMBDA_NODE_CPU = os.getenv("LAMBDA_NODE_CPU")
        # LAMBDA_NODE_RAM = os.getenv("LAMBDA_NODE_RAM")

        # HIGH_CPU_RAM_NODE_CPU = os.getenv("HIGH_CPU_RAM_NODE_CPU")
        # HIGH_CPU_RAM_NODE_RAM = os.getenv("HIGH_CPU_RAM_NODE_RAM")

        # resource percentages

        # SCHEMA_RESOURCE_PERCENTAGE = os.getenv("SCHEMA_RESOURCE_PERCENTAGE")

        # AGGREGATOR_RESOURCE_PERCENTAGE = os.getenv("AGGREGATOR_RESOURCE_PERCENTAGE")

        # CLUSTER_MODEL_RESOURCE_PERCENTAGE = os.getenv("CLUSTER_MODEL_RESOURCE_PERCENTAGE")

        # FLOW_MODEL_RESOURCE_PERCENTAGE = os.getenv("FLOW_MODEL_RESOURCE_PERCENTAGE")

        # CLUSTER_OUT_RESOURCE_PERCENTAGE = os.getenv("CLUSTER_OUT_RESOURCE_PERCENTAGE")

        # FLOW_OUT_RESOURCE_PERCENTAGE = os.getenv("FLOW_OUT_RESOURCE_PERCENTAGE")

        missing_variables = []

        # check if all variables are set
        if not AUTO_UPDATE_HELM_CHART:
            missing_variables.append("AUTO_UPDATE_HELM_CHART")

        if not OWNER:
            missing_variables.append("OWNER")
        if not REPO:
            missing_variables.append("REPO")
        if not BRANCH:
            missing_variables.append("BRANCH")
        if not BUCKET:
            missing_variables.append("BUCKET")

        if not AIRFLOW_LB_NAME:
            missing_variables.append("AIRFLOW_LB_NAME")
        if not AIRFLOW_USERNAME:
            missing_variables.append("AIRFLOW_USERNAME")
        if not AIRFLOW_PASSWORD:
            missing_variables.append("AIRFLOW_PASSWORD")

        if not STACK_ID:
            missing_variables.append("STACK_ID")
        if not STACK_NAME:
            missing_variables.append("STACK_NAME")
        if not CLUSTER_NAME:
            missing_variables.append("CLUSTER_NAME")
        if not LAMBDA_ROLE_ARN:
            missing_variables.append("LAMBDA_ROLE_ARN")

        if not CYPIENTA_NS:
            missing_variables.append("CYPIENTA_NS")
        if not EFS_FILE_SYSTEM_ID:
            missing_variables.append("EFS_FILE_SYSTEM_ID")
        if not AWS_REGION:
            missing_variables.append("AWS_REGION")

        if not LAMBDA_IMAGE_REPO:
            missing_variables.append("LAMBDA_IMAGE_REPO")
        if not LAMBDA_IMAGE_TAG:
            missing_variables.append("LAMBDA_IMAGE_TAG")
        # if not LAMBDA_AGG_IMAGE_REPO:
        #     missing_variables.append("LAMBDA_AGG_IMAGE_REPO")
        # if not LAMBDA_AGG_IMAGE_TAG:
        #     missing_variables.append("LAMBDA_AGG_IMAGE_TAG")
        # if not CLUSTER_1_MODEL_IMAGE_REPO:
        #     missing_variables.append("CLUSTER_1_MODEL_IMAGE_REPO")
        # if not CLUSTER_1_MODEL_IMAGE_TAG:
        #     missing_variables.append("CLUSTER_1_MODEL_IMAGE_TAG")
        # if not FLOW_MODEL_IMAGE_REPO:
        #     missing_variables.append("FLOW_MODEL_IMAGE_REPO")
        # if not FLOW_MODEL_IMAGE_TAG:
        #     missing_variables.append("FLOW_MODEL_IMAGE_TAG")
        # if not UI_IMAGE_REPO:
        #     missing_variables.append("UI_IMAGE_REPO")
        # if not UI_IMAGE_TAG:
        #     missing_variables.append("UI_IMAGE_TAG")
        # if not UI_NGINX_IMAGE_REPO:
        #     missing_variables.append("UI_NGINX_IMAGE_REPO")
        # if not UI_NGINX_IMAGE_TAG:
        #     missing_variables.append("UI_NGINX_IMAGE_TAG")
        if not AIRFLOW_IMAGE_REPO:
            missing_variables.append("AIRFLOW_IMAGE_REPO")
        if not AIRFLOW_IMAGE_TAG:
            missing_variables.append("AIRFLOW_IMAGE_TAG")
        # if not LOG_COLLECTION_IMAGE_REPO:
        #     missing_variables.append("LOG_COLLECTION_IMAGE_REPO")
        # if not LOG_COLLECTION_IMAGE_TAG:
        #     missing_variables.append("LOG_COLLECTION_IMAGE_TAG")

        # if not MAX_ALERTS_FOR_UI_UPLOAD:
        #     missing_variables.append("MAX_ALERTS_FOR_UI_UPLOAD")
        # if not AGG_NUM_WORKERS:
        #     missing_variables.append("AGG_NUM_WORKERS")
        # if not AGG_MEMORY_LIMIT_IN_GB:
        #     missing_variables.append("AGG_MEMORY_LIMIT_IN_GB")
        # if not AGG_BLOCKSIZE:
        #     missing_variables.append("AGG_BLOCKSIZE")
        # if not CHUNK_SIZE:
        #     missing_variables.append("CHUNK_SIZE")
        # if not SEQUENCER_CONTEXT_WINDOW_SIZE:
        #     missing_variables.append("SEQUENCER_CONTEXT_WINDOW_SIZE")

        if not LAMBDA_CPU_LIMIT:
            missing_variables.append("LAMBDA_CPU_LIMIT")
        if not LAMBDA_MEMORY_LIMIT:
            missing_variables.append("LAMBDA_MEMORY_LIMIT")
        # if not AGGREGATOR_CPU_LIMIT:
        #     missing_variables.append("AGGREGATOR_CPU_LIMIT")
        # if not AGGREGATOR_MEMORY_LIMIT:
        #     missing_variables.append("AGGREGATOR_MEMORY_LIMIT")
        # if not CLUSTER_MODEL_CPU_LIMIT:
        #     missing_variables.append("CLUSTER_MODEL_CPU_LIMIT")
        # if not CLUSTER_MODEL_MEMORY_LIMIT:
        #     missing_variables.append("CLUSTER_MODEL_MEMORY_LIMIT")
        # if not FLOW_MODEL_CPU_LIMIT:
        #     missing_variables.append("FLOW_MODEL_CPU_LIMIT")
        # if not FLOW_MODEL_MEMORY_LIMIT:
        #     missing_variables.append("FLOW_MODEL_MEMORY_LIMIT")

        # if not MAX_CPU_TO_USE:
        #     missing_variables.append("MAX_CPU_TO_USE")
        # if not MAX_RAM_TO_USE:
        #     missing_variables.append("MAX_RAM_TO_USE")

        # if not LAMBDA_NODE_CPU:
        #     missing_variables.append("LAMBDA_NODE_CPU")
        # if not LAMBDA_NODE_RAM:
        #     missing_variables.append("LAMBDA_NODE_RAM")

        # if not HIGH_CPU_RAM_NODE_CPU:
        #     missing_variables.append("HIGH_CPU_RAM_NODE_CPU")
        # if not HIGH_CPU_RAM_NODE_RAM:
        #     missing_variables.append("HIGH_CPU_RAM_NODE_RAM")

        # # resource percentages

        # if not SCHEMA_RESOURCE_PERCENTAGE:
        #     missing_variables.append("SCHEMA_RESOURCE_PERCENTAGE")

        # if not AGGREGATOR_RESOURCE_PERCENTAGE:
        #     missing_variables.append("AGGREGATOR_RESOURCE_PERCENTAGE")

        # if not CLUSTER_MODEL_RESOURCE_PERCENTAGE:
        #     missing_variables.append("CLUSTER_MODEL_RESOURCE_PERCENTAGE")

        # if not FLOW_MODEL_RESOURCE_PERCENTAGE:
        #     missing_variables.append("FLOW_MODEL_RESOURCE_PERCENTAGE")

        # if not CLUSTER_OUT_RESOURCE_PERCENTAGE:
        #     missing_variables.append("CLUSTER_OUT_RESOURCE_PERCENTAGE")

        # if not FLOW_OUT_RESOURCE_PERCENTAGE:
        #     missing_variables.append("FLOW_OUT_RESOURCE_PERCENTAGE")


        if missing_variables:
            print(f"Missing variables: {missing_variables}")
            raise ValueError(f"Missing variables: {missing_variables}")

        if AUTO_UPDATE_HELM_CHART == "true":
            AUTO_UPDATE_HELM_CHART = True
        else:
            AUTO_UPDATE_HELM_CHART = False


        # if not MAX_ALERTS_FOR_UI_UPLOAD.isdigit():
        #     raise ValueError("MAX_ALERTS_FOR_UI_UPLOAD must be a number")

        # if not AGG_NUM_WORKERS.isdigit():
        #     raise ValueError("AGG_NUM_WORKERS must be a number")

        # if not AGG_MEMORY_LIMIT_IN_GB.isdigit():
        #     raise ValueError("AGG_MEMORY_LIMIT_IN_GB must be a number")

        # if not AGG_BLOCKSIZE.isdigit():
        #     raise ValueError("AGG_BLOCKSIZE must be a number")

        # if not CHUNK_SIZE.isdigit():
        #     raise ValueError("CHUNK_SIZE must be a number")

        # if not SEQUENCER_CONTEXT_WINDOW_SIZE.isdigit():
        #     raise ValueError("SEQUENCER_CONTEXT_WINDOW_SIZE must be a number")

        if not LAMBDA_CPU_LIMIT.isdigit():
            raise ValueError("LAMBDA_CPU_LIMIT must be a number")
        if not LAMBDA_MEMORY_LIMIT.isdigit():
            raise ValueError("LAMBDA_MEMORY_LIMIT must be a number")

        # if not AGGREGATOR_CPU_LIMIT.isdigit():
        #     raise ValueError("AGGREGATOR_CPU_LIMIT must be a number")
        # if not AGGREGATOR_MEMORY_LIMIT.isdigit():
        #     raise ValueError("AGGREGATOR_MEMORY_LIMIT must be a number")

        # if not CLUSTER_MODEL_CPU_LIMIT.isdigit():
        #     raise ValueError("CLUSTER_MODEL_CPU_LIMIT must be a number")
        # if not CLUSTER_MODEL_MEMORY_LIMIT.isdigit():
        #     raise ValueError("CLUSTER_MODEL_MEMORY_LIMIT must be a number")

        # if not FLOW_MODEL_CPU_LIMIT.isdigit():
        #     raise ValueError("FLOW_MODEL_CPU_LIMIT must be a number")
        # if not FLOW_MODEL_MEMORY_LIMIT.isdigit():
        #     raise ValueError("FLOW_MODEL_MEMORY_LIMIT must be a number")

        # if not MAX_CPU_TO_USE.isdigit():
        #     raise ValueError("MAX_CPU_TO_USE must be a number")
        # if not MAX_RAM_TO_USE.isdigit():
        #     raise ValueError("MAX_RAM_TO_USE must be a number")

        # if not LAMBDA_NODE_CPU.isdigit():
        #     raise ValueError("LAMBDA_NODE_CPU must be a number")
        # if not LAMBDA_NODE_RAM.isdigit():
        #     raise ValueError("LAMBDA_NODE_RAM must be a number")

        # if not HIGH_CPU_RAM_NODE_CPU.isdigit():
        #     raise ValueError("HIGH_CPU_RAM_NODE_CPU must be a number")
        # if not HIGH_CPU_RAM_NODE_RAM.isdigit():
        #     raise ValueError("HIGH_CPU_RAM_NODE_RAM must be a number")

        # if not SCHEMA_RESOURCE_PERCENTAGE.isdigit():
        #     raise ValueError("SCHEMA_RESOURCE_PERCENTAGE must be a number")
        # if not AGGREGATOR_RESOURCE_PERCENTAGE.isdigit():
        #     raise ValueError("AGGREGATOR_RESOURCE_PERCENTAGE must be a number")
        # if not CLUSTER_MODEL_RESOURCE_PERCENTAGE.isdigit():
        #     raise ValueError("CLUSTER_MODEL_RESOURCE_PERCENTAGE must be a number")
        # if not FLOW_MODEL_RESOURCE_PERCENTAGE.isdigit():
        #     raise ValueError("FLOW_MODEL_RESOURCE_PERCENTAGE must be a number")
        # if not CLUSTER_OUT_RESOURCE_PERCENTAGE.isdigit():
        #     raise ValueError("CLUSTER_OUT_RESOURCE_PERCENTAGE must be a number")
        # if not FLOW_OUT_RESOURCE_PERCENTAGE.isdigit():
        #     raise ValueError("FLOW_OUT_RESOURCE_PERCENTAGE must be a number")

        # MAX_ALERTS_FOR_UI_UPLOAD = int(MAX_ALERTS_FOR_UI_UPLOAD)

        # AGG_NUM_WORKERS = int(AGG_NUM_WORKERS)
        # AGG_MEMORY_LIMIT_IN_GB = int(AGG_MEMORY_LIMIT_IN_GB)
        # AGG_BLOCKSIZE = int(AGG_BLOCKSIZE)
        # CHUNK_SIZE = int(CHUNK_SIZE)

        # SEQUENCER_CONTEXT_WINDOW_SIZE = int(SEQUENCER_CONTEXT_WINDOW_SIZE)

        LAMBDA_CPU_LIMIT = int(LAMBDA_CPU_LIMIT)
        LAMBDA_MEMORY_LIMIT = int(LAMBDA_MEMORY_LIMIT)

        # AGGREGATOR_CPU_LIMIT = int(AGGREGATOR_CPU_LIMIT)
        # AGGREGATOR_MEMORY_LIMIT = int(AGGREGATOR_MEMORY_LIMIT)

        # CLUSTER_MODEL_CPU_LIMIT = int(CLUSTER_MODEL_CPU_LIMIT)
        # CLUSTER_MODEL_MEMORY_LIMIT = int(CLUSTER_MODEL_MEMORY_LIMIT)

        # FLOW_MODEL_CPU_LIMIT = int(FLOW_MODEL_CPU_LIMIT)
        # FLOW_MODEL_MEMORY_LIMIT = int(FLOW_MODEL_MEMORY_LIMIT)

        # # node pool counts

        # MAX_CPU_TO_USE = int(MAX_CPU_TO_USE)
        # MAX_RAM_TO_USE = int(MAX_RAM_TO_USE)

        # LAMBDA_NODE_CPU = int(LAMBDA_NODE_CPU)
        # LAMBDA_NODE_RAM = int(LAMBDA_NODE_RAM)

        # HIGH_CPU_RAM_NODE_CPU = int(HIGH_CPU_RAM_NODE_CPU)
        # HIGH_CPU_RAM_NODE_RAM = int(HIGH_CPU_RAM_NODE_RAM)

        # # resource percentages

        # SCHEMA_RESOURCE_PERCENTAGE = int(SCHEMA_RESOURCE_PERCENTAGE)
        # AGGREGATOR_RESOURCE_PERCENTAGE = int(AGGREGATOR_RESOURCE_PERCENTAGE)
        # CLUSTER_MODEL_RESOURCE_PERCENTAGE = int(CLUSTER_MODEL_RESOURCE_PERCENTAGE)
        # FLOW_MODEL_RESOURCE_PERCENTAGE = int(FLOW_MODEL_RESOURCE_PERCENTAGE)
        # CLUSTER_OUT_RESOURCE_PERCENTAGE = int(CLUSTER_OUT_RESOURCE_PERCENTAGE)
        # FLOW_OUT_RESOURCE_PERCENTAGE = int(FLOW_OUT_RESOURCE_PERCENTAGE)


        S3_SERVICE = S3_service()

        SCRATCH_DIR = "scratch"

        TEMP_FILE_PREFIX = "/tmp/lambda"
        os.makedirs(TEMP_FILE_PREFIX, exist_ok=True)

        TEMPLATE_REPO_COMMIT_FILEPATH = "repo/repo_commit.json"
        TEMP_TEMPLATE_REPO_COMMIT_FILEPATH = f"{TEMP_FILE_PREFIX}/repo_commit.json"

        TEMPLATE_FILEPATH = "repo/template.yaml"
        TEMP_TEMPLATE_FILEPATH = f"{TEMP_FILE_PREFIX}/template.yaml"

        KUBE_CONFIG_FILE = f"{TEMP_FILE_PREFIX}/kubeconfig.yaml"


        def lambda_handler(event, context):
            func = "lambda_handler"

            clear_temp_dir()
            os.makedirs(TEMP_FILE_PREFIX, exist_ok=True)

            print(f"{func}: Getting latest commit from the target branch.")
            temp_git_template_filepath = get_helm_chart_git()
            git_template_file_hash = file_hash(temp_git_template_filepath)
            latest_commit = {
                "sha": git_template_file_hash,
            }

            print(f"{func}: Latest commit: {latest_commit}")

            sha = latest_commit["sha"]

            file_exists = fetch_file_if_present(BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH, TEMP_TEMPLATE_REPO_COMMIT_FILEPATH)

            is_update_helm = True

            if file_exists:
                if not AUTO_UPDATE_HELM_CHART:
                    print("Auto update helm chart is disabled")
                    return

                with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'r') as f:
                    template_repo_commit_json = json.load(f)

                # if the stack status on current sha is success, then no need to update the stack
                # if the stack status on current sha is failure, then it had failed to update the stack,
                # keep it in stable state as rolledback. Mark the current commit as failure.
                # if the stack status on current sha is anything else, then check for the current status of the stack and do the updates accordingly.
                if template_repo_commit_json["sha"] == sha:
                    print(f"{func}: No new commits. No need to update the stack.")
                    if template_repo_commit_json["stack_status"] == "success":
                        print(f"{func}: Stack is in success state. No need to update the stack.")
                        is_update_helm = False
                        if template_repo_commit_json["service_status"] == "success":
                            return
                        print(f"{func}: Stack service_status is unknown. Check the stack status.")

                    elif template_repo_commit_json["stack_status"] == "failed":
                        print(f"{func}: Stack is in failed state. Rolled back to the previous working state.")
                        is_update_helm = False
                        if template_repo_commit_json["service_status"] == "success":
                            return
                        print(f"{func}: Stack service_status is unknown. Check the stack status.")

                    else:
                        print(f"{func}: Stack status is unknown. Please check the stack status.")
                        is_update_helm = False

            else:
                print(f"{func}: No repo commit file found. Creating a new one.")
                print(f"{func}: sha: {sha}")

                # As this is a first check on the repo, we assume that the current commit is the latest used for stack
                # adding init status as success that the current commit id is in success state with stack.

                template_repo_commit_json = {
                    "sha": sha,
                    "stack_status": "success",
                    "service_status": "success",
                }

                with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'w') as f:
                    json.dump(template_repo_commit_json, f)

                print(f"{func}: Update helm chart template")

                url = f"https://raw.githubusercontent.com/{OWNER}/{REPO}/{BRANCH}/{HELM_GITHUB_REPO}"
                headers = {}

                try:
                    response = requests.get(url, headers=headers)
                    response.raise_for_status()

                    file_content = response.text

                    with open(TEMP_TEMPLATE_FILEPATH, "w", encoding="utf-8") as file:
                        file.write(file_content)
                    print(f"{func}: Template file saved at: {TEMP_TEMPLATE_FILEPATH}")

                except Exception as e:
                    print(f"{func}: Failed to get the template from the repo: {e}")
                    raise e

                helm_repo_json = json.load(open(TEMP_TEMPLATE_FILEPATH, 'r'))
                helm_repo_url = helm_repo_json["helm_repo_url"]
                helm_chart_version = helm_repo_json["helm_chart_version"]

                print(f"{func}: Helm repo json: {helm_repo_json}")
                print(f"{func}: helm_repo_url: {helm_repo_url}")
                print(f"{func}: helm_chart_version: {helm_chart_version}")

                asyncio.run(upgrade_install_helm_chart(helm_repo_url, helm_chart_version))

                S3_SERVICE.upload_file(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH)

                # update the template file on s3
                S3_SERVICE.upload_file(TEMP_TEMPLATE_FILEPATH, BUCKET, TEMPLATE_FILEPATH)

                return

            if is_update_helm:
                print("New commit found. Update stack.")
                update_stack(latest_commit)
            else:
                print("No new commit found. Check current stack status.")
                check_stack_update_status()

            return


        def update_stack(latest_commit):
            '''
            Update the stack
            Args:
                latest_commit (dict): The latest commit from the target branch
            '''
            func = "update_stack"

            print(f"{func}: Update helm chart")

            url = f"https://raw.githubusercontent.com/{OWNER}/{REPO}/{BRANCH}/{HELM_GITHUB_REPO}"
            headers = {}

            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()

                file_content = response.text

                with open(TEMP_TEMPLATE_FILEPATH, "w", encoding="utf-8") as file:
                    file.write(file_content)
                print(f"Template file saved at: {TEMP_TEMPLATE_FILEPATH}")

            except Exception as e:
                print(f"{func}: Failed to get the template from the repo: {e}")
                raise e

            # update the template file on s3
            S3_SERVICE.upload_file(TEMP_TEMPLATE_FILEPATH, BUCKET, TEMPLATE_FILEPATH)

            helm_repo_json = json.load(open(TEMP_TEMPLATE_FILEPATH, 'r'))
            helm_repo_url = helm_repo_json["helm_repo_url"]
            helm_chart_version = helm_repo_json["helm_chart_version"]

            print(f"{func}: Upgrade helm chart")
            print(f"helm_repo_url: {helm_repo_url}")
            print(f"helm_chart_version: {helm_chart_version}")

            # pause the pipeline
            print(f"{func}: Pause the pipeline on Airflow")
            payload = {
                "is_paused": True
            }
            api_airflow_patch_dag(AIRFLOW_LB_NAME, AIRFLOW_USERNAME, AIRFLOW_PASSWORD, "file_polling", payload)

            # check statuses of dags on Airflow
            print(f"{func}: Check statuses of dags on Airflow")
            can_update_stack = check_pipelines_completion_status(AIRFLOW_LB_NAME, AIRFLOW_USERNAME, AIRFLOW_PASSWORD)

            if can_update_stack:
                print(f"{func}: Update stack")
                asyncio.run(upgrade_install_helm_chart(helm_repo_url, helm_chart_version))
            else:
                print(f"{func}: Cannot update stack yet")

            print("Update the repo commit file")
            template_repo_commit_json = {
                "sha": latest_commit["sha"],
                "stack_status": "update_in_progress",
                "service_status": "pending",
            }

            with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'w') as f:
                json.dump(template_repo_commit_json, f)

            S3_SERVICE.upload_file(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH)


        def check_stack_update_status():
            '''
            Check the stack update status
            '''
            stack_operation_status = check_stack_operation()

            template_repo_commit_json = json.load(open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'r'))

            if stack_operation_status is None:
                print("Stack update is still in progress")
                return

            if stack_operation_status:
                print("Stack update completed successfully")
                template_repo_commit_json["stack_status"] = "success"
                template_repo_commit_json["service_status"] = "pending"
                check_current_task_running()
            else:
                print("Stack update failed")
                template_repo_commit_json["stack_status"] = "failed"
                template_repo_commit_json["service_status"] = "pending"
                rollback_helm_chart()

            with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'w') as f:
                json.dump(template_repo_commit_json, f)

            S3_SERVICE.upload_file(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH)

            print("Resume the pipeline on Airflow")
            payload = {
                "is_paused": False
            }
            api_airflow_patch_dag(AIRFLOW_LB_NAME, AIRFLOW_USERNAME, AIRFLOW_PASSWORD, "file_polling", payload)

            template_repo_commit_json["service_status"] = "success"

            with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'w') as f:
                json.dump(template_repo_commit_json, f)

            S3_SERVICE.upload_file(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH)


        def check_stack_operation():
            """
            Checks for the helm chart to be of specific version.
            status of helm chart should be deployed.
            If its "failed" then return false.
            If its "deployed" then return true.
            If its anything else, then return None.

            Returns: None - if the stack is still in progress
                    True - if the stack is in success state
                    False - if the stack is in failed state

            """
            func = "check_stack_operation"

            print(f"{func}: Checking helm chart status")

            helm_client = Client(kubeconfig=KUBE_CONFIG_FILE, executable=HELM_EXECUTABLE)

            info = asyncio.run(helm_client.get_current_revision(CYPIENTA_NS, namespace=CYPIENTA_NS))

            if info.status == "deployed":
                return True
            elif info.status == "failed":
                return False
            return None


        def check_current_task_running():
            '''
            Check that current task are running with new image
            '''
            # check current template file on s3
            S3_SERVICE.download_file(BUCKET, TEMPLATE_FILEPATH, TEMP_TEMPLATE_FILEPATH)

            # Read the YAML file
            with open(TEMP_TEMPLATE_FILEPATH, 'r') as f:
                helm_repo_json = json.load(f)

            new_images_keys = {
                "airflow": helm_repo_json["images"]["airflow"],
                "ui": helm_repo_json["images"]["ui_web"],
            }

            print("Get tasks currently running on the stack")

            check_and_delete_pods(new_images_keys, namespace=CYPIENTA_NS)


        def check_and_delete_pods(expected_images, namespace):
            """
            Checks all pods in the given namespace
            If any container image in a pod does not match the expected image(s), deletes the pod.

            Args:
                expected_images: dict mapping pod name to expected image.
                namespace: str
            """
            config.load_kube_config(config_file=KUBE_CONFIG_FILE)
            v1 = client.CoreV1Api()

            pods = v1.list_namespaced_pod(namespace=namespace)

            container_image_map = {
                # "bastet-celery": "ui",
                # "bastet-celery-beat": "ui",
                # "bastet-web": "ui",
                "scheduler": "airflow",
                # "triggerer": "airflow",
                "worker": "airflow",
                "webserver": "airflow",
            }

            pods_with_wrong_images = []

            for pod in pods.items:
                pod_name = pod.metadata.name
                ns = pod.metadata.namespace
                containers = pod.spec.containers

                pod_has_wrong_image = False

                for c in containers:
                    container_name = c.name
                    actual_image = c.image

                    # Check if this container is in the map
                    key = container_image_map.get(container_name)
                    if key:
                        expected_image = expected_images[key]
                        if actual_image != expected_image:
                            print(f"Pod {pod_name} (ns: {ns}), container {container_name}: image '{actual_image}' does not match expected '{expected_image}'")
                            pod_has_wrong_image = True

                if pod_has_wrong_image:
                    pods_with_wrong_images.append(pod_name)

            print("Pods with wrong images:", pods_with_wrong_images)

            for pod_name in pods_with_wrong_images:
                print(f"Deleting pod {pod_name} in namespace {namespace}")
                v1.delete_namespaced_pod(name=pod_name, namespace=namespace)


        def rollback_helm_chart():
            '''
            Rollback the helm chart to the previous revision
            '''

            helm_rollback_command = f"{HELM_EXECUTABLE} rollback {CYPIENTA_NS} --namespace {CYPIENTA_NS} --kubeconfig {KUBE_CONFIG_FILE}"
            helm_rollback_command = helm_rollback_command.split()

            subprocess.check_call(helm_rollback_command)


        def get_helm_chart_git():
            '''
            Get the helm chart from the github repo
            Returns:
                temp_git_template_filepath: str file path to git template
            '''
            func = "get_helm_chart_git"

            url = f"https://raw.githubusercontent.com/{OWNER}/{REPO}/{BRANCH}/{HELM_GITHUB_REPO}"
            headers = {}

            temp_git_template_filepath = f"{TEMP_FILE_PREFIX}/git_template.yaml"

            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()

                file_content = response.text

                with open(temp_git_template_filepath, "w", encoding="utf-8") as file:
                    file.write(file_content)
                print(f"{func}: Template file saved at: {temp_git_template_filepath}")

            except Exception as e:
                print(f"{func}: Failed to get the template from the repo: {e}")
                raise e

            return temp_git_template_filepath


        def get_latest_commit():
            '''
            Get the latest commit from the target branch
            Returns:
                commit: dict
            '''
            url = f"https://api.github.com/repos/{OWNER}/{REPO}/commits?sha={BRANCH}"
            headers = {}

            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()

                commits = response.json()
                commit = commits[0]

                return commit
            except Exception as e:
                print(f"Error fetching commits: {e}")

            return None


        def fetch_file_if_present(bucket, volume_file_key, temp_file_path):
            '''
            Check if the file exists on S3. Download if its present
            Args:
                bucket (string): bucket name
                volume_file_key (string): File key path to the cluster ticket output in volume
                temp_file_path (string): local file path to download the cluster ticket output to

            Returns:
                bool: Boolean value false, if there is no such file, true if there is a file
            '''

            if not isinstance(bucket, str):
                raise ValueError("bucket must be a string")
            if not isinstance(volume_file_key, str):
                raise ValueError("volume_file_key must be a string")
            if not isinstance(temp_file_path, str):
                raise ValueError("temp_file_path must be a string")

            func = "fetch_file_if_present"

            print(
                f"{func}: Checking if file exists on S3 path {bucket}/{volume_file_key}")

            file_exists = S3_SERVICE.check_file_exists(bucket, volume_file_key)

            if not file_exists:
                print(f"{func}: File does not exist on S3. Return False")
                return False

            else:
                print(f"{func}: File exists on S3. Downloading file")
                S3_SERVICE.download_file(bucket, volume_file_key, temp_file_path)
                print(f"{func}: File downloaded")
                return True


        def check_pipelines_completion_status(airflow_lb_name, username, password):
            """
            Check the status of all pipelines

            Args:
                airflow_lb_name (str): The load balancer name of the Airflow instance
                username (str): The username for the Airflow instance
                password (str): The password for the Airflow instance

            Returns:
                bool: True if all dag runs are either in failed or success state, False otherwise
            """
            dags_to_monitor = [
                # 'aggregator',
                # 'clustering',
                'file_polling',
                'map_and_dedupe',
                # 'sequencer',
                # 'save_feedback',
            ]

            dag_runs = {}  # Track runs for each DAG

            print("Checking DAG statuses...")

            # Check for new runs and update existing ones
            for dag_id in dags_to_monitor:
                active_runs = api_airflow_check_dag_status(airflow_lb_name, username, password, dag_id)

                if active_runs:
                    if dag_id not in dag_runs:
                        dag_runs[dag_id] = {}

                    for run_status in active_runs:
                        run_id = run_status['run_id']

                        # If this is a new run we haven't seen before
                        if run_id not in dag_runs[dag_id]:
                            dag_runs[dag_id][run_id] = run_status
                            print(f"New run detected for {dag_id} - Run ID: {run_id}")

                        # Update the status for this run
                        dag_runs[dag_id][run_id] = run_status

                        # log current status
                        print(f"{dag_id} (Run: {run_id}): {run_status['status']}")

            # Check if all runs are complete
            all_complete = True
            active_runs_exist = False
            has_failures = False

            for dag_id in dags_to_monitor:
                if dag_id in dag_runs:
                    for run_id, run_status in dag_runs[dag_id].items():
                        active_runs_exist = True
                        current_status = api_airflow_dag_run_status(airflow_lb_name, username, password, dag_id, run_id)

                        if current_status:
                            dag_runs[dag_id][run_id] = current_status

                            # if the dag id is file_polling, then paused dag may have queued statuses which is valid status for all completed
                            if dag_id == "file_polling" and current_status['status'] not in ['success', 'failed', 'queued']:
                                all_complete = False

                            elif current_status['status'] not in ['success', 'failed']:
                                all_complete = False

                            if current_status['status'] == 'failed':
                                has_failures = True

            # log summary of all runs
            print("\nCurrent DAG Run Summary:")
            for dag_id in dags_to_monitor:
                if dag_id in dag_runs:
                    for run_id, run_status in dag_runs[dag_id].items():
                        print(f"{dag_id} - Run {run_id}: {run_status['status']}")

            # If we have active runs and they're all complete, we're done
            if active_runs_exist and all_complete:
                if has_failures:
                    print("\nAll DAG runs completed, but some failed!")
                else:
                    print("\nAll DAG runs completed successfully!")
                return True

            elif not active_runs_exist:
                print("\nNo active runs found. Exiting...")
                return True

            return False


        def api_airflow_patch_dag(airflow_lb_name, username, password, dag_id, payload):
            '''
            Pause the dag
            Args:
                airflow_lb_name (string): Airflow load balancer name
                username (string): Airflow username
                password (string): Airflow password
            '''
            func = "api_airflow_pause_dag"

            airflow_url = get_load_balancer_dns_name(airflow_lb_name, AWS_REGION)

            if airflow_url is None:
                print(f"{func}: Airflow URL is not set. Skipping patch")
                raise ValueError("Airflow URL is not set")

            headers = {"Content-Type": "application/json"}

            try:
                url = f"{airflow_url}/api/v1/dags/{dag_id}"
                response = requests.patch(
                    url,
                    auth=HTTPBasicAuth(username, password),
                    headers=headers,
                    json=payload
                )
                response.raise_for_status()
                print(f"{func}: Pipeline patched")
            except Exception as e:
                print(f"{func}: Failed to patch the pipeline: {e}")
                raise e


        def api_airflow_check_dag_status(airflow_lb_name, username, password, dag_id):
            """
            Check the status of a DAG in Airflow

            Args:
                airflow_lb_name (str): The load balancer name of the Airflow instance
                username (str): The username for the Airflow instance
                password (str): The password for the Airflow instance
                dag_id (str): The ID of the DAG to check

            Returns:
                list: A list of dictionaries containing the status of each DAG run
            """
            airflow_url = get_load_balancer_dns_name(airflow_lb_name, AWS_REGION)

            base_url = f"{airflow_url}/api/v1"

            headers = {"Content-Type": "application/json"}

            try:
                response = requests.get(
                    f"{base_url}/dags/{dag_id}/dagRuns",
                    auth=HTTPBasicAuth(username, password),
                    headers=headers,
                )

                response.raise_for_status()

                dag_runs = response.json()['dag_runs']
                # Return all runs, not just active ones
                runs = []
                for run in dag_runs:
                    runs.append({
                        'dag_id': dag_id,
                        'status': run['state'],
                        'start_date': run['start_date'],
                        'end_date': run['end_date'],
                        'run_id': run['dag_run_id']
                    })
                return runs
            except Exception as e:
                print(f"Error connecting to Airflow API for DAG {dag_id}: {e}")
                raise e


        def api_airflow_dag_run_status(airflow_lb_name, username, password, dag_id, run_id):
            """
            Check the current status of a specific DAG run

            Args:
                airflow_lb_name (str): The load balancer name of the Airflow instance
                username (str): The username for the Airflow instance
                password (str): The password for the Airflow instance
                dag_id (str): The ID of the DAG to check
                run_id (str): The ID of the DAG run to check

            Returns:
                dict: A dictionary containing the status of the DAG run
            """
            airflow_url = get_load_balancer_dns_name(airflow_lb_name, AWS_REGION)
            base_url = f"{airflow_url}/api/v1"

            headers = {'Content-Type': 'application/json'}

            try:
                response = requests.get(
                    f"{base_url}/dags/{dag_id}/dagRuns/{run_id}",
                    auth=HTTPBasicAuth(username, password),
                    headers=headers,
                )

                response.raise_for_status()

                run = response.json()
                return {
                    'dag_id': dag_id,
                    'status': run['state'],
                    'start_date': run['start_date'],
                    'end_date': run['end_date'],
                    'run_id': run['dag_run_id']
                }
            except Exception as e:
                print(f"Error checking status for DAG {dag_id} run {run_id}: {e}")
                raise e


        async def upgrade_install_helm_chart(helm_repo_url, helm_chart_version):
            '''
            Upgrade or install the helm chart
            Args:
                helm_repo_url (str): The URL of the helm repository
                helm_chart_version (str): The version of the helm chart
            '''
            func = "upgrade_install_helm_chart"

            create_kube_config()

            helm_client = Client(kubeconfig=KUBE_CONFIG_FILE, executable=HELM_EXECUTABLE)

            os.environ["HELM_EXPERIMENTAL_OCI"] = "1"

            username, password, proxy_endpoint = get_ecr_login_password(AWS_REGION)

            helm_repo_registry = helm_repo_url[6:].split("/")[0]

            helm_config_filepath = f"{TEMP_FILE_PREFIX}/config.json"

            subprocess.check_call([
                HELM_EXECUTABLE, "registry", "login",
                "--username", username,
                "--password", password,
                "--registry-config", helm_config_filepath,
                helm_repo_registry,
            ])

            print(f"{func}: Helm registry login successful")

            print(f"{func}: Helm pull chart")

            helm_chart_dir = f"{TEMP_FILE_PREFIX}/helm.777r7612"

            subprocess.check_call([
                HELM_EXECUTABLE, "pull", helm_repo_url,
                "--version", helm_chart_version,
                "--registry-config", helm_config_filepath,
                "--destination", helm_chart_dir,
                "--kubeconfig", KUBE_CONFIG_FILE,
                "--untar",
            ])

            chart = await helm_client.get_chart(os.path.join(helm_chart_dir, "cypienta-chart"))

            print(chart.ref)

            revision = await helm_client.install_or_upgrade_release(
                "cypienta",
                chart,
                get_values_override(),
                atomic=False,
                namespace=CYPIENTA_NS,
                timeout="20m",
                # dry_run=True,
            )


        def get_values_override():

            # calculate the max concurrency
            # NOTE: in AWS the nodes are up scaled automatically based on the load.

            # highCpuRamNodePoolCount = min((MAX_CPU_TO_USE / 2) / HIGH_CPU_RAM_NODE_CPU, (MAX_RAM_TO_USE / 2) / HIGH_CPU_RAM_NODE_RAM)

            # totalHighCpuRamCpu = highCpuRamNodePoolCount * HIGH_CPU_RAM_NODE_CPU
            # totalHighCpuRamRam = highCpuRamNodePoolCount * HIGH_CPU_RAM_NODE_RAM

            # lambdaNodePoolCount = int(min((MAX_CPU_TO_USE - totalHighCpuRamCpu) / LAMBDA_NODE_CPU, (MAX_RAM_TO_USE - totalHighCpuRamRam) / LAMBDA_NODE_RAM))

            # totalLambdaCpu = lambdaNodePoolCount * LAMBDA_NODE_CPU
            # totalLambdaRam = lambdaNodePoolCount * LAMBDA_NODE_RAM

            # maxSchemaCpuToUse = int((totalLambdaCpu * SCHEMA_RESOURCE_PERCENTAGE) / 100)
            # maxSchemaRamToUse = int((totalLambdaRam * SCHEMA_RESOURCE_PERCENTAGE) / 100)

            # maxAggregatorCpuToUse = int((totalHighCpuRamCpu * AGGREGATOR_RESOURCE_PERCENTAGE) / 100)
            # maxAggregatorRamToUse = int((totalHighCpuRamRam * AGGREGATOR_RESOURCE_PERCENTAGE) / 100)

            # maxClusterModelCpuToUse = int((totalHighCpuRamCpu * CLUSTER_MODEL_RESOURCE_PERCENTAGE) / 100)
            # maxClusterModelRamToUse = int((totalHighCpuRamRam * CLUSTER_MODEL_RESOURCE_PERCENTAGE) / 100)

            # maxFlowModelCpuToUse = int((totalHighCpuRamCpu * FLOW_MODEL_RESOURCE_PERCENTAGE) / 100)
            # maxFlowModelRamToUse = int((totalHighCpuRamRam * FLOW_MODEL_RESOURCE_PERCENTAGE) / 100)

            # maxClusterOutCpuToUse = int((totalLambdaCpu * CLUSTER_OUT_RESOURCE_PERCENTAGE) / 100)
            # maxClusterOutRamToUse = int((totalLambdaRam * CLUSTER_OUT_RESOURCE_PERCENTAGE) / 100)

            # maxFlowOutCpuToUse = int((totalLambdaCpu * FLOW_OUT_RESOURCE_PERCENTAGE) / 100)
            # maxFlowOutRamToUse = int((totalLambdaRam * FLOW_OUT_RESOURCE_PERCENTAGE) / 100)

            # maxSchemaConcurrency = min(int(maxSchemaCpuToUse / LAMBDA_CPU_LIMIT), int(maxSchemaRamToUse / LAMBDA_MEMORY_LIMIT))
            # maxAggregatorConcurrency = min(int(maxAggregatorCpuToUse / AGGREGATOR_CPU_LIMIT), int(maxAggregatorRamToUse / AGGREGATOR_MEMORY_LIMIT))
            # maxClusterModelConcurrency = min(int(maxClusterModelCpuToUse / CLUSTER_MODEL_CPU_LIMIT), int(maxClusterModelRamToUse / CLUSTER_MODEL_MEMORY_LIMIT))
            # maxFlowModelConcurrency = min(int(maxFlowModelCpuToUse / FLOW_MODEL_CPU_LIMIT), int(maxFlowModelRamToUse / FLOW_MODEL_MEMORY_LIMIT))
            # maxClusterOutConcurrency = min(int(maxClusterOutCpuToUse / LAMBDA_CPU_LIMIT), int(maxClusterOutRamToUse / LAMBDA_MEMORY_LIMIT))
            # maxFlowOutConcurrency = min(int(maxFlowOutCpuToUse / LAMBDA_CPU_LIMIT), int(maxFlowOutRamToUse / LAMBDA_MEMORY_LIMIT))

            # set the max active tasks per dag run with the max concurrency for schema dag
            MAX_ACTIVE_TASKS_PER_SCHEMA_MATCHER_AND_CLASSIFIER_DAG_RUN = 1

            # MAX_ACTIVE_TASKS_PER_AGGREGATOR_DAG_RUN = maxAggregatorConcurrency
            # MAX_ACTIVE_RUNS_CLUSTERING_DAG = maxClusterModelConcurrency
            # MAX_ACTIVE_RUNS_SEQUENCER_DAG = maxFlowModelConcurrency

            # MAX_ACTIVE_RUNS_PROCESS_CLUSTER_OUTPUT_DAG = maxClusterOutConcurrency
            # MAX_ACTIVE_RUNS_PROCESS_SEQUENCER_OUTPUT_DAG = maxFlowOutConcurrency

            airflow_web_secret_key = secrets.token_hex(16)

            values_json = {
                "global": {
                    "storage_class": {
                        "efs": {
                            "file_system_id": EFS_FILE_SYSTEM_ID
                        }
                    },
                    "envVar": {
                        "BUCKET": BUCKET
                    },
                    "aws": {
                        "env": {
                            "AWS_DEFAULT_REGION": AWS_REGION
                        },
                        "ui_lb_name": f"ui-{CYPIENTA_NS}-{STACK_ID.split('-')[-1]}",
                        "airflow_lb_name": f"airflow-{CYPIENTA_NS}-{STACK_ID.split('-')[-1]}"
                    },
                    # "images": {
                    #     "ui": {
                    #         "web": {
                    #             "repo": UI_IMAGE_REPO,
                    #             "tag": UI_IMAGE_TAG
                    #         },
                    #         "nginx": {
                    #             "repo": UI_NGINX_IMAGE_REPO,
                    #             "tag": UI_NGINX_IMAGE_TAG
                    #         }
                    #     },
                    #     "log_collection": {
                    #         "repo": LOG_COLLECTION_IMAGE_REPO,
                    #         "tag": LOG_COLLECTION_IMAGE_TAG
                    #     }
                    # },
                    "lambda": {
                        "env": {
                            "LAMBDA_CPU_LIMIT": LAMBDA_CPU_LIMIT,
                            "LAMBDA_MEMORY_LIMIT": LAMBDA_MEMORY_LIMIT,

                            # "MAX_ALERTS_FOR_UI_UPLOAD": MAX_ALERTS_FOR_UI_UPLOAD,

                            # "AGG_NUM_WORKERS": AGG_NUM_WORKERS,
                            # "AGG_MEMORY_LIMIT_IN_GB": AGG_MEMORY_LIMIT_IN_GB,
                            # "AGG_BLOCKSIZE": AGG_BLOCKSIZE,
                            # "CHUNK_SIZE": CHUNK_SIZE,

                            # "SEQUENCER_CONTEXT_WINDOW_SIZE": SEQUENCER_CONTEXT_WINDOW_SIZE
                        }
                    },
                    "dags": {
                        "webserver_secret_key": airflow_web_secret_key,
                        "env": {
                            "LAMBDA_IMAGE": f"{LAMBDA_IMAGE_REPO}:{LAMBDA_IMAGE_TAG}",
                            # "LAMBDA_AGG_IMAGE": f"{LAMBDA_AGG_IMAGE_REPO}:{LAMBDA_AGG_IMAGE_TAG}",
                            # "CLUSTER_1_MODEL_IMAGE": f"{CLUSTER_1_MODEL_IMAGE_REPO}:{CLUSTER_1_MODEL_IMAGE_TAG}",
                            # "FLOW_MODEL_IMAGE": f"{FLOW_MODEL_IMAGE_REPO}:{FLOW_MODEL_IMAGE_TAG}",

                            "LAMBDA_CPU_LIMIT": LAMBDA_CPU_LIMIT,
                            "LAMBDA_MEMORY_LIMIT": LAMBDA_MEMORY_LIMIT,

                            # "AGGREGATOR_CPU_LIMIT": AGGREGATOR_CPU_LIMIT,
                            # "AGGREGATOR_MEMORY_LIMIT": AGGREGATOR_MEMORY_LIMIT,

                            # "CLUSTER_MODEL_CPU_LIMIT": CLUSTER_MODEL_CPU_LIMIT,
                            # "CLUSTER_MODEL_MEMORY_LIMIT": CLUSTER_MODEL_MEMORY_LIMIT,

                            # "FLOW_MODEL_CPU_LIMIT": FLOW_MODEL_CPU_LIMIT,
                            # "FLOW_MODEL_MEMORY_LIMIT": FLOW_MODEL_MEMORY_LIMIT,

                            "MAX_ACTIVE_TASKS_PER_SCHEMA_MATCHER_AND_CLASSIFIER_DAG_RUN": MAX_ACTIVE_TASKS_PER_SCHEMA_MATCHER_AND_CLASSIFIER_DAG_RUN,

                            # "MAX_ACTIVE_TASKS_PER_AGGREGATOR_DAG_RUN": MAX_ACTIVE_TASKS_PER_AGGREGATOR_DAG_RUN,
                            # "MAX_ACTIVE_RUNS_CLUSTERING_DAG": MAX_ACTIVE_RUNS_CLUSTERING_DAG,
                            # "MAX_ACTIVE_RUNS_SEQUENCER_DAG": MAX_ACTIVE_RUNS_SEQUENCER_DAG,

                            # "MAX_ACTIVE_RUNS_PROCESS_CLUSTER_OUTPUT_DAG": MAX_ACTIVE_RUNS_PROCESS_CLUSTER_OUTPUT_DAG,
                            # "MAX_ACTIVE_RUNS_PROCESS_SEQUENCER_OUTPUT_DAG": MAX_ACTIVE_RUNS_PROCESS_SEQUENCER_OUTPUT_DAG
                        }
                    },
                },
                # "ui": {
                #     "ingress": {
                #         "annotations": {
                #             "alb.ingress.kubernetes.io/load-balancer-name": f"ui-{CYPIENTA_NS}-{STACK_ID.split('-')[-1]}"
                #         }
                #     }
                # },
                "airflow": {
                    "ingress": {
                        "web": {
                            "annotations": {
                                "alb.ingress.kubernetes.io/load-balancer-name": f"airflow-{CYPIENTA_NS}-{STACK_ID.split('-')[-1]}"
                            }
                        }
                    },
                    "images": {
                        "airflow": {
                            "repository": AIRFLOW_IMAGE_REPO,
                            "tag": AIRFLOW_IMAGE_TAG
                        }
                    }
                }
            }

            return values_json


        def create_kube_config():
            global CLUSTER_NAME
            global LAMBDA_ROLE_ARN

            print(f"CLUSTER_NAME: {CLUSTER_NAME}")
            print(f"LAMBDA_ROLE_ARN: {LAMBDA_ROLE_ARN}")

            eks_token = get_token(
                cluster_name=CLUSTER_NAME,
                # role_arn=LAMBDA_ROLE_ARN
            )

            print(f"EKS token: {eks_token}")

            eks_token = eks_token["status"]["token"]

            print(f"EKS token: {eks_token}")

            s = boto3.Session()
            eks_client = s.client("eks")

            # get cluster details
            cluster = eks_client.describe_cluster(name=CLUSTER_NAME)
            cluster_cert = cluster["cluster"]["certificateAuthority"]["data"]
            cluster_ep = cluster["cluster"]["endpoint"]

            print(f"Cluster cert: {cluster_cert}")
            print(f"Cluster ep: {cluster_ep}")

            # build the cluster config hash
            cluster_config = {
                "apiVersion": "v1",
                "kind": "Config",
                "clusters": [
                    {
                        "cluster": {
                            "server": str(cluster_ep),
                            "certificate-authority-data": str(cluster_cert)
                        },
                        "name": "kubernetes"
                    }
                ],
                "contexts": [
                    {
                        "context": {
                            "cluster": "kubernetes",
                            "user": "aws",
                            "namespace": "default"
                        },
                        "name": "aws"
                    }
                ],
                "current-context": "aws",
                "preferences": {},
                "users": [
                    {
                        "name": "aws",
                        "user": {
                            "token": eks_token
                        }
                    }
                ]
            }

            # Write in YAML.
            config_text = yaml.dump(cluster_config, default_flow_style=False)
            open(KUBE_CONFIG_FILE, "w").write(config_text)


        def get_load_balancer_dns_name(load_balancer_name, region_name):
            client = boto3.client('elbv2', region_name=region_name)
            response = client.describe_load_balancers(Names=[load_balancer_name])
            load_balancers = response.get('LoadBalancers', [])
            if load_balancers:
                return load_balancers[0]['DNSName']
            else:
                return None


        def get_ecr_login_password(region, registry_id=None):
            if ENV is not None and ENV == "prod":
                region = "us-east-1"
            ecr = boto3.client('ecr', region_name=region)
            if registry_id:
                response = ecr.get_authorization_token(registryIds=[registry_id])
            else:
                response = ecr.get_authorization_token()
            auth_data = response['authorizationData'][0]
            token = auth_data['authorizationToken']
            username, password = base64.b64decode(token).decode().split(':')
            proxy_endpoint = auth_data['proxyEndpoint']
            return username, password, proxy_endpoint


        def file_hash(path, algo='sha256'):
            h = hashlib.new(algo)
            with open(path, 'rb') as f:
                while chunk := f.read(8192):
                    h.update(chunk)
            return h.hexdigest()


        def clear_temp_dir():
            shutil.rmtree(TEMP_FILE_PREFIX)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          ENV: dev
          AUTO_UPDATE_HELM_CHART: !Ref AutoUpdateHelmChart
          HELM_EXPERIMENTAL_OCI: 1
          OWNER: cypienta
          REPO: AWS
          BRANCH: v0.1.0-helm-mini-test
          BUCKET: !Ref Bucket
          HELM_GITHUB_REPO: helm.json

          STACK_NAME: !Ref AWS::StackName
          STACK_ID: !Ref AWS::StackId
          CLUSTER_NAME: !Ref ControlPlane
          LAMBDA_ROLE_ARN: !GetAtt LambdaRole.Arn

          AIRFLOW_LB_NAME: !Sub
          - airflow-cypienta-${StackSuffix}
          - StackSuffix:
              Fn::Select:
              - 4
              - Fn::Split:
                - '-'
                - Fn::Select:
                  - 2
                  - Fn::Split:
                    - /
                    - !Ref AWS::StackId
          AIRFLOW_USERNAME: !FindInMap
          - Lambda
          - AIRFLOW-USERNAME
          - Value
          AIRFLOW_PASSWORD: !FindInMap
          - Lambda
          - AIRFLOW-PASSWORD
          - Value
          CYPIENTA_NS: !FindInMap
          - Lambda
          - CYPIENTA-NS
          - Value
          EFS_FILE_SYSTEM_ID: !Ref EFSFileSystem

          LAMBDA_CPU_LIMIT: '15'
          LAMBDA_MEMORY_LIMIT: '63'

          # AGGREGATOR_CPU_LIMIT: '2'
          # AGGREGATOR_MEMORY_LIMIT: '14'

          # CLUSTER_MODEL_CPU_LIMIT: '4'
          # CLUSTER_MODEL_MEMORY_LIMIT: '15'

          # FLOW_MODEL_CPU_LIMIT: '4'
          # FLOW_MODEL_MEMORY_LIMIT: '15'

          LAMBDA_IMAGE_REPO: !FindInMap [Images, LambdaContainerImage, ImageUri]
          LAMBDA_IMAGE_TAG: !FindInMap [Images, LambdaContainerImage, Version]
          # LAMBDA_AGG_IMAGE_REPO: !FindInMap [Images, LambdaAggContainerImage, ImageUri]
          # LAMBDA_AGG_IMAGE_TAG: !FindInMap [Images, LambdaAggContainerImage, Version]
          # CLUSTER_1_MODEL_IMAGE_REPO: !FindInMap [Images, ClusterModelPart1ContainerImage,
          #   ImageUri]
          # CLUSTER_1_MODEL_IMAGE_TAG: !FindInMap [Images, ClusterModelPart1ContainerImage,
          #   Version]
          # FLOW_MODEL_IMAGE_REPO: !FindInMap [Images, FlowModelContainerImage, ImageUri]
          # FLOW_MODEL_IMAGE_TAG: !FindInMap [Images, FlowModelContainerImage, Version]

          # UI_IMAGE_REPO: !FindInMap [Images, WebContainerImage, ImageUri]
          # UI_IMAGE_TAG: !FindInMap [Images, WebContainerImage, Version]

          # UI_NGINX_IMAGE_REPO: !FindInMap [Images, NginxContainerImage, ImageUri]
          # UI_NGINX_IMAGE_TAG: !FindInMap [Images, NginxContainerImage, Version]

          AIRFLOW_IMAGE_REPO: !FindInMap [Images, AirflowContainerImage, ImageUri]
          AIRFLOW_IMAGE_TAG: !FindInMap [Images, AirflowContainerImage, Version]

          # LOG_COLLECTION_IMAGE_REPO: !FindInMap [Images, LogCollectionContainerImage,
          #   ImageUri]
          # LOG_COLLECTION_IMAGE_TAG: !FindInMap [Images, LogCollectionContainerImage,
          #   Version]

          # MAX_ALERTS_FOR_UI_UPLOAD: !Ref MaxAlertsForUiUpload

          # AGG_NUM_WORKERS: !Ref AggNumWorkers
          # AGG_MEMORY_LIMIT_IN_GB: !Ref AggMemoryLimitInGb
          # AGG_BLOCKSIZE: !Ref AggBlockSize
          # CHUNK_SIZE: !Ref ChunkSize

          # SEQUENCER_CONTEXT_WINDOW_SIZE: !Ref SequencerContextWindowSize

          # MAX_CPU_TO_USE: !Ref MaxCpuToUse
          # MAX_RAM_TO_USE: !Ref MaxRamToUse

          # LAMBDA_NODE_CPU: '4'
          # LAMBDA_NODE_RAM: '32'

          # HIGH_CPU_RAM_NODE_CPU: '8'
          # HIGH_CPU_RAM_NODE_RAM: '32'

          # SCHEMA_RESOURCE_PERCENTAGE: !Ref SchemaResourcePercentage

          # AGGREGATOR_RESOURCE_PERCENTAGE: !Ref AggregatorResourcePercentage

          # CLUSTER_MODEL_RESOURCE_PERCENTAGE: !Ref ClusterModelResourcePercentage

          # FLOW_MODEL_RESOURCE_PERCENTAGE: !Ref FlowModelResourcePercentage

          # CLUSTER_OUT_RESOURCE_PERCENTAGE: !Ref ClusterOutResourcePercentage

          # FLOW_OUT_RESOURCE_PERCENTAGE: !Ref FlowOutResourcePercentage

      Events:
        Schedule1:
          Type: Schedule
          Properties:
            Schedule: rate(2 minutes)

  autoUpdateStackLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${autoUpdateStack}

  autoUpdateStackLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref autoUpdateStack
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  # EFS

  EFSFileSystem:
    Type: AWS::EFS::FileSystem
    Properties:
      Encrypted: true
      PerformanceMode: generalPurpose
      FileSystemTags:
      - Key: Name
        Value: !Ref AWS::StackName

  EFSMountTarget1:
    Type: AWS::EFS::MountTarget
    Properties:
      FileSystemId: !Ref EFSFileSystem
      SubnetId: !Ref SubnetPrivate1
      SecurityGroups:
      - !Ref ClusterSharedNodeSecurityGroup

  EFSMountTarget2:
    Type: AWS::EFS::MountTarget
    Properties:
      FileSystemId: !Ref EFSFileSystem
      SubnetId: !Ref SubnetPrivate2
      SecurityGroups:
      - !Ref ClusterSharedNodeSecurityGroup

  EFSMountTarget3:
    Type: AWS::EFS::MountTarget
    Properties:
      FileSystemId: !Ref EFSFileSystem
      SubnetId: !Ref SubnetPrivate3
      SecurityGroups:
      - !Ref ClusterSharedNodeSecurityGroup

  # EKS Cluster

  ClusterSharedNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Communication between all nodes in the cluster
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/ClusterSharedNodeSecurityGroup
      VpcId:
        Ref: VPC

  ControlPlane:
    Type: AWS::EKS::Cluster
    Properties:
      AccessConfig:
        AuthenticationMode: API_AND_CONFIG_MAP
        BootstrapClusterCreatorAdminPermissions: true
      BootstrapSelfManagedAddons: false
      ComputeConfig:
        Enabled: true
        NodePools:
        - general-purpose
        - system
        NodeRoleArn:
          Fn::GetAtt:
          - EKSNodeRole
          - Arn
      KubernetesNetworkConfig:
        ElasticLoadBalancing:
          Enabled: true
      Name: !Sub
      - cypienta-${StackSuffix}
      - StackSuffix:
          Fn::Select:
          - 2
          - Fn::Split:
            - /
            - !Ref AWS::StackId
      ResourcesVpcConfig:
        EndpointPrivateAccess: false
        EndpointPublicAccess: true
        SecurityGroupIds:
        - Ref: ControlPlaneSecurityGroup
        SubnetIds:
        - Ref: SubnetPrivate3
        - Ref: SubnetPrivate2
        - Ref: SubnetPrivate1
      RoleArn:
        Fn::GetAtt:
        - EKSClusterRole
        - Arn
      StorageConfig:
        BlockStorage:
          Enabled: true
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/ControlPlane
      Version: '1.32'

  LambdaRoleEKSAccessEntry:
    Type: AWS::EKS::AccessEntry
    Properties:
      ClusterName: !Ref ControlPlane
      PrincipalArn: !GetAtt LambdaRole.Arn
      AccessPolicies:
      - PolicyArn: arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy
        AccessScope:
          Type: cluster
      Type: STANDARD

  EksPodIdentityAssociation:
    Type: AWS::EKS::PodIdentityAssociation
    Properties:
      ClusterName: !Ref ControlPlane
      Namespace: default
      ServiceAccount: default
      RoleArn: !GetAtt EKSPodRole.Arn

  EksPodIdentityAssociationCypientaNS:
    Type: AWS::EKS::PodIdentityAssociation
    Properties:
      ClusterName: !Ref ControlPlane
      Namespace: simple
      ServiceAccount: default
      RoleArn: !GetAtt EKSPodRole.Arn

  # EKS addons

  EFSCsiDriverAddon:
    Type: AWS::EKS::Addon
    Properties:
      AddonName: aws-efs-csi-driver
      ClusterName: !Ref ControlPlane
      PodIdentityAssociations:
      - RoleArn: !GetAtt EKSPodRole.Arn
        ServiceAccount: efs-csi-controller-sa
      ResolveConflicts: OVERWRITE

  EKSMetricsServerAddon:
    Type: AWS::EKS::Addon
    Properties:
      AddonName: metrics-server
      ClusterName: !Ref ControlPlane
      ResolveConflicts: OVERWRITE

  # Networking

  ControlPlaneSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Communication between the control plane and worker nodegroups
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/ControlPlaneSecurityGroup
      VpcId:
        Ref: VPC

  IngressDefaultClusterToNodeSG:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow managed and unmanaged nodes to communicate with each other
        (all ports)
      FromPort: 0
      GroupId:
        Ref: ClusterSharedNodeSecurityGroup
      IpProtocol: '-1'
      SourceSecurityGroupId:
        Fn::GetAtt:
        - ControlPlane
        - ClusterSecurityGroupId
      ToPort: 65535

  IngressInterNodeGroupSG:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow nodes to communicate with each other (all ports)
      FromPort: 0
      GroupId:
        Ref: ClusterSharedNodeSecurityGroup
      IpProtocol: '-1'
      SourceSecurityGroupId:
        Ref: ClusterSharedNodeSecurityGroup
      ToPort: 65535

  IngressNodeToDefaultClusterSG:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow unmanaged nodes to communicate with control plane (all ports)
      FromPort: 0
      GroupId:
        Fn::GetAtt:
        - ControlPlane
        - ClusterSecurityGroupId
      IpProtocol: '-1'
      SourceSecurityGroupId:
        Ref: ClusterSharedNodeSecurityGroup
      ToPort: 65535

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/InternetGateway

  # NAT gateway

  NATGateway:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId:
        Fn::GetAtt:
        - NATIP
        - AllocationId
      SubnetId:
        Ref: SubnetPublic3
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/NATGateway

  NATIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/NATIP

  NATPrivateSubnetRoute1:
    Type: AWS::EC2::Route
    Properties:
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId:
        Ref: NATGateway
      RouteTableId:
        Ref: PrivateRouteTable1
  NATPrivateSubnetRoute2:
    Type: AWS::EC2::Route
    Properties:
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId:
        Ref: NATGateway
      RouteTableId:
        Ref: PrivateRouteTable2
  NATPrivateSubnetRoute3:
    Type: AWS::EC2::Route
    Properties:
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId:
        Ref: NATGateway
      RouteTableId:
        Ref: PrivateRouteTable3

  # Route tables

  PrivateRouteTable1:
    Type: AWS::EC2::RouteTable
    Properties:
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/PrivateRouteTable1
      VpcId:
        Ref: VPC
  PrivateRouteTable2:
    Type: AWS::EC2::RouteTable
    Properties:
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/PrivateRouteTable2
      VpcId:
        Ref: VPC
  PrivateRouteTable3:
    Type: AWS::EC2::RouteTable
    Properties:
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/PrivateRouteTable3
      VpcId:
        Ref: VPC
  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/PublicRouteTable
      VpcId:
        Ref: VPC
  PublicSubnetRoute:
    Type: AWS::EC2::Route
    Properties:
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId:
        Ref: InternetGateway
      RouteTableId:
        Ref: PublicRouteTable
    DependsOn:
    - VPCGatewayAttachment

  # Route table associations

  RouteTableAssociationPrivate1:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId:
        Ref: PrivateRouteTable1
      SubnetId:
        Ref: SubnetPrivate1
  RouteTableAssociationPrivate2:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId:
        Ref: PrivateRouteTable2
      SubnetId:
        Ref: SubnetPrivate2
  RouteTableAssociationPrivate3:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId:
        Ref: PrivateRouteTable3
      SubnetId:
        Ref: SubnetPrivate3
  RouteTableAssociationPublic1:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId:
        Ref: PublicRouteTable
      SubnetId:
        Ref: SubnetPublic1
  RouteTableAssociationPublic2:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId:
        Ref: PublicRouteTable
      SubnetId:
        Ref: SubnetPublic2
  RouteTableAssociationPublic3:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId:
        Ref: PublicRouteTable
      SubnetId:
        Ref: SubnetPublic3

  # Subnets

  SubnetPrivate1:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Select [0, !GetAZs ""]
      CidrBlock: 192.168.160.0/19
      Tags:
      - Key: kubernetes.io/role/internal-elb
        Value: '1'
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/SubnetPrivate1
      VpcId:
        Ref: VPC
  SubnetPrivate2:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Select [1, !GetAZs ""]
      CidrBlock: 192.168.128.0/19
      Tags:
      - Key: kubernetes.io/role/internal-elb
        Value: '1'
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/SubnetPrivate2
      VpcId:
        Ref: VPC
  SubnetPrivate3:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Select [2, !GetAZs ""]
      CidrBlock: 192.168.96.0/19
      Tags:
      - Key: kubernetes.io/role/internal-elb
        Value: '1'
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/SubnetPrivate3
      VpcId:
        Ref: VPC
  SubnetPublic1:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Select [0, !GetAZs ""]
      CidrBlock: 192.168.64.0/19
      MapPublicIpOnLaunch: true
      Tags:
      - Key: kubernetes.io/role/elb
        Value: '1'
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/SubnetPublic1
      VpcId:
        Ref: VPC
  SubnetPublic2:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Select [1, !GetAZs ""]
      CidrBlock: 192.168.32.0/19
      MapPublicIpOnLaunch: true
      Tags:
      - Key: kubernetes.io/role/elb
        Value: '1'
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/SubnetPublic2
      VpcId:
        Ref: VPC
  SubnetPublic3:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Select [2, !GetAZs ""]
      CidrBlock: 192.168.0.0/19
      MapPublicIpOnLaunch: true
      Tags:
      - Key: kubernetes.io/role/elb
        Value: '1'
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/SubnetPublic3
      VpcId:
        Ref: VPC

  # VPC

  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 192.168.0.0/16
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags:
      - Key: Name
        Value:
          Fn::Sub: ${AWS::StackName}/VPC
  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      InternetGatewayId:
        Ref: InternetGateway
      VpcId:
        Ref: VPC

Outputs:
  CypientaBucket:
    Description: The bucket created
    Value: !Ref Bucket

  CypientaEKSCluster:
    Description: The EKS cluster created
    Value: !Ref ControlPlane

  CypientaPodRole:
    Description: The pod role created
    Value: !GetAtt EKSPodRole.Arn

  CypientaEKSNamespace:
    Description: The namespace to use for cypienta mini pipeline
    Value: simple

  CypientaEFS:
    Description: The EFS created
    Value: !Ref EFSFileSystem
