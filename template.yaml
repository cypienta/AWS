Transform: AWS::Serverless-2016-10-31
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:

    - Label:
        default: UI admin credentials
      Parameters:
      - SuperuserEmail
      - SuperuserUsername
      - SuperuserPassword

    - Label:
        default: Database credentials
      Parameters:
      - DbUsername
      - DbPassword

    - Label:
        default: Load Balancer Configuration
      Parameters:
      - UILoadBalancerName
      - DbLoadBalancerName
      - AirflowLoadBalancerName

    - Label:
        default: Input processing Parameters
      Parameters:
      - ChunkSize
      - FlowInputMaxClusters
      - TechLookupLimit

    - Label:
        default: UI lambda function Parameters
      Parameters:
      - EventThreshold
      - TacticThreshold
      - MaxClusterCount
      - MaxFlowCount

    - Label:
        default: ECS AMIs
      Parameters:
      - CpuECSOptimizedAMI
      - DbECSOptimizedAMI
      - GpuECSOptimizedAMI

    - Label:
        default: UI ECS Configuration
      Parameters:
      - Cpu
      - Memory
      - UIECSClusterInstanceType

    - Label:
        default: Database ECS Configuration
      Parameters:
      - DbECSClusterInstanceType

    - Label:
        default: Technique model ECS Configuration
      Parameters:
      - TechECSClusterInstanceType
      - TechTaskMemory

    - Label:
        default: Cluster model Part 1 ECS Configuration
      Parameters:
      - ClusterPart1ECSClusterInstanceType
      - ClusterPart1TaskMemory

    - Label:
        default: Cluster model Part 2 ECS Configuration
      Parameters:
      - ClusterPart2ECSClusterInstanceType
      - ClusterPart2TaskMemory

    - Label:
        default: Flow model ECS Configuration
      Parameters:
      - FlowECSClusterInstanceType
      - FlowTaskMemory

    - Label:
        default: Lambda function ECS Configuration
      Parameters:
      - LambdaECSClusterInstanceType
      - LambdaTaskMemory

    - Label:
        default: Airflow ECS Configuration
      Parameters:
      - AirflowECSClusterInstanceType
      - AirflowTaskMemory

    - Label:
        default: Auto Scaling Configuration
      Parameters:
      - ClusterAutoScalingMinSize
      - ClusterAutoScalingMaxSize

    - Label:
        default: VPC Configuration
      Parameters:
      - VpcCidr
      - Subnet1Cidr
      - Subnet2Cidr

Parameters:

  ChunkSize:
    Type: Number
    Description: Size of single chunk of input to be processed at a time for an input
      file
    Default: 20000

  MaxClusterCount:
    Type: Number
    Description: Maximum number of clusters to keep on UI
    Default: 100000

  MaxFlowCount:
    Type: Number
    Description: Maximum number of flows to keep on UI
    Default: 100000

  TechLookupLimit:
    Type: Number
    Description: Maximum number of alert to techniques to cache
    Default: 100000

  DbUsername:
    Type: String
    Description: Database Username for lambda functions
    Default: lambda
    MinLength: 5

  DbPassword:
    Type: String
    Description: Database Passwrod for lambda functions
    Default: lambda
    MinLength: 5

  FlowInputMaxClusters:
    Type: Number
    Description: Number of clusters as input to Flow detection model to be processed
      at a time for an input file
    Default: 5000

  EventThreshold:
    Type: Number
    Default: 2
    Description: Number of events present in the cluster or flow to create campaign
      on UI. Minimum allowed value '1'.
    MinValue: 1

  TacticThreshold:
    Type: Number
    Default: 0
    Description: Number of tactics present in an event to be part of the campaign.
      Minimum allowed value '0'.
    MinValue: 0

  UIECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for EC2
    Default: t3a.large
    AllowedValues:
    - t2.large
    - t3.large
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  DbECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Database EC2
    Default: m5.large
    AllowedValues:
    - t2.large
    - t3.large
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  TechECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Technique model EC2. Choose a GPU instance
      type
    Default: g4dn.xlarge
    AllowedValues:
    - g4dn.xlarge
    - g4dn.2xlarge
    - g4dn.4xlarge
    - g4dn.8xlarge
    - g4dn.12xlarge
    - g4dn.16xlarge
    - p5.48xlarge
    - p3.2xlarge
    - p3.8xlarge
    - p3.16xlarge
    - p2.xlarge
    - p2.8xlarge
    - p2.16xlarge
    - g5.xlarge
    - g5.2xlarge
    - g5.4xlarge
    - g5.8xlarge
    - g5.12xlarge
    - g5.16xlarge
    - g5.24xlarge
    - g5.48xlarge

  ClusterPart1ECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Cluster model part 1 EC2. Choose a
      GPU instance type
    Default: g4dn.xlarge
    AllowedValues:
    - g4dn.xlarge
    - g4dn.2xlarge
    - g4dn.4xlarge
    - g4dn.8xlarge
    - g4dn.12xlarge
    - g4dn.16xlarge
    - p5.48xlarge
    - p3.2xlarge
    - p3.8xlarge
    - p3.16xlarge
    - p2.xlarge
    - p2.8xlarge
    - p2.16xlarge
    - g5.xlarge
    - g5.2xlarge
    - g5.4xlarge
    - g5.8xlarge
    - g5.12xlarge
    - g5.16xlarge
    - g5.24xlarge
    - g5.48xlarge

  ClusterPart2ECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for cluster model part 2 EC2. Choose a
      GPU instance type
    Default: c5.xlarge
    AllowedValues:
    - c5.large
    - c5.xlarge
    - c5.2xlarge
    - c5.4xlarge
    - c5.9xlarge
    - c5.12xlarge
    - c5.18xlarge
    - c5.24xlarge
    - c4.large
    - c4.xlarge
    - c4.2xlarge
    - c4.4xlarge
    - c4.8xlarge

  FlowECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Technique model EC2. Choose a GPU instance
      type
    Default: c5.xlarge
    AllowedValues:
    - c5.large
    - c5.xlarge
    - c5.2xlarge
    - c5.4xlarge
    - c5.9xlarge
    - c5.12xlarge
    - c5.18xlarge
    - c5.24xlarge
    - c4.large
    - c4.xlarge
    - c4.2xlarge
    - c4.4xlarge
    - c4.8xlarge

  LambdaECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for EC2
    Default: t3a.large
    AllowedValues:
    - t2.micro
    - t2.small
    - t2.medium
    - t2.large
    - t3.micro
    - t3.small
    - t3.medium
    - t3.large
    - t3a.micro
    - t3a.small
    - t3a.medium
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.medium
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  AirflowECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Airflow.
    Default: t3.xlarge
    AllowedValues:
    - t2.xlarge
    - t2.2xlarge
    - t3.xlarge
    - t3.2xlarge
    - t3a.xlarge
    - t3a.2xlarge
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.medium
    - m6g.xlarge
    - m6g.2xlarge

  Cpu:
    Description: Number of CPU units used by the UI and Database task. 1 vCPU = 1024
    Type: String
    Default: 2048
    AllowedValues:
    - 256
    - 512
    - 1024
    - 2048
    - 4096
    - 8192
    - 16384

  Memory:
    Description: Amount of memory (in MiB) used by the UI and Database task. 1 GB
      = 1024
    Type: String
    Default: 7168
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  TechTaskMemory:
    Description: Amount of memory (in MiB) used by the technique model task. 1 GB
      = 1024
    Type: String
    Default: 10240
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterPart1TaskMemory:
    Description: Amount of memory (in MiB) used by the cluster model part 1 task.
      1 GB = 1024
    Type: String
    Default: 10240
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterPart2TaskMemory:
    Description: Amount of memory (in MiB) used by the cluster model part 2 task.
      1 GB = 1024
    Type: String
    Default: 5120
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  FlowTaskMemory:
    Description: Amount of memory (in MiB) used by the flow model task. 1 GB = 1024
    Type: String
    Default: 5120
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  LambdaTaskMemory:
    Description: Amount of memory (in MiB) used by the lambda function task. 1 GB
      = 1024
    Type: String
    Default: 5120
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  AirflowTaskMemory:
    Description: Amount of memory (in MiB) used by the airflow. 1 GB = 1024
    Type: String
    Default: 15360
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  SuperuserEmail:
    Description: Email of superuser
    Type: String
    Default: admin@admin.com
    AllowedPattern: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$
    ConstraintDescription: Must be a valid email address

  SuperuserUsername:
    Description: Superuser username. Minimum length of 3.
    Type: String
    Default: maestro
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  SuperuserPassword:
    Description: Superuser password. Minimum length of 8
    Type: String
    Default: changemenow
    MinLength: 8
    ConstraintDescription: Must be minimum length of 8

  VpcCidr:
    Description: The CIDR block for the VPC
    Type: String
    Default: 10.0.0.0/16
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  Subnet1Cidr:
    Description: The CIDR block for the first public subnet
    Type: String
    Default: 10.0.0.0/20
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  Subnet2Cidr:
    Description: The CIDR block for the second public subnet
    Type: String
    Default: 10.0.16.0/20
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  UILoadBalancerName:
    Description: Name of UI Load balancer
    Type: String
    Default: cypienta-ui
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  DbLoadBalancerName:
    Description: Name of Database Load balancer
    Type: String
    Default: cypienta-db
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  AirflowLoadBalancerName:
    Description: Name of airflow Load balancer
    Type: String
    Default: cypienta-airflow
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  ClusterAutoScalingMinSize:
    Description: The minimum size of the auto scaling group for the ECS cluster
    Type: String
    Default: 0
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterAutoScalingMaxSize:
    Description: The maximum size of the auto scaling group for the ECS cluster
    Type: String
    Default: 5
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  CpuECSOptimizedAMI:
    Description: AMI ID for UI task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/recommended/image_id

  DbECSOptimizedAMI:
    Description: AMI ID for DB task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux/recommended/image_id

  GpuECSOptimizedAMI:
    Description: AMI ID for Database task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/gpu/recommended/image_id

Mappings:
  Images:
    LambdaContainerImage: 
      709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-lambda-function:v0.9.1.3
    AirflowContainerImage: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-airflow:v0.9.1.3
    TechModelContainerImage: 
      709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-technique-detector:v0.9.1.1
    ClusterModelPart1ContainerImage: 
      709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-cluster-part-1:v0.9.1.1
    ClusterModelPart2ContainerImage: 
      709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-cluster-part-2:v0.9.1.1
    FlowModelContainerImage: 
      709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-flow-detector:v0.9.1.1
    WebContainerImage: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-ui:v0.9.1.3
    NginxContainerImage: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-ui-nginx:v0.9.1.1
  RegionMap:
    af-south-1:
      lambdaLayer: arn:aws:lambda:af-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-3:336392948345:layer:AWSSDKPandas-Python311:12
    ap-south-1:
      lambdaLayer: arn:aws:lambda:ap-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ca-central-1:
      lambdaLayer: arn:aws:lambda:ca-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-central-1:
      lambdaLayer: arn:aws:lambda:eu-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-north-1:
      lambdaLayer: arn:aws:lambda:eu-north-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-1:
      lambdaLayer: arn:aws:lambda:eu-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-2:
      lambdaLayer: arn:aws:lambda:eu-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-3:
      lambdaLayer: arn:aws:lambda:eu-west-3:336392948345:layer:AWSSDKPandas-Python311:12
    sa-east-1:
      lambdaLayer: arn:aws:lambda:sa-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-1:
      lambdaLayer: arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-2:
      lambdaLayer: arn:aws:lambda:us-east-2:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-1:
      lambdaLayer: arn:aws:lambda:us-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-2:
      lambdaLayer: arn:aws:lambda:us-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-east-1:
      lambdaLayer: arn:aws:lambda:ap-east-1:839552336658:layer:AWSSDKPandas-Python311:14
    ap-south-2:
      lambdaLayer: arn:aws:lambda:ap-south-2:246107603503:layer:AWSSDKPandas-Python311:13
    ap-southeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-3:258944054355:layer:AWSSDKPandas-Python311:14
    ap-southeast-4:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-4:945386623051:layer:AWSSDKPandas-Python311:13
    eu-central-2:
      lambdaLayer: arn:aws:lambda:eu-central-2:956415814219:layer:AWSSDKPandas-Python311:13
    eu-south-1:
      lambdaLayer: arn:aws:lambda:eu-south-1:774444163449:layer:AWSSDKPandas-Python311:14
    eu-south-2:
      lambdaLayer: arn:aws:lambda:eu-south-2:982086096842:layer:AWSSDKPandas-Python311:13
    il-central-1:
      lambdaLayer: arn:aws:lambda:il-central-1:263840725265:layer:AWSSDKPandas-Python311:12
    me-central-1:
      lambdaLayer: arn:aws:lambda:me-central-1:593833071574:layer:AWSSDKPandas-Python311:12
    me-south-1:
      lambdaLayer: arn:aws:lambda:me-south-1:938046470361:layer:AWSSDKPandas-Python311:14
    cn-north-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-north-1:406640652441:layer:AWSSDKPandas-Python311:10
    cn-northwest-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-northwest-1:406640652441:layer:AWSSDKPandas-Python311:10

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DependsOn:
    - LambdaRole
    - EcsInstanceExecutionRole
    - EcsInstanceProfile
    Properties:
      BucketName: !Sub
      - cypienta-${StackSuffix}
      - StackSuffix:
          Fn::Select:
          - 2
          - Fn::Split:
            - /
            - !Ref AWS::StackId

  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-AllowStepFunctionExecution"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - states:StartExecution
            Resource: arn:aws:states:*
      - PolicyName: !Sub "${AWS::StackName}-AllowECS"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ecs:*
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonS3FullAccess

  createMapping:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_create_mapping
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: createMapping
      InlineCode: |
        '''
        Create mapping for uploaded file

        Input: input.json, input.xml, input.csv
        '''

        import os
        import subprocess
        import sys
        import urllib
        import json
        import glob
        import copy
        from dateutil import parser
        import itertools

        import boto3
        import pandas as pd
        import requests

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        os.makedirs("/tmp/nltk_download", exist_ok=True)
        subprocess.call('pip install xmltodict -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import xmltodict

        UI_LB_URL = os.getenv("ui_lb_url")
        # UI_LB_URL = "cypienta-ui-8279-1336236658.us-east-1.elb.amazonaws.com"
        # UI_LB_URL = "localhost"

        UI_USERNAME = os.getenv("ui_username")
        # UI_USERNAME = "maestro"

        UI_PASSWORD = os.getenv("ui_password")
        # UI_PASSWORD = "changemenow"
        # UI_PASSWORD = "changeme"

        missing_variables = []
        if UI_LB_URL is None:
            missing_variables.append("ui_lb_url")
        if UI_USERNAME is None:
            missing_variables.append("ui_username")
        if UI_PASSWORD is None:
            missing_variables.append("ui_password")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        HOST = f"http://{UI_LB_URL}:8000/"

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_INPUT_FILENAME

            func = "lambda_handler"

            event = json.loads(event["body"])

            bucket, input_filename = parse_s3_path(event["S3InputPath"])

            # bucket = event["Records"][0]["s3"]["bucket"]["name"]
            # bucket = "airflow-cy-test"
            try:
                # input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")
                # input_filename = "uploads/1/pptest.csv"
                # input_filename = "uploads/1/pptest.json"
                # input_filename = "uploads/1/endpoint_alerts_converted.json"
                # input_filename = "uploads/1/endpoint_alerts.xml"

                extension = os.path.splitext(input_filename)[-1]
                TEMP_INPUT_FILENAME = f"/tmp/input.{extension}"

                print(f"{func}: Download input file: {bucket}/{input_filename}")
                try:
                    S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                    print(f"{func}: Input file download completed")

                except Exception as e:
                    print(f"{func}: Failed to download input file")
                    raise e

                if extension == ".csv":
                    mapping_resp = process_csv()

                elif extension == ".json":
                    mapping_resp = process_json()

                elif extension == ".xml":
                    mapping_resp = process_xml()

                else:
                    lambda_response = {
                        "error": f"Unable to process the current file with extension: {extension}"
                    }
                    return lambda_response

                # ui_cookies, ui_headers, ui_session = get_ui_session()
                # api_push_mapping_to_ui(ui_cookies, ui_headers, ui_session, mapping_resp)

                if "error" in mapping_resp:
                    lambda_response = {
                        "error": mapping_resp["error"]
                    }
                    return lambda_response

                return mapping_resp

            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                # raise ex
                lambda_response = {
                    "error": "Failed to process the input file"
                }
                return lambda_response


        def process_csv():
            '''
            map data from csv file to internal fields
            Returns:
                mapping data for UI
            '''
            global TEMP_INPUT_FILENAME
            func = "process_csv"

            try:
                og_user_data = pd.read_csv(TEMP_INPUT_FILENAME)
            except Exception as e:
                lambda_response = {
                    "error": "Invalid CSV file. Please check your file format and try again."
                }
                return lambda_response

            try:
                user_data = json.loads(og_user_data.iloc[0].to_json())
            except Exception as e:
                lambda_response = {
                    "error": "Unable to fetch first alert from the file. Please check you have atleast 1 input alert in the file."
                }
                return lambda_response

            user_fields = user_data.keys()
            print(f"{func}: user fields: {user_fields}")

            sample_values = {}

            i = 0
            while i < 3:
                if len(og_user_data) <= i:
                    break
                fetch_user_data_values = json.loads(og_user_data.iloc[i].to_json())
                for field in user_fields:
                    if field not in sample_values:
                        sample_values[field] = []
                    if field not in fetch_user_data_values or fetch_user_data_values[field] is None:
                        sample_values[field].append("")
                    else:
                        sample_values[field].append(fetch_user_data_values[field])
                i += 1

            mapper = get_mapper()

            transformed_event, mapping, node_feature_associations = map_to_internal(user_data, mapper)

            print(f"{func}: transformed event: {transformed_event}")
            print(f"{func}: mapping: {mapping}")
            print(f"{func}: node_feature_associations: {node_feature_associations}")

            priorities = get_priorities(mapping)
            print(f"{func}: priorities: {priorities}")

            mapping_resp = {
                "mapping": {
                    "mapping": mapping,
                    "priorities": priorities,
                    "node_feature": node_feature_associations,
                    "sample_data": transformed_event,
                    "sample_values": sample_values,
                }
            }

            return mapping_resp


        def process_json():
            '''
            map data from json file to internal fields
            Returns:
                mapping data for UI
            '''
            global TEMP_INPUT_FILENAME
            func = "process_json"

            try:
                og_user_data = json.load(open(TEMP_INPUT_FILENAME, "r"))
            except Exception as e:
                lambda_response = {
                    "error": "Invalid JSON file. Please check your file format and try again."
                }
                return lambda_response

            try:
                user_data = og_user_data[0]
            except Exception as e:
                lambda_response = {
                    "error": "Unable to fetch first alert from the file. Please check you have atleast 1 input alert in the file."
                }
                return lambda_response

            user_data = flatten_json(user_data)

            user_fields = user_data.keys()
            print(f"{func}: user fields: {user_fields}")

            sample_values = {}

            i = 0
            while i < 3:
                if len(og_user_data) <= i:
                    break
                fetch_user_data_values = og_user_data[i]
                fetch_user_data_values = flatten_json(fetch_user_data_values)
                for field in user_fields:
                    if field not in sample_values:
                        sample_values[field] = []
                    if field not in fetch_user_data_values or fetch_user_data_values[field] is None:
                        sample_values[field].append("")
                    else:
                        sample_values[field].append(fetch_user_data_values[field])
                i += 1

            mapper = get_mapper()

            transformed_event, mapping, node_feature_associations = map_to_internal_nested(user_data, mapper)

            print(f"{func}: transformed event: {transformed_event}")
            print(f"{func}: mapping: {mapping}")
            print(f"{func}: node_feature_associations: {node_feature_associations}")

            priorities = get_priorities(mapping)
            print(f"{func}: priorities: {priorities}")

            mapping_resp = {
                "mapping": {
                    "mapping": mapping,
                    "priorities": priorities,
                    "node_feature": node_feature_associations,
                    "sample_data": transformed_event,
                    "sample_values": sample_values,
                }
            }

            return mapping_resp


        def process_xml():
            '''
            map data from json file to internal fields
            Returns:
                mapping data for UI
            '''
            global TEMP_INPUT_FILENAME
            func = "process_xml"

            converted_json_file = "/tmp/input.json"
            lambda_response = convert_xml_to_json(TEMP_INPUT_FILENAME, converted_json_file)
            if lambda_response is not None:
                return lambda_response

            og_user_data = json.load(open(converted_json_file, "r"))

            og_user_data = find_first_list_of_dicts(og_user_data)

            if og_user_data is None:
                lambda_response = {
                    "error": "No list of alerts found in file. Please check your input data and try again."
                }

            try:
                user_data = og_user_data[0]
            except Exception as e:
                lambda_response = {
                    "error": "Unable to fetch first alert from the file. Please check you have atleast 1 input alert in the file."
                }
                return lambda_response

            user_data = flatten_json(user_data)

            user_fields = user_data.keys()
            print(f"{func}: user fields: {user_fields}")

            sample_values = {}

            i = 0
            while i < 3:
                if len(og_user_data) <= i:
                    break
                fetch_user_data_values = og_user_data[i]
                fetch_user_data_values = flatten_json(fetch_user_data_values)
                for field in user_fields:
                    if field not in sample_values:
                        sample_values[field] = []
                    if field not in fetch_user_data_values or fetch_user_data_values[field] is None:
                        sample_values[field].append("")
                    else:
                        sample_values[field].append(fetch_user_data_values[field])
                i += 1

            mapper = get_mapper()

            transformed_event, mapping, node_feature_associations = map_to_internal_nested(user_data, mapper)

            print(f"{func}: transformed event: {transformed_event}")
            print(f"{func}: mapping: {mapping}")
            print(f"{func}: node_feature_associations: {node_feature_associations}")

            priorities = get_priorities(mapping)
            print(f"{func}: priorities: {priorities}")

            mapping_resp = {
                "mapping": {
                    "mapping": mapping,
                    "priorities": priorities,
                    "node_feature": node_feature_associations,
                    "sample_data": transformed_event,
                    "sample_values": sample_values,
                }
            }

            if transformed_event is None:
                mapping_resp

            return mapping_resp


        def get_priorities(mapping):
            '''
            Create priorities for mapped fields
            Args:
                mapping: mapping of fields
            Return:
                priority dictionary
            '''

            priority_list_for_fields = ["id", "src", "dst", "time", "name"]

            # priorities = {
            #     "src": {
            #         "source_ip": 1
            #     },
            #     "dst": {
            #         "destination_ip": 1
            #     },
            #     "id": {
            #         "user_id": 1
            #     },
            #     "name": {
            #         "username": 1
            #     },
            #     "time": {
            #         "timestamp": 1
            #     }
            # }

            priorities = {f: {} for f in priority_list_for_fields}

            for field in priority_list_for_fields:
                field_list = mapping[field]
                if len(field_list) <= 0:
                    continue

                count = len(field_list)
                for f in field_list:
                    priorities[field][f] = count
                    count -= 1

            return priorities


        def map_to_internal_nested(event, cef_to_internal_mappings):
            '''
            Map cef fields to internal
            Args:
                event: event from chunk of input
                cef_to_internal_mappings: cef to internal mappings list
            Returns:
                transformed_event, mapping, node_feature_associations
            '''

            self_loop = None
            self_loop_key = None

            transformed_event = {
                "id": "",
                "src": "",
                "dst": "",
                "time": "",
                "name": "",
                "event_feature": {},
                "node_feature": {},
            }

            event_feature = {}

            event_node_feature = {
                "src": {},
                "dst": {},
            }

            node_feature_association = {}

            to_internal_mappings = {
                "id": [],
                "time": [],
                "src": [],
                "dst": [],
                "name": [],
                "event_feature": [],
                "node_feature": [],
                "unused": [],
            }

            self_loop_keys = []

            for mapping in cef_to_internal_mappings:
                source_field = mapping["source_field"]
                dst_field = mapping["dest_field"]

                src_fields = []

                # search for the source field to be part of any of the keys in flattened keys
                for k in event.keys():
                    if source_field in k.split("."):
                        src_fields.append(k)

                if not src_fields:
                    continue

                for src_field in src_fields:
                    src_field_value = event[src_field]
                    if src_field_value is not None and src_field_value != "":
                        if dst_field in ["id"]:
                            transformed_event["id"] = str(src_field_value)
                            to_internal_mappings["id"].append(src_field)
                        elif dst_field in ["dst"]:
                            transformed_event["dst"] = src_field_value
                            to_internal_mappings["dst"].append(src_field)
                        elif dst_field in ["src"]:
                            transformed_event["src"] = src_field_value
                            to_internal_mappings["src"].append(src_field)
                        elif dst_field in ["time"]:
                            transformed_event["time"] = src_field_value
                            to_internal_mappings["time"].append(src_field)
                        elif dst_field in ["name"]:
                            transformed_event["name"] = str(src_field_value)
                            to_internal_mappings["name"].append(src_field)

                        elif dst_field in ["self_loop_src (when both dst & src r empty) "]:
                            self_loop = src_field_value
                            self_loop_key = src_field
                            self_loop_keys.append(src_field)

                        elif dst_field in ["event_ftr_IMPRTNT_priority"]:
                            try:
                                src_field_value = int(src_field_value)
                                event_feature["priority"] = src_field_value
                            except Exception as e:
                                pass

                            to_internal_mappings["event_feature"].append(src_field)

                        elif dst_field in ["src_port", "dst_port"]:
                            try:
                                src_field_value = int(src_field_value)
                                event_feature["port"] = src_field_value
                            except Exception as e:
                                pass
                            to_internal_mappings["event_feature"].append(src_field)
                        elif dst_field in ["event_ftr", "event_ftr_IMPRTNT"]:
                            event_feature[src_field] = src_field_value
                            to_internal_mappings["event_feature"].append(src_field)
                    else:
                        to_internal_mappings["unused"].append(src_field)

            transformed_event["event_feature"].update(event_feature)

            # if none of the src, dst is present, then use self loop
            if transformed_event["src"] and not transformed_event["dst"]:
                transformed_event["dst"] = transformed_event["src"]
                to_internal_mappings["dst"].append(to_internal_mappings["src"][-1])

            elif not transformed_event["src"] and transformed_event["dst"]:
                transformed_event["src"] = transformed_event["dst"]
                to_internal_mappings["src"].append(to_internal_mappings["dst"][-1])

            elif not transformed_event["src"] and not transformed_event["dst"]:
                # self loop get the cef field for the self loop
                if self_loop is not None:
                    transformed_event["src"] = self_loop
                    transformed_event["dst"] = self_loop
                    to_internal_mappings["src"] = self_loop_keys
                    to_internal_mappings["dst"] = self_loop_keys

            # convert to unix timestamp
            unix_timestamp = convert_to_unix_timestamp(transformed_event["time"])
            if unix_timestamp is None:
                to_remove_mapping = []
                for field in to_internal_mappings["time"]:
                    try:
                        unix_timestamp = convert_to_unix_timestamp(event[field])
                        if unix_timestamp is None:
                            to_remove_mapping.append(field)
                        else:
                            break
                    except Exception as e:
                        pass
                to_internal_mappings["time"] = list(set(to_internal_mappings["time"]) - set(to_remove_mapping))
                to_internal_mappings["unused"] = copy.copy(to_remove_mapping)
            transformed_event["time"] = unix_timestamp

            # create node feature

            for mapping in cef_to_internal_mappings:
                source_field = mapping["source_field"]
                dst_field = mapping["dest_field"]

                if dst_field not in ["dst_node_ftr", "src_node_ftr", "self_loop_node_ftr"]:
                    continue

                src_fields = []

                for k in event.keys():
                    if source_field in k.split("."):
                        src_fields.append(k)

                if not src_fields:
                    continue

                for src_field in src_fields:
                    src_field_value = event[src_field]
                    if src_field_value is not None and src_field_value != "":
                        if dst_field in ["dst_node_ftr"]:
                            event_node_feature["dst"][src_field] = src_field_value
                            node_feature_association[src_field] = "dst"

                        elif dst_field in ["src_node_ftr"]:
                            event_node_feature["src"][src_field] = src_field_value
                            node_feature_association[src_field] = "src"

                        elif dst_field in ["self_loop_node_ftr"]:
                            event_node_feature["src"][src_field] = src_field_value
                            event_node_feature["dst"][src_field] = src_field_value
                            node_feature_association[src_field] = "both"

                        to_internal_mappings["node_feature"].append(src_field)

                    else:
                        to_internal_mappings["unused"].append(src_field)

            read_fields = list(itertools.chain(*to_internal_mappings.values()))

            add_to_unused_fields = list(set(event.keys()) - set(read_fields))

            to_internal_mappings["unused"].extend(add_to_unused_fields)

            transformed_event["node_feature"] = event_node_feature

            for k in to_internal_mappings.keys():
                to_internal_mappings[k] = list(set(to_internal_mappings[k]))

            return transformed_event, to_internal_mappings, node_feature_association


        def map_to_internal(event, cef_to_internal_mappings):
            '''
            Map cef fields to internal
            Args:
                event: event from chunk of input
                cef_to_internal_mappings: cef to internal mappings list
            Returns:
                transformed_event, mapping, node_feature_associations
            '''

            self_loop = None
            self_loop_key = None

            transformed_event = {
                "id": "",
                "src": "",
                "dst": "",
                "time": "",
                "name": "",
                "event_feature": {},
                "node_feature": {},
            }

            event_feature = {}

            event_node_feature = {
                "src": {},
                "dst": {},
            }

            node_feature_association = {}

            to_internal_mappings = {
                "id": [],
                "time": [],
                "src": [],
                "dst": [],
                "name": [],
                "event_feature": [],
                "node_feature": [],
                "unused": [],
            }

            self_loop_keys = []

            for mapping in cef_to_internal_mappings:
                src_field = mapping["source_field"]
                dst_field = mapping["dest_field"]
                if src_field not in event:
                    continue
                src_field_value = event[src_field]
                if src_field_value is not None and src_field_value != "":
                    if dst_field in ["id"]:
                        transformed_event["id"] = str(src_field_value)
                        to_internal_mappings["id"].append(src_field)
                    elif dst_field in ["dst"]:
                        transformed_event["dst"] = src_field_value
                        to_internal_mappings["dst"].append(src_field)
                    elif dst_field in ["src"]:
                        transformed_event["src"] = src_field_value
                        to_internal_mappings["src"].append(src_field)
                    elif dst_field in ["time"]:
                        transformed_event["time"] = src_field_value
                        to_internal_mappings["time"].append(src_field)
                    elif dst_field in ["name"]:
                        transformed_event["name"] = str(src_field_value)
                        to_internal_mappings["name"].append(src_field)

                    elif dst_field in ["self_loop_src (when both dst & src r empty) "]:
                        self_loop = src_field_value
                        self_loop_key = src_field
                        self_loop_keys.append(src_field)

                    elif dst_field in ["event_ftr_IMPRTNT_priority"]:
                        try:
                            src_field_value = int(src_field_value)
                            event_feature["priority"] = src_field_value
                        except Exception as e:
                            pass

                        to_internal_mappings["event_feature"].append(src_field)

                    elif dst_field in ["src_port", "dst_port"]:
                        try:
                            src_field_value = int(src_field_value)
                            event_feature["port"] = src_field_value
                        except Exception as e:
                            pass
                        to_internal_mappings["event_feature"].append(src_field)
                    elif dst_field in ["event_ftr", "event_ftr_IMPRTNT"]:
                        event_feature[src_field] = src_field_value
                        to_internal_mappings["event_feature"].append(src_field)
                else:
                    to_internal_mappings["unused"].append(src_field)

            transformed_event["event_feature"].update(event_feature)

            # if none of the src, dst is present, then use self loop
            if transformed_event["src"] and not transformed_event["dst"]:
                transformed_event["dst"] = transformed_event["src"]
                to_internal_mappings["dst"].append(to_internal_mappings["src"][-1])

            elif not transformed_event["src"] and transformed_event["dst"]:
                transformed_event["src"] = transformed_event["dst"]
                to_internal_mappings["src"].append(to_internal_mappings["dst"][-1])

            elif not transformed_event["src"] and not transformed_event["dst"]:
                # self loop get the cef field for the self loop
                if self_loop is not None:
                    transformed_event["src"] = self_loop
                    transformed_event["dst"] = self_loop
                    to_internal_mappings["src"] = self_loop_keys
                    to_internal_mappings["dst"] = self_loop_keys
                else:
                    # if self loop is absent too, then required fields src, dest cannot be mapped. skip event
                    # TODO: map all remaingin fields to unused
                    return None, to_internal_mappings, event_node_feature

            # convert to unix timestamp
            unix_timestamp = convert_to_unix_timestamp(transformed_event["time"])
            if unix_timestamp is None:
                to_remove_mapping = []
                for field in to_internal_mappings["time"]:
                    try:
                        unix_timestamp = convert_to_unix_timestamp(event[field])
                        if unix_timestamp is None:
                            to_remove_mapping.append(field)
                        else:
                            break
                    except Exception as e:
                        pass
                to_internal_mappings["time"] = list(set(to_internal_mappings["time"]) - set(to_remove_mapping))
                to_internal_mappings["unused"] = copy.copy(to_remove_mapping)
            transformed_event["time"] = unix_timestamp

            # create node feature

            for mapping in cef_to_internal_mappings:
                src_field = mapping["source_field"]
                dst_field = mapping["dest_field"]

                if dst_field not in ["dst_node_ftr", "src_node_ftr", "self_loop_node_ftr"]:
                    continue

                if src_field not in event:
                    continue

                src_field_value = event[src_field]
                if src_field_value is not None and src_field_value != "":
                    if dst_field in ["dst_node_ftr"]:
                        event_node_feature["dst"][src_field] = src_field_value
                        node_feature_association[src_field] = "dst"

                    elif dst_field in ["src_node_ftr"]:
                        event_node_feature["src"][src_field] = src_field_value
                        node_feature_association[src_field] = "src"

                    elif dst_field in ["self_loop_node_ftr"]:
                        event_node_feature["src"][src_field] = src_field_value
                        event_node_feature["dst"][src_field] = src_field_value
                        node_feature_association[src_field] = "both"

                    to_internal_mappings["node_feature"].append(src_field)

                else:
                    to_internal_mappings["unused"].append(src_field)

            read_fields = list(itertools.chain(*to_internal_mappings.values()))

            add_to_unused_fields = list(set(event.keys()) - set(read_fields))

            to_internal_mappings["unused"].extend(add_to_unused_fields)

            transformed_event["node_feature"] = event_node_feature

            for k in to_internal_mappings.keys():
                to_internal_mappings[k] = list(set(to_internal_mappings[k]))

            return transformed_event, to_internal_mappings, node_feature_association


        def api_push_mapping_to_ui(ui_cookies, ui_headers, ui_session, payload):
            '''
            API request to push mapping to UI
            '''
            func = "api_push_mapping_to_ui"

            print(ui_headers)


            try:
                response_from_ui = ui_session.post(f"{HOST}api/v2/alert/fieldmapping/", json=payload,
                                                   cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)
                response_from_ui.raise_for_status()
                print(f"{func}: Mapping added successful")
            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e


        def get_ui_session():
            '''
            Get UI session and cookies
            Returns:
                cookies: Cookies for the connection to UI
                headers: Headers for the connection to UI
                s: session for the connection to UI
            '''
            global HOST
            global UI_PASSWORD
            global UI_USERNAME

            try:
                s = requests.Session()
                response = s.get(HOST + "login")
                response.raise_for_status()

                cookies = s.cookies.get_dict()
                csrf = cookies.get("csrftoken")
                if not csrf:
                    raise ValueError("CSRF token not found in cookies")

                headers = {"X-CSRFToken": csrf, "Accept": "application/json"}

                response = s.post(f"{HOST}login/?next",
                                  data={"username": UI_USERNAME, "password": UI_PASSWORD, "csrfmiddlewaretoken": csrf})

                if response.status_code != 404 and not response.url.endswith("/accounts/profile/"):
                    response.raise_for_status()

                print("login successful")
                return cookies, headers, s

            except requests.exceptions.RequestException as e:
                print("Request error occurred.")
                raise e
            except Exception as e:
                print("An unexpected error occurred.")
                raise e


        def convert_to_unix_timestamp(time_str):
            """
            Convert any date/time format to a UNIX timestamp.
            Args:
                time_str: The date/time string or UNIX timestamp to convert.
            Returns
                return: The UNIX timestamp.
            """
            try:
                # Check if the input is already a UNIX timestamp
                if isinstance(time_str, (int, float)):
                    return time_str

                # Attempt to convert string to a float or int
                try:
                    timestamp = float(time_str)
                    return timestamp
                except ValueError:
                    pass

                # Parse the date/time string into a datetime object
                dt = parser.parse(time_str)
                # Convert the datetime object to a UNIX timestamp
                unix_timestamp = int(dt.timestamp())
                return unix_timestamp
            except Exception as e:
                print(f"Error: {e}")
                return None


        def get_mapper():
            '''
            Get static field mapper
            Returns:
                list of cef to internal mappings
            '''

            cef_to_internal_mappings = [
                {
                    "source_field": "accessGroup",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientLogonId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientMachineName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddress",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddress",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddressIPv6",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceCustomString1",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceHostName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceNtHost",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "domain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "group",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "groupDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "groupTypeChange",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "hostname",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonAccount",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberDn",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberNtDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "primaryDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "primaryUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectAccountDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectAccountName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectDomainName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectLogonId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectSecurityId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "suser",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "user",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userGroup",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userGroupId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userType",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "workstation",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "workstationName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "acl",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "act",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "act",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachDisposition",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachFileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachSize",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachSizeDecoded",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachTransferEncoding",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "authMethod",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "awsAccountId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "body",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bucket",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytesIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytesOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerComputerName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerDomain",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerLogonId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerMachineName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerUserName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cat",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "categoryString",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cd",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeClass",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeDescription",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaMessageId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaUser",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaVendorAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn1",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn2",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn3",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn4",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn5",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn6",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "command",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "commProto",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "content",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "count",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "creatorProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs1",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs1Label",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs2",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs3",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs4",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs5",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs6",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceDirection",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceProduct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceProduct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceReceiptTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVendor",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVersion",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "duration",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationHour",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationMinute",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "errorCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventHour",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventMinute",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventMonth",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventSubtype",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventTypeColor",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventWeekDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventYear",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventZone",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "exitStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "filePath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileSize",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "flowId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fragmentCount",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "hashCodes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "icmpCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "icmpType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "id",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "idsType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "imageFileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "index",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "indexTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "initialRtt",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "interfaceId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "keywords",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "kv",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "laction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "lineCount",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logLevel",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "messageId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "messageType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "name",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newAccountName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newDomain",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "object",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectAttrs",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "opCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "outcome",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packets",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packetsIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packetsOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcess",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessPath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "preMsg",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "privilegeList",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "product",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "punct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "query",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "queryType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "raw",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "recordNumber",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "region",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryPath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryValueName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryValueType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "replyCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "replyCodeId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestClientApplication",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestMethod",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestURL",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "responseTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "rule",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ruleId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "securityId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serial",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "service",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serviceId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serviceName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sessionId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "si",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "signatureId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "source",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceContent",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceNetworkAddress",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceSgInfo",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "splunkServer",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "splunkServerGroup",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sslIsValid",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "status",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "subject",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "subSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "suppliedRealmName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tag",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagApp",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagEventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagObjectCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "taskCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "timeTaken",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tokenElevationType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tokenElevationType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tos",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalBytes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalPacketsIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalPacketsOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalResponseTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "transactionId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "transport",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ttl",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "type",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "valuesFlowId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "version",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "vpcFlowAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winSecurityCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationNtHost",
                    "dest_field": "dst",
                    "priority": 6.0
                },
                {
                    "source_field": "destinationAddress",
                    "dest_field": "dst",
                    "priority": 5.0
                },
                {
                    "source_field": "destinationAddressIPv6",
                    "dest_field": "dst",
                    "priority": 4.0
                },
                {
                    "source_field": "destinationHostName",
                    "dest_field": "dst",
                    "priority": 3.0
                },
                {
                    "source_field": "destinationTranslatedAddress",
                    "dest_field": "dst",
                    "priority": 1.0
                },
                {
                    "source_field": "destContent",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destHost",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationInterface",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationInterfaceId",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationMacAddress",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationNtDomain",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetAccountId",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationPriority",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetAccountName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationZone",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destSgInfo",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "duser",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetDomain",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetDomainName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetProcessName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetServerName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetUserName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "description",
                    "dest_field": "name",
                    "priority": 5.0
                },
                {
                    "source_field": "name",
                    "dest_field": "name",
                    "priority": 4.0
                },
                {
                    "source_field": "ruleName",
                    "dest_field": "name",
                    "priority": 3.0
                },
                {
                    "source_field": "description",
                    "dest_field": "name",
                    "priority": 2.0
                },
                {
                    "source_field": "msg",
                    "dest_field": "name",
                    "priority": 2.0
                },
                {
                    "source_field": "message",
                    "dest_field": "name",
                    "priority": 1.0
                },
                {
                    "source_field": "destPublicPort",
                    "dest_field": "dst_port",
                    "priority": 3.0
                },
                {
                    "source_field": "destinationTranslatedPort",
                    "dest_field": "dst_port",
                    "priority": 2.0
                },
                {
                    "source_field": "destinationPort",
                    "dest_field": "dst_port",
                    "priority": 1.0
                },
                {
                    "source_field": "process",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processCommandLine",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processExec",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processId",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processName",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processPath",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "proto",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoCode",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoFullName",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoId",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoStack",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoVersion",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "sourceType",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "sourceType",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendor",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorAccount",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorAction",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorClass",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorDefinition",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorPrivilege",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorSeverity",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "severity",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "severityId",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "severityLevel",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "computerName",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 6.0
                },
                {
                    "source_field": "hostAddr",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 5.0
                },
                {
                    "source_field": "assignedIp",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 4.0
                },
                {
                    "source_field": "clientAddress",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 3.0
                },
                {
                    "source_field": "ip",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 2.0
                },
                {
                    "source_field": "ipAddress",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 1.0
                },
                {
                    "source_field": "sourceAddressIPv6",
                    "dest_field": "src",
                    "priority": 6.0
                },
                {
                    "source_field": "sourceTranslatedAddress",
                    "dest_field": "src",
                    "priority": 5.0
                },
                {
                    "source_field": "sourceAddress",
                    "dest_field": "src",
                    "priority": 3.0
                },
                {
                    "source_field": "sourceNtHost",
                    "dest_field": "src",
                    "priority": 2.0
                },
                {
                    "source_field": "sourceHostName",
                    "dest_field": "src",
                    "priority": 1.0
                },
                {
                    "source_field": "sourceHost",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceInterface",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceInterfaceId",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceMacAddress",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceNtDomain",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourcePriority",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceUserName",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceWorkstation",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceZone",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "suser",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourcePublicPort",
                    "dest_field": "src_port",
                    "priority": 3.0
                },
                {
                    "source_field": "sourceTranslatedPort",
                    "dest_field": "src_port",
                    "priority": 2.0
                },
                {
                    "source_field": "sourcePort",
                    "dest_field": "src_port",
                    "priority": 1.0
                },
                {
                    "source_field": "endTime",
                    "dest_field": "time",
                    "priority": 4.0
                },
                {
                    "source_field": "startTime",
                    "dest_field": "time",
                    "priority": 3.0
                },
                {
                    "source_field": "timestamp",
                    "dest_field": "time",
                    "priority": 2.0
                },
                {
                    "source_field": "time",
                    "dest_field": "time",
                    "priority": 1.0
                },
                {
                    "source_field": "id",
                    "dest_field": "id",
                    "priority": 1.0
                }
            ]

            return cef_to_internal_mappings


        def flatten_json(y, separator='.', prefix=''):
            """
            Flattens a nested JSON object.

            :param y: The JSON object (dict) to flatten.
            :param separator: The string used to separate concatenated keys.
            :param prefix: The prefix for the keys (used in recursion).
            :return: A flattened dictionary.
            """
            out = {}

            def flatten(x, name=''):
                if isinstance(x, dict):
                    for a in x:
                        flatten(x[a], f"{name}{a}{separator}")
                elif isinstance(x, list):
                    for i, a in enumerate(x):
                        flatten(a, f"{name}{i}{separator}")
                else:
                    out[name[:-1]] = x  # Remove the trailing separator

            flatten(y, prefix)
            return out


        def find_first_list_of_dicts(data):
            """
            Recursively traverse the JSON data to find the first list containing dictionaries.

            :param data: The JSON data (parsed into Python structures).
            :return: The first dictionary found in the first list of dictionaries, or None if not found.
            """
            if isinstance(data, dict):
                for key, value in data.items():
                    result = find_first_list_of_dicts(value)
                    if result is not None:
                        return result
            elif isinstance(data, list):
                if len(data) > 0 and isinstance(data[0], dict):
                    return data
                for item in data:
                    result = find_first_list_of_dicts(item)
                    if result is not None:
                        return result
            return None


        def convert_xml_to_json(xml_file_path, json_file_path):
            """
            Converts an XML file to a JSON file.

            :param xml_file_path: Path to the input XML file.
            :param json_file_path: Path to the output JSON file.
            """
            try:
                with open(xml_file_path, 'r', encoding='utf-8') as xml_file:
                    xml_content = xml_file.read()

            except Exception as e:
                lambda_response = {
                    "error": "Unable to parse xml file. Invalid XML file. Please check your file format and try again."
                }
                return lambda_response

            try:
                # Parse XML to OrderedDict
                parsed_xml = xmltodict.parse(xml_content, process_namespaces=True)

                # Convert OrderedDict to JSON string
                json_data = json.dumps(parsed_xml)

                with open(json_file_path, 'w', encoding='utf-8') as json_file:
                    json_file.write(json_data)

                print(f"Successfully converted '{xml_file_path}' to '{json_file_path}'")

            except Exception as e:
                lambda_response = {
                    "error": "Unable to parse xml file to json. Please check your file format and try again."
                }
                return lambda_response

            return None


        def parse_s3_path(s3_path):
            '''
            Parse an S3 path and extract the bucket name and object key.
            Args:
                s3_path: The S3 path to parse. Should be in the format: s3://bucket-name/path/to/object
            Returns:
                (bucket_name, object_key).
                (None, None) if the path is invalid.
            '''
            if not s3_path.startswith("s3://"):
                return None, None

            # Remove the "s3://" prefix
            path_without_prefix = s3_path[5:]

            # Split the remaining path into parts
            parts = path_without_prefix.split('/', 1)

            # If there's only one part, it means there's no object key
            if len(parts) == 1:
                return parts[0], None

            # If there are two parts, the first is the bucket name, the second is the object key
            return parts[0], parts[1]


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          ui_lb_url: !GetAtt UILoadBalancer.DNSName
          ui_password: !Ref SuperuserPassword
          ui_username: !Ref SuperuserUsername
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: uploads/
  createMappingLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${createMapping}
  createMappingLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref createMapping
      MaximumRetryAttempts: 0
      Qualifier: $LATEST
  createMappingUrl:
    Type: AWS::Lambda::Url
    Properties:
      AuthType: NONE  # or AWS_IAM, depending on your needs
      TargetFunctionArn: !GetAtt createMapping.Arn
      Cors:
        AllowCredentials: false
        AllowOrigins:
        - '*'
        ExposeHeaders:
        - Content-Type
        AllowHeaders:
        - Content-Type
        MaxAge: 60
        AllowMethods:
        - POST

  createMappingInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunctionUrl
      FunctionName: !Ref createMapping
      Principal: '*'  # Allows public access; restrict as needed
      FunctionUrlAuthType: NONE  # Match with FunctionUrlConfig AuthType

  preprocessInput:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_preprocess_input
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: preprocessInput
      InlineCode: |
        '''
        Create mapping for uploaded file

        Input: input.json, input.xml, input.csv
        '''

        import os
        import subprocess
        import sys
        import urllib
        import json
        import glob
        import copy
        from dateutil import parser
        import itertools

        import boto3
        import pandas as pd
        import requests

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        os.makedirs("/tmp/nltk_download", exist_ok=True)
        subprocess.call('pip install xmltodict -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import xmltodict

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        TEMP_USER_MAPPING = "/tmp/user_mapping.json"
        TEMP_PIPELINE_MAPPING = "/tmp/mapping.json"

        TEMP_PIPELINE_INPUT = "/tmp/pipeline_input.json"

        PATH_TO_UI_UPLOAD_CONFIG = "mapping/upload_config.json"
        TEMP_UI_UPLOAD_CONFIG = "/TMP/upload_config.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_INPUT_FILENAME
            global TEMP_USER_MAPPING
            global TEMP_PIPELINE_MAPPING
            global TEMP_PIPELINE_INPUT

            func = "lambda_handler"

            bucket = event["Records"][0]["s3"]["bucket"]["name"]
            # bucket = "airflow-cy-test"
            try:
                input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")
                # input_filename = "mapping/input/1/22.csv"

                upload_filename = os.path.splitext(input_filename)[0].split("/")[-1]
                field_mapping_name = os.path.splitext(input_filename)[0].split("/")[-2]

                extension = os.path.splitext(input_filename)[-1]
                TEMP_INPUT_FILENAME = f"/tmp/input.{extension}"

                if is_disable_upload(bucket):
                    return

                print(f"{func}: Download input file: {bucket}/{input_filename}")
                try:
                    S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                    print(f"{func}: Input file download completed")

                except Exception as e:
                    print(f"{func}: Failed to download input file")
                    raise e

                path_to_user_mapping = f"mapping/user_field_mapping/{field_mapping_name}.json"
                try:
                    S3_CLIENT.download_file(bucket, path_to_user_mapping, TEMP_USER_MAPPING)
                    print(f"{func}: Mapping file download completed")

                except Exception as e:
                    print(f"{func}: Failed to download mapping file")
                    raise e

                mapper = get_mapper()
                json.dump(mapper, open(TEMP_PIPELINE_MAPPING, "w"))

                if extension == ".csv":
                    process_csv()

                elif extension == ".json":
                    process_json()

                elif extension == ".xml":
                    process_xml()

                else:
                    raise ValueError(f"Unable to process the current file with extension: {extension}")

                pipeline_input_json = json.load(open(TEMP_PIPELINE_INPUT, "r"))
                pipeline_input_json = {"input": pipeline_input_json}
                json.dump(pipeline_input_json, open(TEMP_PIPELINE_INPUT, "w"))

                path_to_pipeline_mapping = f"mapping/field_mapping/{field_mapping_name}.json"
                try:
                    S3_CLIENT.upload_file(TEMP_PIPELINE_MAPPING, bucket, path_to_pipeline_mapping)
                    print(f"{func}: Mapping file upload completed")

                except Exception as e:
                    print(f"{func}: Failed to upload mapping file")
                    raise e

                path_to_pipeline_input = f"input/{field_mapping_name}/{upload_filename}.json"

                try:
                    S3_CLIENT.upload_file(TEMP_PIPELINE_INPUT, bucket, path_to_pipeline_input)
                    print(f"{func}: Mapping file upload completed")

                except Exception as e:
                    print(f"{func}: Failed to upload mapping file")
                    raise e

            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex


        def process_csv():
            '''
            map data from csv file to internal fields
            Returns:
                mapping data for UI
            '''
            global TEMP_INPUT_FILENAME
            global TEMP_PIPELINE_INPUT
            func = "process_csv"

            user_data = pd.read_csv(TEMP_INPUT_FILENAME)
            user_data.to_json(TEMP_PIPELINE_INPUT, orient="records", index=False)


        def process_json():
            '''
            map data from json file to internal fields
            Returns:
                mapping data for UI
            '''
            global TEMP_INPUT_FILENAME
            global TEMP_PIPELINE_INPUT
            func = "process_json"

            user_data = json.load(open(TEMP_INPUT_FILENAME, "r"))

            for alert in user_data:
                alert = flatten_json(alert)

            json.dump(user_data, open(TEMP_PIPELINE_INPUT, "w"))


        def process_xml():
            '''
            map data from json file to internal fields
            Returns:
                mapping data for UI
            '''
            global TEMP_INPUT_FILENAME
            global TEMP_PIPELINE_INPUT
            func = "process_xml"

            converted_json_file = "/tmp/input.json"
            convert_xml_to_json(TEMP_INPUT_FILENAME, converted_json_file)

            user_data = json.load(open(converted_json_file, "r"))

            user_data = find_first_list_of_dicts(user_data)

            if user_data is None:
                raise ValueError("No list of dictionaries found in the JSON data.")

            for alert in user_data:
                alert = flatten_json(alert)

            json.dump(user_data, open(TEMP_PIPELINE_INPUT, "w"))


        def get_mapper():
            '''
            Get static field mapper
            Returns:
                list of cef to internal mappings
            '''
            global TEMP_USER_MAPPING

            to_internal_mappings = []

            mapping_json = json.load(open(TEMP_USER_MAPPING, "r"))

            mapping = mapping_json["mapping"]
            priorities = mapping_json["priorities"]
            node_feature = mapping_json["node_feature"]

            for key in priorities.keys():
                internal_to_user_field_dict = priorities[key]
                sorted_keys = sorted(internal_to_user_field_dict, key=internal_to_user_field_dict.get, reverse=True)

                for sorted_key in sorted_keys:
                    internal_mapping_record = {
                        "source_field": sorted_key,
                        "dest_field": key
                    }

                    to_internal_mappings.append(internal_mapping_record)

            for ftr in mapping["event_feature"]:
                internal_mapping_record = {
                    "source_field": ftr,
                    "dest_field": "event_feature"
                }

                to_internal_mappings.append(internal_mapping_record)

            for key, value in node_feature.items():
                internal_mapping_record = {
                    "source_field": key,
                    "dest_field": "node_feature",
                    "association": value,
                }

                to_internal_mappings.append(internal_mapping_record)

            return to_internal_mappings


        def is_disable_upload(bucket):
            '''
            Check for a file stored in s3, which will tell if upload should be disabled
            Args:
                bucket: Bucket name
            '''
            global S3_CLIENT
            func = "is_disable_upload"

            try:
                S3_CLIENT.head_object(Bucket=bucket, Key=PATH_TO_UI_UPLOAD_CONFIG)
            except Exception as e:
                return False

            S3_CLIENT.download_object(bucket, PATH_TO_UI_UPLOAD_CONFIG, TEMP_UI_UPLOAD_CONFIG)

            upload_config_json = json.load(open(TEMP_UI_UPLOAD_CONFIG, "r"))

            if "disable_upload" in upload_config_json:
                return upload_config_json["disable_upload"]
            else:
                return False


        def flatten_json(y, separator='.', prefix=''):
            """
            Flattens a nested JSON object.

            :param y: The JSON object (dict) to flatten.
            :param separator: The string used to separate concatenated keys.
            :param prefix: The prefix for the keys (used in recursion).
            :return: A flattened dictionary.
            """
            out = {}

            def flatten(x, name=''):
                if isinstance(x, dict):
                    for a in x:
                        flatten(x[a], f"{name}{a}{separator}")
                elif isinstance(x, list):
                    for i, a in enumerate(x):
                        flatten(a, f"{name}{i}{separator}")
                else:
                    out[name[:-1]] = x  # Remove the trailing separator

            flatten(y, prefix)
            return out


        def find_first_list_of_dicts(data):
            """
            Recursively traverse the JSON data to find the first list containing dictionaries.

            :param data: The JSON data (parsed into Python structures).
            :return: The first dictionary found in the first list of dictionaries, or None if not found.
            """
            if isinstance(data, dict):
                for key, value in data.items():
                    result = find_first_list_of_dicts(value)
                    if result is not None:
                        return result
            elif isinstance(data, list):
                if len(data) > 0 and isinstance(data[0], dict):
                    return data
                for item in data:
                    result = find_first_list_of_dicts(item)
                    if result is not None:
                        return result
            return None


        def convert_xml_to_json(xml_file_path, json_file_path):
            """
            Converts an XML file to a JSON file.

            :param xml_file_path: Path to the input XML file.
            :param json_file_path: Path to the output JSON file.
            """
            try:
                with open(xml_file_path, 'r', encoding='utf-8') as xml_file:
                    xml_content = xml_file.read()

                # Parse XML to OrderedDict
                parsed_xml = xmltodict.parse(xml_content, process_namespaces=True)

                # Convert OrderedDict to JSON string
                json_data = json.dumps(parsed_xml)

                with open(json_file_path, 'w', encoding='utf-8') as json_file:
                    json_file.write(json_data)

                print(f"Successfully converted '{xml_file_path}' to '{json_file_path}'")

            except Exception as e:
                print(f"Error: {e}")
                raise e


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: mapping/input/
  preprocessInputLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${preprocessInput}
  preprocessInputLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref preprocessInput
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  saveFeedback:
    Type: AWS::Serverless::Function
    DependsOn:
    - UILoadBalancer
    - DbLoadBalancer
    - ECSClusterLogGroup
    Properties:
      FunctionName: !Sub ${AWS::StackName}_save_feedback
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: saveFeedback
      InlineCode: |
        '''
        Save copy/cut action feedback to S3

        Input: log message with action id and campaign ids involved
        '''

        import ast
        import base64
        import gzip
        import json
        import os
        import time
        import uuid
        import glob
        from datetime import datetime, timezone
        import sqlite3
        from contextlib import closing

        import pandas as pd
        import boto3
        from botocore.exceptions import ClientError
        import requests
        from requests.exceptions import RequestException
        import requests.adapters
        from urllib3.util.retry import Retry

        import sys
        import subprocess

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        subprocess.call('pip install psycopg2-binary==2.9.9 sqlalchemy==2.0.32 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import psycopg2
        from psycopg2 import sql
        from psycopg2.extras import DictCursor

        pd.options.mode.chained_assignment = None

        UI_LB_URL = os.getenv("ui_lb_url")

        BUCKET = os.getenv("bucket")

        UI_USERNAME = os.getenv("ui_username")

        UI_PASSWORD = os.getenv("ui_password")

        DB_CONNECTION_STRING = os.getenv("db_connection_string")

        missing_variables = []

        if UI_LB_URL is None:
            missing_variables.append("ui_lb_url")
        if BUCKET is None:
            missing_variables.append("bucket")
        if UI_USERNAME is None:
            missing_variables.append("ui_username")
        if UI_PASSWORD is None:
            missing_variables.append("ui_password")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        SCRATCH_DIR = "scratch"

        S3_CLIENT = boto3.client("s3")

        TEMP_CLUSTER_OF_ALERTS_JSON_FILENAME = "/tmp/feedback_alerts_cluster.json"

        TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME = "/tmp/attribute_weights.json"
        TEMP_GLOBAL_CLUSTER_OPERATIONS_FILENAME = "/tmp/global_cluster_operations.json"

        TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME = "/tmp/current_cluster_ticket_output.json"

        PATH_TO_FINAL_CLUSTER_OUTPUT = f"{SCRATCH_DIR}/cluster.json"
        TEMP_FINAL_CLUSTER_OUTPUT_FILENAME = "/tmp/final_cluster_output.json"

        PATH_TO_FINAL_CLUSTER_TICKET_OUTPUT = f"{SCRATCH_DIR}/cluster_ticket_output.json"
        TEMP_FINAL_CLUSTER_TICKET_OUTPUT_FILENAME = "/tmp/final_cluster_ticket_output.json"

        INITIAL_WEIGHTS_VALUE = 100
        MIN_WEIGHTS_VALUE = 0
        MAX_WEIGHTS_VALUE = 200

        SOURCE_EMPTY = False

        HOST = f"http://{UI_LB_URL}:8000/"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global SCRATCH_DIR
            global SOURCE_EMPTY

            func = "lambda_handler"

            SOURCE_EMPTY = False

            print(f"{func}: Received event: {json.dumps(event)}")

            aws_log = event['awslogs']['data']
            compressed_payload = base64.b64decode(aws_log)
            uncompressed_payload = gzip.decompress(compressed_payload)
            decoded_payload = json.loads(uncompressed_payload.decode('utf-8'))

            print("Decoded CloudWatch data: ", decoded_payload)

            message = decoded_payload["logEvents"][0]["message"]

            clear_temp_dir()

            try:
                message_json = json.loads(message)

                campaign_id_list = message_json["message"]["campaign_id"]
                action_id = message_json["message"]["action_id"]
            except json.JSONDecodeError as json_error:
                print(f"Error parsing JSON from message: {message}")
                raise json_error
            except KeyError as key_error:
                print("Missing key in JSON message")
                raise key_error

            print(f"{func}: Save feedback for campaign id(s): {campaign_id_list}")
            print(f"{func}: Save feedback for action id: {action_id}")

            save_feedback(action_id)


        def save_feedback(action_id):
            '''
            Fetch action performed on campaigns and create operations list, update final cluster json
            create new cluster ticket output
            Args:
                action_id: action id in UI that has details for the action performed
            '''
            global BUCKET
            global S3_CLIENT

            func = "save_feedback"

            try:
                ui_cookies, ui_headers, ui_session = get_ui_session()
            except Exception as e:
                print(f"{func}: Failed to get session.")
                raise e

            try:
                action_attributes = get_action_attributes(ui_cookies, ui_headers, ui_session, action_id)
            except Exception as e:
                print(f"{func}: Failed to get action attributes.")
                raise e

            print(action_attributes)

            # get weights from the action and save to global weights
            update_global_attributes_weights(action_attributes)

            # create current action operations
            source_cluster_id, destination_cluster_id = create_current_operations(action_attributes, ui_cookies, ui_headers, ui_session)

            metrics = update_cluster_output_sql(source_cluster_id, destination_cluster_id)

            update_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id, metrics)

            create_current_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id)

            # upload feedback cluster ticket output to S3
            current_timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %z")
            path_to_feedback_cluster_ticket_output = f"{SCRATCH_DIR}/feedback/cluster_ticket_output_{current_timestamp}.json"
            try:
                S3_CLIENT.upload_file(TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME, BUCKET, path_to_feedback_cluster_ticket_output)
                print(f"{func}: Uploaded file to S3: {BUCKET}/{path_to_feedback_cluster_ticket_output}")
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {BUCKET}/{path_to_feedback_cluster_ticket_output}")
                raise e


        def create_current_operations(action_attributes, ui_cookies, ui_headers, ui_session):
            '''
            Create operations of create and delete to perform on clusters
            Args:
                action_attributes: Record of current action performed on which the function is triggered
            '''

            func = "create_current_operations"

            print(f"{func}: Get involved events list")
            events_involved = action_attributes["events_involved"][str(action_attributes['dst_campaign'][0])]
            print(f"{func}: Involved events: {events_involved}")

            # get user alert ids
            print(f"{func}: Get involved user alert ids")
            involved_user_alert_ids = []

            campaign_event_list = get_events(ui_cookies, ui_headers, ui_session, action_attributes['dst_campaign'][0])

            for campaign_event in campaign_event_list:
                if campaign_event["id"] in events_involved:
                    involved_user_alert_ids.append(campaign_event["mapped_data"]["UID"])

            print(f"{func}: Involved user alert ids: {involved_user_alert_ids}")

            insert_operation_query = """
            INSERT INTO operation_on_cluster (
                cluster_id,
                alert_ids,
                operation_type
            )
            VALUES (
                %(cluster_id)s,
                %(alert_ids)s,
                %(action_type)s
            )
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        campaign_id_list = ",".join(map(str, [action_attributes['src_campaign'], action_attributes['dst_campaign'][0]]))
                        campaign_id_list = f"({campaign_id_list})"
                        select_cluster_output = f"""
                        SELECT cluster_id, campaign_id
                        FROM cluster_output
                        WHERE campaign_id IN {campaign_id_list}
                        ;
                        """

                        cursor.execute(select_cluster_output)
                        result = cursor.fetchall()

                        source_cluster_id = None
                        destination_cluster_id = None

                        for row in result:
                            row = dict(row)
                            if row["campaign_id"] == action_attributes['src_campaign']:
                                source_cluster_id = row["cluster_id"]
                            if row["campaign_id"] == action_attributes['dst_campaign'][0]:
                                destination_cluster_id = row["cluster_id"]

                        if source_cluster_id is None:
                            raise ValueError(f"Unable to find cluster id for campaign id: {action_attributes['src_campaign']}")
                        if destination_cluster_id is None:
                            raise ValueError(f"Unable to find cluster id for campaign id: {action_attributes['dst_campaign'][0]}")

                        print(f"{func}: Source cluster id: {source_cluster_id}")
                        print(f"{func}: Destination cluster id: {destination_cluster_id}")

                        insert_operation_params = [
                            {
                                "action_type": "DELETE",
                                "cluster_id": source_cluster_id,
                                "alert_ids": json.dumps([involved_user_alert_ids])
                            },
                            {
                                "action_type": "INSERT",
                                "cluster_id": destination_cluster_id,
                                "alert_ids": json.dumps([involved_user_alert_ids])
                            }
                        ]

                        cursor.executemany(insert_operation_query, insert_operation_params)

                        update_event_params = {
                            "cluster_id": destination_cluster_id
                        }
                        alert_id_list = "','".join(map(str, involved_user_alert_ids))
                        alert_id_list = f"('{alert_id_list}')"
                        update_event_query = f"""
                        UPDATE event
                        SET cluster_id = %(cluster_id)s
                        WHERE alert_id IN {alert_id_list}
                        ;
                        """
                        print(update_event_query)
                        cursor.execute(update_event_query, update_event_params)
                    conn.commit()

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            return source_cluster_id, destination_cluster_id


        def update_cluster_output_sql(source_cluster_id, destination_cluster_id):
            '''
            Update cluster output table in sqlite
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
            Returns:
                metrics dictionary for cluster id (contains count of unique tech, tac, stage per cluster)
            '''
            global SOURCE_EMPTY

            func = "update_cluster_output_sql"

            batch_cluster_id_list = [source_cluster_id, destination_cluster_id]

            update_cluster_query = """
            UPDATE cluster_output
            SET cluster_starttime = %(cluster_starttime)s,
                cluster_endtime = %(cluster_endtime)s,
                cluster_srcips = %(cluster_srcips)s,
                cluster_dstips = %(cluster_dstips)s,
                cluster_techs = %(cluster_techs)s,
                cluster_tacs = %(cluster_tacs)s,
                cluster_stages = %(cluster_stages)s
            WHERE cluster_id = %(cluster_id)s
            ;
            """

            metrics = {}

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"

                        select_events_filter_cluster_query = f"""
                        SELECT
                            cluster_id,
                            MIN(time) AS min_time,
                            MAX(time) AS max_time,
                            STRING_AGG(DISTINCT src, ',') AS unique_src,
                            STRING_AGG(DISTINCT dst, ',') AS unique_dst,
                            STRING_AGG(tech,';') AS combined_tech,
                            STRING_AGG(tac,';') AS combined_tac,
                            STRING_AGG(stage,';') AS combined_stage
                        FROM
                            event
                        WHERE cluster_id IN {cluster_id_list}
                        GROUP BY cluster_id
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_events_filter_cluster_query)
                        result = cursor.fetchall()

                        update_cluster_output_params = []
                        for row in result:
                            row = dict(row)
                            src_ips = row['unique_src']
                            src_ips = src_ips.split(",")

                            dst_ips = row['unique_dst']
                            dst_ips = dst_ips.split(",")

                            combined_tech = row['combined_tech']
                            combined_tech = combined_tech.split(";")
                            tech_list = []
                            for techs in combined_tech:
                                tech_list += json.loads(techs)
                            tech_list = list(set(tech_list))

                            combined_tac = row['combined_tac']
                            combined_tac = combined_tac.split(";")
                            tac_list = []
                            for tacs in combined_tac:
                                tac_list += json.loads(tacs)
                            tac_list = list(set(tac_list))

                            combined_stage = row['combined_stage']
                            combined_stage = combined_stage.split(";")
                            stage_list = []
                            for stage in combined_stage:
                                stage_list += json.loads(stage)
                            stage_list = list(set(stage_list))

                            metrics[row['cluster_id']] = {
                                "tech": {"count": len(tech_list)},
                                "tac": {"count": len(tac_list)},
                                "stage": {"count": len(stage_list)}
                            }

                            # if cluster id is present in table. Update fields for existing row.
                            params = {
                                "cluster_starttime": row['min_time'],
                                "cluster_endtime": row['max_time'],
                                "cluster_srcips": json.dumps(src_ips),
                                "cluster_dstips": json.dumps(dst_ips),
                                "cluster_techs": json.dumps(tech_list),
                                "cluster_tacs": json.dumps(tac_list),
                                "cluster_stages": json.dumps(stage_list),
                                "cluster_id": row['cluster_id']
                            }

                            update_cluster_output_params.append(params)

                        if source_cluster_id not in metrics:
                            SOURCE_EMPTY = True
                            params = {
                                "cluster_starttime": 0.0,
                                "cluster_endtime": 0.0,
                                "cluster_srcips": json.dumps([]),
                                "cluster_dstips": json.dumps([]),
                                "cluster_techs": json.dumps([]),
                                "cluster_tacs": json.dumps([]),
                                "cluster_stages": json.dumps([]),
                                "cluster_id": source_cluster_id
                            }

                            update_cluster_output_params.append(params)

                        cursor.executemany(update_cluster_query, update_cluster_output_params)

                    conn.commit()
                return metrics
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id, metrics):
            '''
            Update cluster ticket output metrics
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
                metrics: list of metric for cluster id (contains count of unique tech, tac, stage per cluster)
            '''

            func = "update_cluster_ticket_output_sql"

            batch_cluster_id_list = [source_cluster_id, destination_cluster_id]

            metrics_cols = ["tech", "tac", "stage"]

            update_cluster_ticket_output_query = """
            UPDATE cluster_ticket_output
            SET metrics = %(metrics)s
            WHERE cluster_id = %(cluster_id)s
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"
                        select_event_query = f"""
                        SELECT *
                        FROM event
                        WHERE cluster_id IN {cluster_id_list}
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_event_query)
                        results = cursor.fetchall()
                        results = [dict(row) for row in results]
                        events_df = pd.DataFrame(results)

                        update_cluster_ticket_output_params = []
                        for cluster_id in batch_cluster_id_list:
                            if cluster_id in metrics:
                                filtered_events = events_df[events_df["cluster_id"] == cluster_id]
                                cluster_metrics = per_cluster_metrics_events(filtered_events)

                                for metric_col in metrics_cols:
                                    cluster_metrics[metric_col]["count"] = float(metrics[cluster_id][metric_col]["count"])

                                params = {
                                    "cluster_id": cluster_id,
                                    "metrics": json.dumps(cluster_metrics)
                                }
                                update_cluster_ticket_output_params.append(params)
                            else:
                                params = {
                                    "cluster_id": cluster_id,
                                    "metrics": json.dumps({})
                                }
                                update_cluster_ticket_output_params.append(params)
                        cursor.executemany(update_cluster_ticket_output_query, update_cluster_ticket_output_params)

                    conn.commit()

            except Exception as e:
                print(f"{func}: Failed to update cluster ticket output table.")
                raise e


        def per_cluster_metrics_events(involved_events_df):
            '''
            Get metrics per cluster
            Args:
                row_cluster_ticket_output: one record of cluster ticket output
            Returns:
                metrics of one record of cluster ticket output
            '''

            metrics_cols = ["tech", "tac", "stage"]

            metrics = {}

            for metric_col in metrics_cols:
                involved_events_df[f'{metric_col}'] = involved_events_df[f'{metric_col}'].apply(json.loads)
                involved_events_df[f'{metric_col}_count'] = involved_events_df[f'{metric_col}'].apply(len)

                metrics[metric_col] = {
                    "avg": float(involved_events_df[f"{metric_col}_count"].mean()),
                    "min": float(involved_events_df[f"{metric_col}_count"].min()),
                    "max": float(involved_events_df[f"{metric_col}_count"].max()),
                    "median": float(involved_events_df[f"{metric_col}_count"].median())
                }

            return metrics


        def create_current_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id):
            '''
            Create cluster ticket output for current operation clusters
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME
            global SOURCE_EMPTY

            func = "create_current_cluster_ticket_output_sql"

            input_cluster_id_list = [source_cluster_id, destination_cluster_id]

            try:
                with open(TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME, "w") as f:
                    with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                        with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                            cluster_id_list = ",".join(map(str, input_cluster_id_list))
                            cluster_id_list = f"({cluster_id_list})"
                            select_query = f"""
                            SELECT
                                co.cluster_id,
                                co.cluster_starttime,
                                co.cluster_endtime,
                                co.cluster_srcips,
                                co.cluster_dstips,
                                co.cluster_techs,
                                co.cluster_tacs,
                                co.cluster_stages,
                                cto.ticket_id,
                                cto.metrics,
                                e.alert_id,
                                e.src AS event_src,
                                e.dst AS event_dst,
                                e.time AS event_time,
                                e.name AS event_name,
                                e.tech AS event_tech,
                                e.tac AS event_tac,
                                e.stage AS event_stage,
                                e.other_attributes AS event_other_attributes
                            FROM cluster_ticket_output cto
                            INNER JOIN cluster_output co ON co.cluster_id = cto.cluster_id
                            INNER JOIN event e ON e.cluster_id = cto.cluster_id
                            WHERE cto.cluster_id IN {cluster_id_list}
                            ORDER BY cto.cluster_id
                            ;
                            """
                            cursor.execute(select_query)

                            row = cursor.fetchone()
                            prev_cluster = None

                            involved_events = []

                            while row:
                                row = dict(row)

                                curr_cluster = row
                                if prev_cluster is None:
                                    prev_cluster = curr_cluster

                                if curr_cluster["cluster_id"] != prev_cluster["cluster_id"]:
                                    row_cluster_ticket_output = {
                                        "ticket_id": prev_cluster["ticket_id"],
                                        "cluster_id": prev_cluster["cluster_id"],
                                        "involved_events": involved_events,
                                        "start_time": prev_cluster["cluster_starttime"],
                                        "end_time": prev_cluster["cluster_endtime"],
                                        "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                        "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                        "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                        "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                        "metrics": json.loads(prev_cluster["metrics"]),
                                    }
                                    f.write(json.dumps(row_cluster_ticket_output) + "\n")
                                    involved_events = []

                                events = {
                                    "id": row["alert_id"],
                                    "src": row["event_src"],
                                    "dst": row["event_dst"],
                                    "time": row["event_time"],
                                    "name": row["event_name"],
                                    "tech": json.loads(row["event_tech"]),
                                    "tac": json.loads(row["event_tac"]),
                                    "stage": json.loads(row["event_stage"]),
                                    "other_attributes_dict": json.loads(row["event_other_attributes"]),
                                }
                                involved_events.append(events)

                                prev_cluster = curr_cluster
                                row = cursor.fetchone()

                            row_cluster_ticket_output = {
                                "ticket_id": prev_cluster["ticket_id"],
                                "cluster_id": prev_cluster["cluster_id"],
                                "involved_events": involved_events,
                                "start_time": prev_cluster["cluster_starttime"],
                                "end_time": prev_cluster["cluster_endtime"],
                                "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                "metrics": json.loads(prev_cluster["metrics"]),
                            }
                            f.write(json.dumps(row_cluster_ticket_output) + "\n")

                            if SOURCE_EMPTY:
                                select_query = f"""
                                SELECT
                                    co.cluster_id,
                                    co.cluster_starttime,
                                    co.cluster_endtime,
                                    co.cluster_srcips,
                                    co.cluster_dstips,
                                    co.cluster_techs,
                                    co.cluster_tacs,
                                    co.cluster_stages,
                                    cto.ticket_id,
                                    cto.metrics
                                FROM cluster_ticket_output cto
                                INNER JOIN cluster_output co ON co.cluster_id = cto.cluster_id
                                WHERE cto.cluster_id = {source_cluster_id}
                                ORDER BY cto.cluster_id
                                ;
                                """
                                cursor.execute(select_query)

                                row = cursor.fetchone()
                                row_cluster_ticket_output = {
                                    "ticket_id": row["ticket_id"],
                                    "cluster_id": row["cluster_id"],
                                    "involved_events": [],
                                    "start_time": row["cluster_starttime"],
                                    "end_time": row["cluster_endtime"],
                                    "involved_entities": list(set(json.loads(row["cluster_dstips"]) + json.loads(row["cluster_srcips"]))),
                                    "involved_techs": json.loads(row["cluster_techs"]),
                                    "involved_tacs": json.loads(row["cluster_tacs"]),
                                    "involved_stages": json.loads(row["cluster_stages"]),
                                    "metrics": json.loads(row["metrics"]),
                                }
                            f.write(json.dumps(row_cluster_ticket_output) + "\n")

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_global_attributes_weights(action_attributes):
            '''
            Update global attributes weights file
            Args:
                action_attributes: Record of current action performed on which the function is triggered
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME
            global INITIAL_WEIGHTS_VALUE

            func = "update_global_attributes_weights"

            path_to_global_attribute_weights = f"{SCRATCH_DIR}/attribute_weights.json"
            fetch_or_create_global_attribute_weights(path_to_global_attribute_weights)

            global_attribute_weights_json = json.load(open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME))

            print(f"{func}: Current weights: {global_attribute_weights_json}")

            attributes_list = ["node", "event"]

            map_attributes = {
                "tech": "technique"
            }

            for attr in attributes_list:
                for key, value in action_attributes["attributes"][attr].items():
                    if key in map_attributes:
                        key = map_attributes[key]
                    if key not in global_attribute_weights_json[attr]:
                        global_attribute_weights_json[attr][key] = INITIAL_WEIGHTS_VALUE

                    global_attribute_weights_json[attr][key] += value

                    if global_attribute_weights_json[attr][key] < MIN_WEIGHTS_VALUE:
                        global_attribute_weights_json[attr][key] = MIN_WEIGHTS_VALUE

                    if global_attribute_weights_json[attr][key] > MAX_WEIGHTS_VALUE:
                        global_attribute_weights_json[attr][key] = MAX_WEIGHTS_VALUE
            
            print(f"{func}: After update weights: {global_attribute_weights_json}")
            json.dump(global_attribute_weights_json, open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, "w"))

            try:
                print(f"{func}: Upload file to S3: {BUCKET}/{path_to_global_attribute_weights}")
                S3_CLIENT.upload_file(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, BUCKET, path_to_global_attribute_weights)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {BUCKET}/{path_to_global_attribute_weights}")
                raise e


        def fetch_or_create_global_attribute_weights(path_to_global_attribute_weights):
            '''
            Get or initialize the global attribute weights file
            Args:
                path_to_global_attribute_weights: global attribute weights object key
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global BUCKET
            global TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME
            global INITIAL_WEIGHTS_VALUE
            func = "fetch_or_create_global_attribute_weights"

            try:
                print(
                    f"{func}: Checking if file exists on S3 path {BUCKET}/{path_to_global_attribute_weights}")

                S3_CLIENT.head_object(Bucket=BUCKET, Key=path_to_global_attribute_weights)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: File does not exist on S3. Initialize file")

                global_attribute_weights_json = {
                    "event": {
                        "technique": INITIAL_WEIGHTS_VALUE,
                        "tactic": INITIAL_WEIGHTS_VALUE,
                        "stage": INITIAL_WEIGHTS_VALUE,
                        "count": INITIAL_WEIGHTS_VALUE,
                        "priority": INITIAL_WEIGHTS_VALUE,
                        "port": INITIAL_WEIGHTS_VALUE,
                        "url": INITIAL_WEIGHTS_VALUE,
                        "user_agent": INITIAL_WEIGHTS_VALUE,
                        "cert": INITIAL_WEIGHTS_VALUE
                    },
                    "node": {
                        "os": INITIAL_WEIGHTS_VALUE,
                        "risk": INITIAL_WEIGHTS_VALUE,
                        "user": INITIAL_WEIGHTS_VALUE,
                        "domain": INITIAL_WEIGHTS_VALUE,
                        "subnet": INITIAL_WEIGHTS_VALUE,
                        "usergroup": INITIAL_WEIGHTS_VALUE,
                        "geolocation": INITIAL_WEIGHTS_VALUE
                    }
                }
                json.dump(global_attribute_weights_json, open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, "w"))

                print(f"{func}: Initiated file")
            else:
                try:
                    print(f"{func}: Download available file from S3")

                    S3_CLIENT.download_file(BUCKET, path_to_global_attribute_weights, TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {BUCKET}/{path_to_global_attribute_weights}")
                    raise e


        def get_campaign(ui_cookies, ui_headers, ui_session, campaign_id):
            '''
            Get campaign details by sending a request to the UI
            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                campaign_id: ID of the campaign
            Returns:
                JSON response containing the campaign
            '''
            global HOST

            func = "get_campaign"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/campaign/{campaign_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get campaign details for campaign id: {campaign_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response for campaign_id id: {campaign_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_raw_event(ui_cookies, ui_headers, ui_session, campaign_id, event_id):
            '''
            Get raw event details by event ID within a campaign.

            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                campaign_id: ID of the campaign
                event_id: ID of the event
            Returns:
                JSON response containing the raw event details
            '''
            global HOST

            func = "get_raw_event"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/campaign/{campaign_id}/event/{event_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get event details for event id: {event_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response for event id: {event_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_events(ui_cookies, ui_headers, ui_session, campaign_id):
            '''
            Get event details by campaign ID within a campaign.

            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                campaign_id: ID of the campaign
            Returns:
                JSON response containing the event details
            '''
            global HOST

            func = "get_events"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/campaign/{campaign_id}/event/bulk", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get event details")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response. Response: {response_from_ui.content}")
                raise json_err


        def get_action_attributes(ui_cookies, ui_headers, ui_session, action_id):
            '''
            Get action attributes by action ID.

            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                action_id: ID of the action

            Returns:
                JSON response containing the action attributes
            '''
            global HOST

            func = "get_action_attributes"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/action/{action_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get attributes from action id: {action_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response from action id: {action_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_ui_session():
            '''
            Get UI session and cookies
            Returns:
                cookies: Cookies for the connection to UI
                headers: Headers for the connection to UI
                s: session for the connection to UI
            '''
            global HOST
            global UI_PASSWORD
            global UI_USERNAME

            try:
                s = requests.Session()

                # Define the retry strategy
                retry_strategy = Retry(
                    total=2,  # Retry up to 5 times
                    status_forcelist=[429, 500, 502, 503, 504],  # Retry on these status codes
                    allowed_methods=["HEAD", "GET", "OPTIONS", "POST", "PATCH"],  # Methods to retry
                    backoff_factor=5  # Wait time between retries (1 second, 2 seconds, 4 seconds, etc.)
                )

                # Mount the retry strategy to the session
                adapter = requests.adapters.HTTPAdapter(max_retries=retry_strategy)
                s.mount("http://", adapter)
                s.mount("https://", adapter)

                response = s.get(HOST + "login")
                response.raise_for_status()

                cookies = s.cookies.get_dict()
                csrf = cookies.get("csrftoken")
                if not csrf:
                    raise ValueError("CSRF token not found in cookies")

                headers = {"X-CSRFToken": csrf, "Accept": "application/json"}

                response = s.post(f"{HOST}login/?next",
                                  data={"username": UI_USERNAME, "password": UI_PASSWORD, "csrfmiddlewaretoken": csrf})

                if response.status_code != 404 and not response.url.endswith("/accounts/profile/"):
                    response.raise_for_status()

                print("login successful")
                return cookies, headers, s

            except requests.exceptions.RequestException as e:
                print("Request error occurred.")
                raise e
            except Exception as e:
                print("An unexpected error occurred.")
                raise e


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref Bucket
          ui_lb_url: !GetAtt UILoadBalancer.DNSName
          ui_password: !Ref SuperuserPassword
          ui_username: !Ref SuperuserUsername
          db_connection_string: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/postgres
      Events:
        MyLogGroupTrigger:
          Type: CloudWatchLogs
          Properties:
            LogGroupName: !Ref ECSClusterLogGroup
            FilterPattern: Save campaign json
  saveFeedbackLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${saveFeedback}
  saveFeedbackLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref saveFeedback
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  EcsModelExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ecs-tasks.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-ecsModelPolicy"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:*
            - states:*
            - logs:*
            - iam:PassRole
            - ecs:*
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy
      - arn:aws:iam::aws:policy/AWSMarketplaceMeteringRegisterUsage

  EcsInstanceExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-RexrayPolicy"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ec2:AttachVolume
            - ec2:CreateVolume
            - ec2:CreateSnapshot
            - ec2:CreateTags
            - ec2:DeleteVolume
            - ec2:DeleteSnapshot
            - ec2:DescribeAvailabilityZones
            - ec2:DescribeInstances
            - ec2:DescribeVolumes
            - ec2:DescribeVolumeAttribute
            - ec2:DescribeVolumeStatus
            - ec2:DescribeSnapshots
            - ec2:CopySnapshot
            - ec2:DescribeSnapshotAttribute
            - ec2:DetachVolume
            - ec2:ModifySnapshotAttribute
            - ec2:ModifyVolumeAttribute
            - ec2:DescribeTag
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role

  EcsInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
      - !Ref EcsInstanceExecutionRole

  # UI ECS
  UITaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - UILoadBalancer
    Properties:
      Family: !Sub ui-task-${AWS::StackName}
      Cpu: !Ref Cpu
      Memory: !Ref Memory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      PlacementConstraints:
      - Type: memberOf
        Expression: !Sub
        - attribute:ecs.availability-zone==${AvailabilityZone}
        - AvailabilityZone: !Select
          - '0'
          - Fn::GetAZs: !Ref AWS::Region
      ContainerDefinitions:
      - Name: bastet_db
        Image: postgres:13-bullseye
        Cpu: 0
        PortMappings: []
        Essential: false
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_DB
          Value: bastet
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_ui_postgres_data"
          ContainerPath: /var/lib/postgresql/data/
          ReadOnly: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_redis
        Image: redis:6.2.6-alpine
        Cpu: 0
        PortMappings: []
        Essential: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_rabbitmq
        Image: rabbitmq:alpine
        Cpu: 0
        PortMappings:
        - Name: bastet_rabbitmq-5672-tcp
          ContainerPort: 5672
          HostPort: 5672
          Protocol: tcp
        Essential: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_web
        Image: !FindInMap [Images, WebContainerImage]
        Cpu: 0
        PortMappings:
        - Name: bastet_web-8000-tcp
          ContainerPort: 8000
          HostPort: 8000
          Protocol: tcp
        Essential: false
        Command:
        - ./entry.sh
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: bastet.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_EMAIL
          Value: !Ref SuperuserEmail
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_USERNAME
          Value: !Ref SuperuserUsername
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_PASSWORD
          Value: !Ref SuperuserPassword
        - Name: POSTGRES_DB_NAME
          Value: bastet
        - Name: AWS_REGION
          Value: !Ref AWS::Region
        - Name: MAX_USER_FIELD_MAPPING
          Value: 20
        - Name: LAMBDA_URL
          Value: !GetAtt createMappingUrl.FunctionUrl
        - Name: AWS_STORAGE_BUCKET_NAME
          Value: !Ref Bucket
        - Name: GEN_AI_MODEL
          Value: gpt-3.5-turbo
        MountPoints:
        - SourceVolume: static_volume
          ContainerPath: /code/static
          ReadOnly: false
        - SourceVolume: media_volume
          ContainerPath: /code/media
          ReadOnly: false
        - SourceVolume: !Sub "${AWS::StackName}_shared_temp"
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_db
          Condition: START
        - ContainerName: bastet_redis
          Condition: START
        - ContainerName: bastet_rabbitmq
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_celery
        Image: !FindInMap [Images, WebContainerImage]
        Cpu: 0
        Essential: true
        Command:
        - celery
        - -A
        - bastet
        - worker
        - -l
        - info
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: bastet.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: POSTGRES_DB_NAME
          Value: bastet
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_shared_temp"
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_web
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_nginx
        Image: !FindInMap [Images, NginxContainerImage]
        Cpu: 0
        PortMappings:
        - Name: bastet_nginx-80-tcp
          ContainerPort: 80
          HostPort: 80
          Protocol: tcp
        Essential: true
        MountPoints:
        - SourceVolume: static_volume
          ContainerPath: /home/app/web/staticfiles
          ReadOnly: false
        - SourceVolume: media_volume
          ContainerPath: /home/app/web/mediafiles
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_celery
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      Volumes:
      - Name: !Sub "${AWS::StackName}_ui_postgres_data"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '30'
            volumetype: gp2
      - Name: static_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: media_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: !Sub "${AWS::StackName}_shared_temp"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2

  UIEcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - UITaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Sub ui-service-${AWS::StackName}
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt UITaskDefinition.TaskDefinitionArn
      CapacityProviderStrategy:
      - CapacityProvider: !Ref UIEC2CapacityProvider
        Base: 0
        Weight: 1
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: bastet_nginx
        ContainerPort: 80
        TargetGroupArn: !GetAtt UITargetGroup.TargetGroupArn

  UILoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - UITargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${UILoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      Subnets:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      SecurityGroups:
      - !Ref SecurityGroup

  UITargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${UILoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      VpcId: !Ref VPC
      Port: 80
      Protocol: HTTP
      HealthCheckProtocol: HTTP
      HealthCheckPort: 80
      HealthCheckPath: /
      Matcher:
        HttpCode: 200-399
      TargetType: instance

  UIListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt UILoadBalancer.LoadBalancerArn
      Port: 8000
      Protocol: HTTP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt UITargetGroup.TargetGroupArn

  UIECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref DbECSOptimizedAMI
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        InstanceType: !Ref UIECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        UserData:
          Fn::Base64:
            Fn::Sub: |
              #!/bin/bash
              yum install -y aws-cfn-bootstrap
              /opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource UIECSLaunchTemplate
              /opt/aws/bin/cfn-signal -e $? --region ${AWS::Region} --stack ${AWS::StackName} --resource UIECSAutoScalingGroup
              exec 2>>/var/log/ecs/ecs-agent-install.log
              set -x
              until curl -s http://localhost:51678/v1/metadata
              do
                sleep 1
              done
              docker plugin install rexray/ebs REXRAY_PREEMPT=true EBS_REGION=${AWS::Region} --grant-all-permissions
              stop ecs
              start ecs

    Metadata:
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              aws-cli: []
              jq: []
              ecs-init: []
          commands:
            01_add_instance_to_cluster:
              command: !Sub echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config
            02_start_ecs_agent:
              command: start ecs
          files:
            /etc/cfn/cfn-hup.conf:
              mode: 256
              owner: root
              group: root
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.UIECSLaunchTemplate.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource UIECSLaunchTemplate
          services:
            sysvinit:
              cfn-hup:
                enabled: 'true'
                ensureRunning: 'true'
                files:
                - /etc/cfn/cfn-hup.conf
                - /etc/cfn/hooks.d/cfn-auto-reloader.conf

  UIECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 1
      LaunchTemplate:
        LaunchTemplateId: !Ref UIECSLaunchTemplate
        Version: !GetAtt UIECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      AvailabilityZones:
      - !Select
        - '0'
        - Fn::GetAZs: !Ref AWS::Region
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-ui-instance

  UIEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref UIECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # DB ECS
  DbTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub db-task-${AWS::StackName}
      Cpu: !Ref Cpu
      Memory: !Ref Memory
      NetworkMode: host
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      RequiresCompatibilities:
      - EC2
      PlacementConstraints:
      - Type: memberOf
        Expression: !Sub
        - attribute:ecs.availability-zone==${AvailabilityZone}
        - AvailabilityZone: !Select
          - '0'
          - Fn::GetAZs: !Ref AWS::Region
      ContainerDefinitions:
      - Name: lambda_db
        Image: postgres:13-bullseye
        Cpu: 0
        PortMappings:
        - Name: lambda_db-5432-tcp
          ContainerPort: 5432
          HostPort: 5432
          Protocol: tcp
        Essential: true
        Environment:
        - Name: POSTGRES_USER
          Value: !Ref DbUsername
        - Name: POSTGRES_PASSWORD
          Value: !Ref DbPassword
        - Name: POSTGRES_DB
          Value: lambda
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_postgres_data"
          ContainerPath: /var/lib/postgresql/data/
          ReadOnly: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      Volumes:
      - Name: !Sub "${AWS::StackName}_postgres_data"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2

  DbEcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - DbTaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Sub db-service-${AWS::StackName}
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt DbTaskDefinition.TaskDefinitionArn
      CapacityProviderStrategy:
      - CapacityProvider: !Ref DbEC2CapacityProvider
        Base: 0
        Weight: 1
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: lambda_db
        ContainerPort: 5432
        TargetGroupArn: !GetAtt DbTargetGroup.TargetGroupArn

  DbLoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - DbTargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${DbLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      Type: network
      Scheme: internet-facing
      Subnets:
      - !Ref PublicSubnet1
      SecurityGroups:
      - !Ref SecurityGroup

  DbTargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${DbLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      VpcId: !Ref VPC
      Port: 5432
      Protocol: TCP
      HealthCheckProtocol: TCP
      HealthCheckPort: 5432
      TargetType: instance

  DbListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt DbLoadBalancer.LoadBalancerArn
      Port: 5432
      Protocol: TCP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt DbTargetGroup.TargetGroupArn

  DbECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref DbECSOptimizedAMI
        InstanceType: !Ref DbECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        UserData:
          Fn::Base64:
            Fn::Sub: |
              #!/bin/bash
              yum install -y aws-cfn-bootstrap
              /opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSLaunchTemplate
              /opt/aws/bin/cfn-signal -e $? --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSAutoScalingGroup
              exec 2>>/var/log/ecs/ecs-agent-install.log
              set -x
              until curl -s http://localhost:51678/v1/metadata
              do
                sleep 1
              done
              docker plugin install rexray/ebs REXRAY_PREEMPT=true EBS_REGION=${AWS::Region} --grant-all-permissions
              stop ecs
              start ecs
      # Optionally, you can specify a LaunchTemplateName or other properties as needed

    Metadata:
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              aws-cli: []
              jq: []
              ecs-init: []
          commands:
            01_add_instance_to_cluster:
              command: !Sub echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config
            02_start_ecs_agent:
              command: start ecs
          files:
            /etc/cfn/cfn-hup.conf:
              mode: 256
              owner: root
              group: root
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.DbECSLaunchTemplate.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSLaunchTemplate
          services:
            sysvinit:
              cfn-hup:
                enabled: 'true'
                ensureRunning: 'true'
                files:
                - /etc/cfn/cfn-hup.conf
                - /etc/cfn/hooks.d/cfn-auto-reloader.conf

  DbECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 1
      LaunchTemplate:
        LaunchTemplateId: !Ref DbECSLaunchTemplate
        Version: !GetAtt DbECSLaunchTemplate.LatestVersionNumber
      # LaunchConfigurationName: !Ref DbECSLaunchConfiguration
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      AvailabilityZones:
      - !Select
        - '0'
        - Fn::GetAZs: !Ref AWS::Region
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-db-instance

  DbEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref DbECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Tech Model ECS
  TechTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub tech-model-task-${AWS::StackName}
      Memory: !Ref TechTaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !FindInMap [Images, TechModelContainerImage]
        Cpu: 0
        Essential: true
        EntryPoint:
        - main
        ResourceRequirements:
        - Type: GPU
          Value: '1'
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: NVIDIA_DRIVER_CAPABILITIES
          Value: all
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  TechECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref GpuECSOptimizedAMI
        InstanceType: !Ref TechECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '45'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
            echo ECS_ENABLE_GPU_SUPPORT=true >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  TechECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref TechECSLaunchTemplate
        Version: !GetAtt TechECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-tech-instance

  TechEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref TechECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Cluster part 1 Model ECS
  ClusterPart1TaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub cluster-model-part-1-task-${AWS::StackName}
      Memory: !Ref ClusterPart1TaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !FindInMap [Images, ClusterModelPart1ContainerImage]
        Cpu: 0
        Essential: true
        EntryPoint:
        - main
        ResourceRequirements:
        - Type: GPU
          Value: '1'
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: NVIDIA_DRIVER_CAPABILITIES
          Value: all
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  ClusterPart1ECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref GpuECSOptimizedAMI
        InstanceType: !Ref ClusterPart1ECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '45'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
            echo ECS_ENABLE_GPU_SUPPORT=true >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  ClusterPart1ECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref ClusterPart1ECSLaunchTemplate
        Version: !GetAtt ClusterPart1ECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-cluster-1-instance

  ClusterPart1EC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref ClusterPart1ECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Cluster part 2 Model ECS
  ClusterPart2TaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub cluster-model-part-2-task-${AWS::StackName}
      Memory: !Ref ClusterPart2TaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !FindInMap [Images, ClusterModelPart2ContainerImage]
        Cpu: 0
        Essential: true
        EntryPoint:
        - main
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  ClusterPart2ECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        InstanceType: !Ref ClusterPart2ECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  ClusterPart2ECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref ClusterPart2ECSLaunchTemplate
        Version: !GetAtt ClusterPart2ECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-cluster-2-instance

  ClusterPart2EC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn: MyWaitCondition
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref ClusterPart2ECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Flow Model ECS
  FlowTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub flow-model-task-${AWS::StackName}
      Memory: !Ref FlowTaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !FindInMap [Images, FlowModelContainerImage]
        Cpu: 0
        Essential: true
        EntryPoint:
        - main
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  FlowECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        InstanceType: !Ref FlowECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  FlowECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref FlowECSLaunchTemplate
        Version: !GetAtt FlowECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-flow-instance

  FlowEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn: MyWaitCondition
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref FlowECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Lambda ECS
  LambdaTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    Properties:
      Family: !Sub lambda-task-${AWS::StackName}
      Memory: !Ref LambdaTaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: lambda
        Image: !FindInMap [Images, LambdaContainerImage]
        Cpu: 0
        Essential: true
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: tac_threshold
          Value: '0'
        - Name: db_connection_string
          Value: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/postgres
        - Name: chunk_size
          Value: !Ref ChunkSize
        - Name: campaign_map
          Value: cluster
        - Name: technique_lookup_object
          Value: scratch/lambda/data.csv
        - Name: embedding_task_arn
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: max_cluster_count
          Value: !Ref MaxClusterCount
        - Name: encode_node_feature
          Value: 'true'
        - Name: bucket
          Value: !Ref Bucket
        - Name: cluster_task_arn
          Value: !GetAtt ClusterPart2TaskDefinition.TaskDefinitionArn
        - Name: event_threshold
          Value: !Ref EventThreshold
        - Name: technique_lookup_limit
          Value: !Ref TechLookupLimit
        - Name: ui_password
          Value: !Ref SuperuserPassword
        - Name: ui_username
          Value: !Ref SuperuserUsername
        - Name: ecs_cluster_arn
          Value: !GetAtt ECSCluster.Arn
        - Name: encode_other_attrs
          Value: 'true'
        - Name: tech_task_arn
          Value: !GetAtt TechTaskDefinition.TaskDefinitionArn
        - Name: flow_input_window_size
          Value: !Ref FlowInputMaxClusters
        - Name: max_flow_count
          Value: !Ref MaxFlowCount
        - Name: map_cef_to_internal
          Value: 'true'
        - Name: skip_single_alert
          Value: 'true'
        - Name: ui_lb_url
          Value: !GetAtt UILoadBalancer.DNSName
        - Name: flow_task_arn
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: clustering_min_alerts
          Value: '5'
        - Name: ui_upload_limit
          Value: '100'
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  LambdaECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        InstanceType: !Ref LambdaECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  LambdaECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref LambdaECSLaunchTemplate
        Version: !GetAtt LambdaECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-lambda-instance

  LambdaEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn: MyWaitCondition
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref LambdaECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Airflow ECS
  AirflowTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - AirflowLoadBalancer
    Properties:
      Family: !Sub airflow-task-${AWS::StackName}
      TaskRoleArn: !Ref EcsModelExecutionRole
      ExecutionRoleArn: !Ref EcsModelExecutionRole
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      Memory: !Ref AirflowTaskMemory
      PlacementConstraints:
      - Type: memberOf
        Expression: !Sub
        - attribute:ecs.availability-zone==${AvailabilityZone}
        - AvailabilityZone: !Select
          - '0'
          - Fn::GetAZs: !Ref AWS::Region
      Volumes:
      - Name: !Sub "${AWS::StackName}_postgres-db-volume"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2
      - Name: !Sub "${AWS::StackName}_dags_volume"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2
      - Name: !Sub "${AWS::StackName}_logs_volume"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '30'
            volumetype: gp2
      - Name: !Sub "${AWS::StackName}_config_volume"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2
      - Name: !Sub "${AWS::StackName}_plugins_volume"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2
      ContainerDefinitions:
      - Name: postgres
        Image: postgres:13
        Cpu: 0
        PortMappings:
        - Name: postgres-5432-tcp
          ContainerPort: 5432
          HostPort: 5432
          Protocol: tcp
        Essential: true
        Environment:
        - Name: POSTGRES_USER
          Value: airflow
        - Name: POSTGRES_PASSWORD
          Value: airflow
        - Name: POSTGRES_DB
          Value: airflow
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_postgres-db-volume"
          ContainerPath: /var/lib/postgresql/data
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD
          - pg_isready
          - -U
          - airflow
          Interval: 10
          Timeout: 5
          Retries: 5
          StartPeriod: 5
        SystemControls: []
        VolumesFrom: []
      - Name: redis
        Image: redis:latest
        Cpu: 0
        PortMappings:
        - Name: redis-6379-tcp
          ContainerPort: 6379
          HostPort: 6379
          Protocol: tcp
        Essential: true
        Environment: []
        MountPoints: []
        VolumesFrom: []
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD
          - redis-cli
          - ping
          Interval: 10
          Timeout: 5
          Retries: 5
          StartPeriod: 10
        SystemControls: []
      - Name: airflow-webserver
        Image: !FindInMap [Images, AirflowContainerImage]
        Cpu: 0
        PortMappings:
        - Name: airflow-webserver-8080-tcp
          ContainerPort: 8080
          HostPort: 8080
          Protocol: tcp
        Essential: true
        Command:
        - webserver
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: BUCKET_NAME
          Value: !Ref Bucket
        - Name: INPUT_PREFIX
          Value: input/
        - Name: INPUT_SUFFIX
          Value: .json
        - Name: ECS_CLUSTER
          Value: !Ref ECSCluster
        - Name: LAMBDA_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_CONTAINER_NAME
          Value: lambda
        - Name: LAMBDA_CP
          Value: !Ref LambdaEC2CapacityProvider
        - Name: TECH_MODEL_CP
          Value: !Ref TechEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_CP
          Value: !Ref ClusterPart1EC2CapacityProvider
        - Name: CLUSTER_2_MODEL_CP
          Value: !Ref ClusterPart2EC2CapacityProvider
        - Name: FLOW_MODEL_CP
          Value: !Ref FlowEC2CapacityProvider
        - Name: TECH_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt TechTaskDefinition.TaskDefinitionArn
        - Name: CLUSTER_1_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: CLUSTER_2_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart2TaskDefinition.TaskDefinitionArn
        - Name: FLOW_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: MODEL_ECS_CONTAINER_NAME
          Value: model
        - Name: QUEUE_NAME
          Value: job_queue
        - Name: UI_LB_URL
          Value: !GetAtt UILoadBalancer.DNSName
        - Name: UI_USERNAME
          Value: !Ref SuperuserUsername
        - Name: UI_PASSWORD
          Value: !Ref SuperuserPassword
        - Name: AIRFLOW_LB_URL
          Value: !GetAtt AirflowLoadBalancer.DNSName
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_dags_volume"
          ContainerPath: /opt/airflow/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /opt/airflow/logs
        - SourceVolume: !Sub "${AWS::StackName}_config_volume"
          ContainerPath: /opt/airflow/config
        - SourceVolume: !Sub "${AWS::StackName}_plugins_volume"
          ContainerPath: /opt/airflow/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        - ContainerName: airflow-init
          Condition: SUCCESS
        - ContainerName: airflow-triggerer
          Condition: START
        - ContainerName: airflow-worker
          Condition: START
        - ContainerName: airflow-scheduler
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD
          - curl
          - --fail
          - http://localhost:8080/health
          Interval: 30
          Timeout: 10
          Retries: 5
          StartPeriod: 30
        SystemControls: []
      - Name: airflow-scheduler
        Image: !FindInMap [Images, AirflowContainerImage]
        Cpu: 0
        PortMappings: []
        Essential: true
        Command:
        - scheduler
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: BUCKET_NAME
          Value: !Ref Bucket
        - Name: INPUT_PREFIX
          Value: input/
        - Name: INPUT_SUFFIX
          Value: .json
        - Name: ECS_CLUSTER
          Value: !Ref ECSCluster
        - Name: LAMBDA_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_CONTAINER_NAME
          Value: lambda
        - Name: LAMBDA_CP
          Value: !Ref LambdaEC2CapacityProvider
        - Name: TECH_MODEL_CP
          Value: !Ref TechEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_CP
          Value: !Ref ClusterPart1EC2CapacityProvider
        - Name: CLUSTER_2_MODEL_CP
          Value: !Ref ClusterPart2EC2CapacityProvider
        - Name: FLOW_MODEL_CP
          Value: !Ref FlowEC2CapacityProvider
        - Name: TECH_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt TechTaskDefinition.TaskDefinitionArn
        - Name: CLUSTER_1_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: CLUSTER_2_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart2TaskDefinition.TaskDefinitionArn
        - Name: FLOW_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: MODEL_ECS_CONTAINER_NAME
          Value: model
        - Name: QUEUE_NAME
          Value: job_queue
        - Name: UI_LB_URL
          Value: !GetAtt UILoadBalancer.DNSName
        - Name: UI_USERNAME
          Value: !Ref SuperuserUsername
        - Name: UI_PASSWORD
          Value: !Ref SuperuserPassword
        - Name: AIRFLOW_LB_URL
          Value: !GetAtt AirflowLoadBalancer.DNSName
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_dags_volume"
          ContainerPath: /opt/airflow/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /opt/airflow/logs
        - SourceVolume: !Sub "${AWS::StackName}_config_volume"
          ContainerPath: /opt/airflow/config
        - SourceVolume: !Sub "${AWS::StackName}_plugins_volume"
          ContainerPath: /opt/airflow/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        - ContainerName: airflow-init
          Condition: SUCCESS
        - ContainerName: airflow-triggerer
          Condition: START
        - ContainerName: airflow-worker
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD
          - curl
          - --fail
          - http://localhost:8974/health
          Interval: 30
          Timeout: 10
          Retries: 5
          StartPeriod: 30
        SystemControls: []
      - Name: airflow-worker
        Image: !FindInMap [Images, AirflowContainerImage]
        Cpu: 0
        PortMappings: []
        Essential: true
        Command:
        - celery
        - worker
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: DUMB_INIT_SETSID
          Value: '0'
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: BUCKET_NAME
          Value: !Ref Bucket
        - Name: INPUT_PREFIX
          Value: input/
        - Name: INPUT_SUFFIX
          Value: .json
        - Name: ECS_CLUSTER
          Value: !Ref ECSCluster
        - Name: LAMBDA_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_CONTAINER_NAME
          Value: lambda
        - Name: LAMBDA_CP
          Value: !Ref LambdaEC2CapacityProvider
        - Name: TECH_MODEL_CP
          Value: !Ref TechEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_CP
          Value: !Ref ClusterPart1EC2CapacityProvider
        - Name: CLUSTER_2_MODEL_CP
          Value: !Ref ClusterPart2EC2CapacityProvider
        - Name: FLOW_MODEL_CP
          Value: !Ref FlowEC2CapacityProvider
        - Name: TECH_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt TechTaskDefinition.TaskDefinitionArn
        - Name: CLUSTER_1_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: CLUSTER_2_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart2TaskDefinition.TaskDefinitionArn
        - Name: FLOW_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: MODEL_ECS_CONTAINER_NAME
          Value: model
        - Name: QUEUE_NAME
          Value: job_queue
        - Name: UI_LB_URL
          Value: !GetAtt UILoadBalancer.DNSName
        - Name: UI_USERNAME
          Value: !Ref SuperuserUsername
        - Name: UI_PASSWORD
          Value: !Ref SuperuserPassword
        - Name: AIRFLOW_LB_URL
          Value: !GetAtt AirflowLoadBalancer.DNSName
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_dags_volume"
          ContainerPath: /opt/airflow/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /opt/airflow/logs
        - SourceVolume: !Sub "${AWS::StackName}_config_volume"
          ContainerPath: /opt/airflow/config
        - SourceVolume: !Sub "${AWS::StackName}_plugins_volume"
          ContainerPath: /opt/airflow/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        - ContainerName: airflow-init
          Condition: SUCCESS
        - ContainerName: airflow-triggerer
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        SystemControls: []
      - Name: airflow-triggerer
        Image: !FindInMap [Images, AirflowContainerImage]
        Cpu: 0
        PortMappings: []
        Essential: true
        Command:
        - triggerer
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: BUCKET_NAME
          Value: !Ref Bucket
        - Name: INPUT_PREFIX
          Value: input/
        - Name: INPUT_SUFFIX
          Value: .json
        - Name: ECS_CLUSTER
          Value: !Ref ECSCluster
        - Name: LAMBDA_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_CONTAINER_NAME
          Value: lambda
        - Name: LAMBDA_CP
          Value: !Ref LambdaEC2CapacityProvider
        - Name: TECH_MODEL_CP
          Value: !Ref TechEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_CP
          Value: !Ref ClusterPart1EC2CapacityProvider
        - Name: CLUSTER_2_MODEL_CP
          Value: !Ref ClusterPart2EC2CapacityProvider
        - Name: FLOW_MODEL_CP
          Value: !Ref FlowEC2CapacityProvider
        - Name: TECH_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt TechTaskDefinition.TaskDefinitionArn
        - Name: CLUSTER_1_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: CLUSTER_2_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart2TaskDefinition.TaskDefinitionArn
        - Name: FLOW_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: MODEL_ECS_CONTAINER_NAME
          Value: model
        - Name: QUEUE_NAME
          Value: job_queue
        - Name: UI_LB_URL
          Value: !GetAtt UILoadBalancer.DNSName
        - Name: UI_USERNAME
          Value: !Ref SuperuserUsername
        - Name: UI_PASSWORD
          Value: !Ref SuperuserPassword
        - Name: AIRFLOW_LB_URL
          Value: !GetAtt AirflowLoadBalancer.DNSName
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_dags_volume"
          ContainerPath: /opt/airflow/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /opt/airflow/logs
        - SourceVolume: !Sub "${AWS::StackName}_config_volume"
          ContainerPath: /opt/airflow/config
        - SourceVolume: !Sub "${AWS::StackName}_plugins_volume"
          ContainerPath: /opt/airflow/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        - ContainerName: airflow-init
          Condition: SUCCESS
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD-SHELL
          - airflow jobs check --job-type TriggererJob --hostname "$(HOSTNAME)"
          Interval: 30
          Timeout: 10
          Retries: 5
          StartPeriod: 30
        SystemControls: []
      - Name: airflow-init
        Image: !FindInMap [Images, AirflowContainerImage]
        Cpu: 0
        PortMappings: []
        Essential: false
        Command:
        - bash
        - -c
        - echo completed
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _AIRFLOW_WWW_USER_CREATE
          Value: 'true'
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: _AIRFLOW_WWW_USER_USERNAME
          Value: airflow
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: _AIRFLOW_DB_MIGRATE
          Value: 'true'
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: _AIRFLOW_WWW_USER_PASSWORD
          Value: airflow
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_dags_volume"
          ContainerPath: /sources/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /sources/logs
        - SourceVolume: !Sub "${AWS::StackName}_config_volume"
          ContainerPath: /sources/config
        - SourceVolume: !Sub "${AWS::StackName}_plugins_volume"
          ContainerPath: /sources/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        User: 0:0
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        SystemControls: []

  AirflowEcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - AirflowTaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Sub airflow-service-${AWS::StackName}
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt AirflowTaskDefinition.TaskDefinitionArn
      CapacityProviderStrategy:
      - CapacityProvider: !Ref AirflowEC2CapacityProvider
        Base: 0
        Weight: 1
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: airflow-webserver
        ContainerPort: 8080
        TargetGroupArn: !GetAtt AirflowTargetGroup.TargetGroupArn

  AirflowLoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - AirflowTargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${AirflowLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      Subnets:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      SecurityGroups:
      - !Ref SecurityGroup

  AirflowTargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${AirflowLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      VpcId: !Ref VPC
      Port: 8080
      Protocol: HTTP
      HealthCheckProtocol: HTTP
      HealthCheckPort: 8080
      HealthCheckPath: /
      HealthCheckTimeoutSeconds: 20
      HealthCheckIntervalSeconds: 60
      HealthyThresholdCount: 2
      UnhealthyThresholdCount: 5
      Matcher:
        HttpCode: 200-399
      TargetType: instance

  AirflowListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt AirflowLoadBalancer.LoadBalancerArn
      Port: 8080
      Protocol: HTTP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt AirflowTargetGroup.TargetGroupArn

  AirflowECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref DbECSOptimizedAMI
        InstanceType: !Ref AirflowECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        UserData:
          Fn::Base64:
            Fn::Sub: |
              #!/bin/bash
              yum install -y aws-cfn-bootstrap
              /opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource AirflowECSLaunchTemplate
              /opt/aws/bin/cfn-signal -e $? --region ${AWS::Region} --stack ${AWS::StackName} --resource AirflowECSAutoScalingGroup
              exec 2>>/var/log/ecs/ecs-agent-install.log
              set -x
              until curl -s http://localhost:51678/v1/metadata
              do
                sleep 1
              done
              docker plugin install rexray/ebs REXRAY_PREEMPT=true EBS_REGION=${AWS::Region} --grant-all-permissions
              stop ecs
              start ecs
      # Optionally, you can specify a LaunchTemplateName or other properties as needed

    Metadata:
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              aws-cli: []
              jq: []
              ecs-init: []
          commands:
            01_add_instance_to_cluster:
              command: !Sub echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config
            02_start_ecs_agent:
              command: start ecs
          files:
            /etc/cfn/cfn-hup.conf:
              mode: 256
              owner: root
              group: root
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.AirflowECSLaunchTemplate.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource AirflowECSLaunchTemplate
          services:
            sysvinit:
              cfn-hup:
                enabled: 'true'
                ensureRunning: 'true'
                files:
                - /etc/cfn/cfn-hup.conf
                - /etc/cfn/hooks.d/cfn-auto-reloader.conf

  AirflowECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 1
      LaunchTemplate:
        LaunchTemplateId: !Ref AirflowECSLaunchTemplate
        Version: !GetAtt AirflowECSLaunchTemplate.LatestVersionNumber
      # LaunchConfigurationName: !Ref DbECSLaunchConfiguration
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      AvailabilityZones:
      - !Select
        - '0'
        - Fn::GetAZs: !Ref AWS::Region
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-airflow-instance

  AirflowEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn: MyWaitCondition
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref AirflowECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  ECSCluster:
    Type: AWS::ECS::Cluster
    DependsOn:
    - EcsInstanceProfile
    Properties:
      ClusterName: !Sub ${AWS::StackName}-cluster
      ClusterSettings:
      - Name: containerInsights
        Value: disabled
      Configuration:
        ExecuteCommandConfiguration:
          Logging: DEFAULT
      ServiceConnectDefaults:
        Namespace: !Sub ${AWS::StackName}-cluster
      Tags: []
  ECSClusterLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /ecs/${AWS::StackName}-cluster

  ClusterCPAssociation:
    Type: AWS::ECS::ClusterCapacityProviderAssociations
    DependsOn:
    - ECSCluster
    Properties:
      Cluster: !Sub ${AWS::StackName}-cluster
      CapacityProviders:
      - !Ref LambdaEC2CapacityProvider
      - !Ref AirflowEC2CapacityProvider
      - !Ref DbEC2CapacityProvider
      - !Ref UIEC2CapacityProvider
      - !Ref TechEC2CapacityProvider
      - !Ref ClusterPart1EC2CapacityProvider
      - !Ref ClusterPart2EC2CapacityProvider
      - !Ref FlowEC2CapacityProvider
      DefaultCapacityProviderStrategy:
      - Base: 0
        Weight: 1
        CapacityProvider: !Ref DbEC2CapacityProvider

  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCidr
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-InternetGateway"

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    DependsOn:
    - VPC
    - InternetGateway
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet1Cidr
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [0, !GetAZs ""]
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSubnet1"

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet2Cidr
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [1, !GetAZs ""]
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSubnet2"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicRouteTable"

  Route:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation1:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref RouteTable

  SubnetRouteTableAssociation2:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref RouteTable

  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Allow HTTP
      VpcId: !Ref VPC
      SecurityGroupIngress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      SecurityGroupEgress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSecurityGroup"

  MyWaitHandle:
    Type: AWS::CloudFormation::WaitConditionHandle

  MyWaitCondition:
    Type: AWS::CloudFormation::WaitCondition
    DependsOn:
    - UIEC2CapacityProvider
    - DbEC2CapacityProvider
    - TechEC2CapacityProvider
    - ClusterPart1EC2CapacityProvider
    Properties:
      Handle: !Ref MyWaitHandle
      Timeout: '120'  # Time in seconds to wait (e.g., 5 minutes)
      Count: 0  # No signals required, just wait for the timeout

Outputs:
  CypientaBucket:
    Description: The bucket created
    Value: !Ref Bucket

  CypientaUI:
    Description: The DNS name of the load balancer
    Value: !Sub "http://${UILoadBalancer.DNSName}:8000"

  CypientaAirflow:
    Description: The DNS name of the load balancer
    Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
