Transform: AWS::Serverless-2016-10-31
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
    - Label:
        default: General S3 Parameters
      Parameters:
      - BucketName

    - Label:
        default: SageMaker Model ARN
      Parameters:
      - TechniqueModelARN
      - ClusterModelARN1
      - ClusterModelARN
      - FlowModelARN

    - Label:
        default: Container images
      Parameters:
      - VRLLambdaImage
      - NginxContainerImage
      - WebContainerImage

    - Label:
        default: UI admin credentials
      Parameters:
      - SuperuserEmail
      - SuperuserUsername
      - SuperuserPassword

    - Label:
        default: Load Balancer Configuration
      Parameters:
      - LoadBalancerName

    - Label:
        default: SageMaker Model Parameters
      Parameters:
      - TechniqueModelName
      - ClusterModelName1
      - ClusterModelName
      - FlowModelName

    - Label:
        default: Input processing Parameters
      Parameters:
      - TechniqueLookupObject
      - ChunkSize
      - FlowInputMaxClusters

    - Label:
        default: Technique detector Batch Job Parameters
      Parameters:
      - EnrichTechBatchInstanceType
      - EnrichTechBatchInstanceCount

    - Label:
        default: Temporal clustering 1 Batch Job Parameters
      Parameters:
      - ClusterBatchInstanceType1
      - ClusterBatchBatchInstanceCount1

    - Label:
        default: Temporal clustering Batch Job Parameters
      Parameters:
      - ClusterBatchInstanceType

    - Label:
        default: Flow detector Batch Job Parameters
      Parameters:
      - FlowBatchInstanceType

    - Label:
        default: UI lambda function Parameters
      Parameters:
      - ClusterOrFlowMapToCampaign
      - EventThreshold
      - TacticThreshold

    - Label:
        default: ECS Configuration
      Parameters:
      - ECSClusterName
      - ServiceAndTaskDefinitionName
      - Cpu
      - Memory
      - LatestECSOptimizedAMI

    - Label:
        default: Auto Scaling Configuration
      Parameters:
      - ClusterAutoScalingMinSize
      - ClusterAutoScalingMaxSize

    - Label:
        default: VPC Configuration
      Parameters:
      - VPCNamePrefix
      - VpcCidr
      - Subnet1Cidr
      - Subnet2Cidr

Parameters:
  BucketName:
    Type: String
    Description: Name of the S3 bucket to create.
    MinLength: 1

  TechniqueModelName:
    Type: String
    Description: Name of Sagemaker model for technique classification
    Default: technique-model
    MinLength: 1
  TechniqueModelARN:
    Type: String
    Description: Model package ARN for the technique model subscribed from marketplace
    MinLength: 1
    AllowedPattern: ^arn:aws(-[a-z]+)*:[a-zA-Z0-9\-]+:[a-z0-9\-]*:[0-9]{12}:(.+)$
    ConstraintDescription: Enter a valid AWS ARN for technique model

  ClusterModelName1:
    Type: String
    Description: Name of Sagemaker model for cluster detection part 1
    Default: cluster1-model
    MinLength: 1
  ClusterModelARN1:
    Type: String
    Description: Model package ARN for the cluster part 1 model subscribed from marketplace
    MinLength: 1
    AllowedPattern: ^arn:aws(-[a-z]+)*:[a-zA-Z0-9\-]+:[a-z0-9\-]*:[0-9]{12}:(.+)$
    ConstraintDescription: Enter a valid AWS ARN for technique model

  ClusterModelName:
    Type: String
    Description: Name of Sagemaker model for cluster detection part 2
    Default: cluster-model
    MinLength: 1
  ClusterModelARN:
    Type: String
    Description: Model package ARN for the cluster part 2 model subscribed from marketplace
    MinLength: 1
    AllowedPattern: ^arn:aws(-[a-z]+)*:[a-zA-Z0-9\-]+:[a-z0-9\-]*:[0-9]{12}:(.+)$
    ConstraintDescription: Enter a valid AWS ARN for technique model

  FlowModelName:
    Type: String
    Description: Name of Sagemaker model for flow detection
    Default: flow-model
    MinLength: 1
  FlowModelARN:
    Type: String
    Description: Model package ARN for the flow model subscribed from marketplace
    MinLength: 1
    AllowedPattern: ^arn:aws(-[a-z]+)*:[a-zA-Z0-9\-]+:[a-z0-9\-]*:[0-9]{12}:(.+)$
    ConstraintDescription: Enter a valid AWS ARN for technique model

  VRLLambdaImage:
    Description: VRL Lambda ECR container image URI
    Type: String
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+:[a-zA-Z0-9.-]+$

  TechniqueLookupObject:
    Type: String
    Description: Path to S3 object where you want to save lookup table for previously
      classified techniques for alerts. The object file must have an extension of
      '.csv'
    Default: scratch/lambda/data.csv
    MinLength: 5

  ChunkSize:
    Type: Number
    Description: Size of single chunk of input to be processed at a time for an input
      file
    Default: 20000

  EnrichTechBatchInstanceType:
    Type: String
    Default: ml.p2.xlarge
    Description: EC2 instance type for the enrich technology batch transform job.
    AllowedValues:
    - ml.p2.xlarge
    - ml.p2.8xlarge
    - ml.p2.16xlarge
    - ml.p3.2xlarge
    - ml.p3.8xlarge
    - ml.p3.16xlarge

  ClusterBatchInstanceType1:
    Type: String
    Default: ml.p3.2xlarge
    Description: EC2 instance type for the cluster part 1 batch transform job.
    AllowedValues:
    - ml.p3.2xlarge
    - ml.p3.8xlarge
    - ml.p3.16xlarge

  ClusterBatchInstanceType:
    Type: String
    Default: ml.c5.4xlarge
    Description: EC2 instance type for the cluster part 2 batch transform job.
    AllowedValues:
    - ml.m5.xlarge
    - ml.m5.2xlarge
    - ml.m5.4xlarge
    - ml.m5.12xlarge
    - ml.m5.24xlarge
    - ml.m4.xlarge
    - ml.m4.2xlarge
    - ml.m4.4xlarge
    - ml.m4.10xlarge
    - ml.c5.xlarge
    - ml.c5.2xlarge
    - ml.c5.4xlarge
    - ml.c5.9xlarge
    - ml.c5.18xlarge
    - ml.c4.xlarge
    - ml.c4.2xlarge
    - ml.c4.4xlarge
    - ml.c4.8xlarge
    - ml.m4.16xlarge

  FlowBatchInstanceType:
    Type: String
    Default: ml.c5.4xlarge
    Description: EC2 instance type for the flow batch transform job.
    AllowedValues:
    - ml.m5.xlarge
    - ml.m5.2xlarge
    - ml.m5.4xlarge
    - ml.m5.12xlarge
    - ml.m5.24xlarge
    - ml.m4.xlarge
    - ml.m4.2xlarge
    - ml.m4.4xlarge
    - ml.m4.10xlarge
    - ml.c5.xlarge
    - ml.c5.2xlarge
    - ml.c5.4xlarge
    - ml.c5.9xlarge
    - ml.c5.18xlarge
    - ml.c4.xlarge
    - ml.c4.2xlarge
    - ml.c4.4xlarge
    - ml.c4.8xlarge
    - ml.m4.16xlarge

  EnrichTechBatchInstanceCount:
    Type: Number
    Description: Parallel instance count for Technique classification batch transform
      job. Set the value based on the available service quota for batch tranform job
      for instance type set in EnrichTechBatchInstanceType.
    Default: 1

  ClusterBatchBatchInstanceCount1:
    Type: Number
    Description: Parallel instance count for Temporal clustering part 1 batch transform
      job. Set the value based on the available service quota for batch tranform job
      for instance type set in ClusterBatchInstanceType1.
    Default: 1

  FlowInputMaxClusters:
    Type: Number
    Description: Number of clusters as input to Flow detection model to be processed
      at a time for an input file
    Default: 5000

  ClusterOrFlowMapToCampaign:
    Type: String
    Default: cluster
    AllowedValues:
    - cluster
    - flow

  EventThreshold:
    Type: Number
    Default: 2
    Description: Number of events present in the cluster or flow to create campaign
      on UI. Minimum allowed value '1'.
    MinValue: 1

  TacticThreshold:
    Type: Number
    Default: 0
    Description: Number of tactics present in an event to be part of the campaign.
      Minimum allowed value '0'.
    MinValue: 0

  ECSClusterName:
    Type: String
    Description: Specifies the ECS Cluster Name with which the resources would be
      associated
    Default: cypienta-cluster
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  ECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for EC2
    Default: t3a.large
    AllowedValues:
    - t2.micro
    - t2.small
    - t2.medium
    - t2.large
    - t3.micro
    - t3.small
    - t3.medium
    - t3.large
    - t3a.micro
    - t3a.small
    - t3a.medium
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.medium
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  ServiceAndTaskDefinitionName:
    Description: Name of the ECS task definition and ECS Service
    Type: String
    Default: cypienta-ui
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  Cpu:
    Description: Number of CPU units used by the task. 1 vCPU = 1024
    Type: String
    Default: 1024
    AllowedValues:
    - 256
    - 512
    - 1024
    - 2048
    - 4096
    - 8192
    - 16384

  Memory:
    Description: Amount of memory (in MiB) used by the task. 1 GB = 1024
    Type: String
    Default: 4096
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  SuperuserEmail:
    Description: Email of superuser
    Type: String
    Default: admin@admin.com
    AllowedPattern: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$
    ConstraintDescription: Must be a valid email address

  SuperuserUsername:
    Description: Superuser username. Minimum length of 3.
    Type: String
    Default: maestro
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  SuperuserPassword:
    Description: Superuser password. Minimum length of 8
    Type: String
    Default: changemenow
    MinLength: 8
    ConstraintDescription: Must be minimum length of 8

  NginxContainerImage:
    Description: Nginx container image with tag nginx-market*
    Type: String
    Default: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/cytech:nginx-marketv0.0.3
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+/[a-z0-9-]+:[a-zA-Z0-9.-]+$
  WebContainerImage:
    Description: Container image for web app with tag market*
    Type: String
    Default: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/cytech:marketv0.2.3
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+/[a-z0-9-]+:[a-zA-Z0-9.-]+$

  VPCNamePrefix:
    Description: Prefix for naming VPC and corresponding resources
    Type: String
    Default: cypienta
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  VpcCidr:
    Description: The CIDR block for the VPC
    Type: String
    Default: 10.0.0.0/16
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  Subnet1Cidr:
    Description: The CIDR block for the first public subnet
    Type: String
    Default: 10.0.0.0/20
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  Subnet2Cidr:
    Description: The CIDR block for the second public subnet
    Type: String
    Default: 10.0.16.0/20
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  LoadBalancerName:
    Description: Name of Load balancer
    Type: String
    Default: cypienta-ui
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  ClusterAutoScalingMinSize:
    Description: The minimum size of the auto scaling group for the ECS cluster
    Type: String
    Default: 0
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterAutoScalingMaxSize:
    Description: The maximum size of the auto scaling group for the ECS cluster
    Type: String
    Default: 5
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  LatestECSOptimizedAMI:
    Description: AMI ID
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/recommended/image_id

Mappings:
  RegionMap:
    af-south-1:
      lambdaLayer: arn:aws:lambda:af-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-3:336392948345:layer:AWSSDKPandas-Python311:12
    ap-south-1:
      lambdaLayer: arn:aws:lambda:ap-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ca-central-1:
      lambdaLayer: arn:aws:lambda:ca-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-central-1:
      lambdaLayer: arn:aws:lambda:eu-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-north-1:
      lambdaLayer: arn:aws:lambda:eu-north-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-1:
      lambdaLayer: arn:aws:lambda:eu-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-2:
      lambdaLayer: arn:aws:lambda:eu-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-3:
      lambdaLayer: arn:aws:lambda:eu-west-3:336392948345:layer:AWSSDKPandas-Python311:12
    sa-east-1:
      lambdaLayer: arn:aws:lambda:sa-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-1:
      lambdaLayer: arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-2:
      lambdaLayer: arn:aws:lambda:us-east-2:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-1:
      lambdaLayer: arn:aws:lambda:us-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-2:
      lambdaLayer: arn:aws:lambda:us-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-east-1:
      lambdaLayer: arn:aws:lambda:ap-east-1:839552336658:layer:AWSSDKPandas-Python311:14
    ap-south-2:
      lambdaLayer: arn:aws:lambda:ap-south-2:246107603503:layer:AWSSDKPandas-Python311:13
    ap-southeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-3:258944054355:layer:AWSSDKPandas-Python311:14
    ap-southeast-4:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-4:945386623051:layer:AWSSDKPandas-Python311:13
    eu-central-2:
      lambdaLayer: arn:aws:lambda:eu-central-2:956415814219:layer:AWSSDKPandas-Python311:13
    eu-south-1:
      lambdaLayer: arn:aws:lambda:eu-south-1:774444163449:layer:AWSSDKPandas-Python311:14
    eu-south-2:
      lambdaLayer: arn:aws:lambda:eu-south-2:982086096842:layer:AWSSDKPandas-Python311:13
    il-central-1:
      lambdaLayer: arn:aws:lambda:il-central-1:263840725265:layer:AWSSDKPandas-Python311:12
    me-central-1:
      lambdaLayer: arn:aws:lambda:me-central-1:593833071574:layer:AWSSDKPandas-Python311:12
    me-south-1:
      lambdaLayer: arn:aws:lambda:me-south-1:938046470361:layer:AWSSDKPandas-Python311:14
    cn-north-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-north-1:406640652441:layer:AWSSDKPandas-Python311:10
    cn-northwest-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-northwest-1:406640652441:layer:AWSSDKPandas-Python311:10

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DependsOn:
    - SagemakerRole
    - LambdaRole
    - ExecutionRole
    - EcsInstanceExecutionRole
    - EcsInstanceProfile
    Properties:
      BucketName: !Ref BucketName

  SagemakerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: sagemaker.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
      Policies:
      - PolicyName: S3access
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:GetObject
            - s3:PutObject
            - s3:DeleteObject
            - s3:ListBucket
            Resource:
            - arn:aws:s3:::*
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: AllowStepFunctionExecution
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - states:StartExecution
            Resource: arn:aws:states:*
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonSageMakerServiceCatalogProductsLambdaServiceRolePolicy
      - arn:aws:iam::aws:policy/AmazonS3FullAccess
  StepFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: states.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: StepFunction
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogDelivery
            - logs:GetLogDelivery
            - logs:UpdateLogDelivery
            - logs:DeleteLogDelivery
            - logs:ListLogDeliveries
            - logs:PutResourcePolicy
            - logs:DescribeResourcePolicies
            - logs:DescribeLogGroups
            - lambda:InvokeFunction
            - sagemaker:DescribeTransformJob
            - xray:PutTraceSegments
            - xray:PutTelemetryRecords
            - xray:GetSamplingRules
            - xray:GetSamplingTargets
            - s3:*
            - states:StartExecution
            Resource: '*'

  TechModel:
    Type: AWS::SageMaker::Model
    Properties:
      Containers:
      - ModelPackageName: !Ref TechniqueModelARN
      ExecutionRoleArn: !GetAtt SagemakerRole.Arn
      ModelName: !Ref TechniqueModelName
      EnableNetworkIsolation: true
  ClusterModel1:
    Type: AWS::SageMaker::Model
    Properties:
      Containers:
      - ModelPackageName: !Ref ClusterModelARN1
      ExecutionRoleArn: !GetAtt SagemakerRole.Arn
      ModelName: !Ref ClusterModelName1
      EnableNetworkIsolation: true
  ClusterModel:
    Type: AWS::SageMaker::Model
    Properties:
      Containers:
      - ModelPackageName: !Ref ClusterModelARN
      ExecutionRoleArn: !GetAtt SagemakerRole.Arn
      ModelName: !Ref ClusterModelName
      EnableNetworkIsolation: true
  FlowModel:
    Type: AWS::SageMaker::Model
    Properties:
      Containers:
      - ModelPackageName: !Ref FlowModelARN
      ExecutionRoleArn: !GetAtt SagemakerRole.Arn
      ModelName: !Ref FlowModelName
      EnableNetworkIsolation: true

  skipInput:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_skip_input
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: skipInput
      InlineCode: |
        '''
        Skip input file if one execution is running else start execution in step function.

        Input: User input file
        '''

        import os
        import json
        import glob
        import base64
        import gzip

        import urllib
        import boto3
        from botocore.exceptions import ClientError

        BUCKET = os.getenv("bucket")

        STEP_FUNCTION_ARN = os.getenv("step_function_arn")

        missing_variables = []

        if BUCKET is None:
            missing_variables.append("bucket")
        if STEP_FUNCTION_ARN is None:
            missing_variables.append("step_function_arn")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")
        STEP_CLIENT = boto3.client('stepfunctions')

        SCRATCH_DIR = "scratch"

        QUEUE_LOOKUP_FILE = f"{SCRATCH_DIR}/queue_lookup.json"

        TEMP_QUEUE_LOOKUP_FILE = "/tmp/queue_lookup.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE
            func = "lambda_handler"

            try:

                clear_temp_dir()

                fetch_or_create_queue_lookup_file(BUCKET)

                if 'Records' in event and 's3' in event['Records'][0]:
                    print(f"{func}: Triggered by S3")
                    handle_s3_input_trigger(event)
                else:
                    print(f"{func}: Triggered by cloudwatch")
                    handle_cloud_watch_trigger(event)
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(BUCKET, error_message)
                raise ex


        def handle_s3_input_trigger(event):
            '''
            Handle s3 input trigger
            Args:
                event: event from s3 object created trigger
            '''
            global S3_CLIENT
            global STEP_CLIENT
            global BUCKET
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE
            global STEP_FUNCTION_ARN

            func = "handle_s3_input_trigger"

            input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            skip_input = skip_processing_input_file(BUCKET, input_filename)

            save_queue_lookup_file()

            if skip_input:
                print(f"{func}: Skipping current input")
            else:
                print(f"{func}: Process current input")

                execution_input = {
                    "bucket": BUCKET,
                    "key": input_filename
                }

                try:
                    response = STEP_CLIENT.start_execution(
                        stateMachineArn=STEP_FUNCTION_ARN,
                        input=json.dumps(execution_input)
                    )

                    print(f"Step Function execution started: {response['executionArn']}")

                except Exception as e:
                    print(f"{func}: Error starting Step Function execution")
                    raise e


        def handle_cloud_watch_trigger(event):
            '''
            Handle cloud watch input trigger
            Args:
                event: event from s3 object created trigger
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE

            func = "handle_cloud_watch_trigger"

            aws_log = event['awslogs']['data']
            compressed_payload = base64.b64decode(aws_log)
            uncompressed_payload = gzip.decompress(compressed_payload)
            decoded_payload = json.loads(uncompressed_payload.decode('utf-8'))

            print("Decoded CloudWatch data: ", decoded_payload)

            message = decoded_payload["logEvents"][0]["message"]

            try:
                message_json = json.loads(message)
            except json.JSONDecodeError as json_error:
                print(f"Error parsing JSON from message: {message}")
                raise json_error
            except KeyError as key_error:
                print("Missing key in JSON message")
                raise key_error

            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            if len(queue_lookup_json["input_queue"]) > 0:
                print(f"{func}: Remove current input from input queue.")
                queue_lookup_json["input_queue"].pop(0)
                queue_lookup_json["prev"] = message_json["payload"]["prev"]
                if "first" in message_json["payload"]:
                    queue_lookup_json["first"] = message_json["payload"]["first"]
                json.dump(queue_lookup_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))

            save_queue_lookup_file()


        def save_queue_lookup_file():
            '''
            Save queue lookup file to S3
            '''
            global BUCKET
            global S3_CLIENT
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE
            func = "save_queue_lookup_file"
            print("Save queue lookup file")
            try:
                S3_CLIENT.upload_file(TEMP_QUEUE_LOOKUP_FILE, BUCKET, QUEUE_LOOKUP_FILE)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {BUCKET}/{QUEUE_LOOKUP_FILE}")
                raise e


        def skip_processing_input_file(bucket, input_filename):
            '''
            Check if current input file should be processed or skipped
            Args:
                bucket: Bucket name
                input_filename: S3 object key of the input file on which lambda function is triggered
            Returns:
                true/false if current input processed should be skipped
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_QUEUE_LOOKUP_FILE
            func = "skip_processing_input_file"

            skip_input = False

            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            previous_input_last_batch_unique_id = queue_lookup_json["prev"]

            input_queue_entry = f"{bucket}/{input_filename}"
            print(f"{func}: Input queue entry: {input_queue_entry}")

            if previous_input_last_batch_unique_id is None:
                print(f"{func}: There are no previous batches to check")
                skip_input = False

                if len(queue_lookup_json["input_queue"]) > 0:
                    print(f"{func}: Current input queue is not empty. Skip input processing.")
                    skip_input = True
                    if input_queue_entry not in queue_lookup_json["input_queue"]:
                        print(f"{func}: Current input is not present in queue, push to queue.")
                        queue_lookup_json["input_queue"].append(input_queue_entry)
                else:
                    print(f"{func}: Current input is not present in queue, push to queue.")
                    queue_lookup_json["input_queue"].append(input_queue_entry)

                print(f"{func}: Save any updates to queue lookup file")
                json.dump(queue_lookup_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))
                return skip_input

            flow_output_for_previous_input_filename = f"{SCRATCH_DIR}/intermediate/{previous_input_last_batch_unique_id}/flow.json"
            flow_output_for_previous_input_present = False
            try:
                print(f"{func}: Check if the flow output for previous input is available: {bucket}/{flow_output_for_previous_input_filename}")
                S3_CLIENT.head_object(Bucket=bucket, Key=flow_output_for_previous_input_filename)
                flow_output_for_previous_input_present = True
            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Flow output is not present for previous input. Cannot start processing current input.")
                flow_output_for_previous_input_present = False

            if not flow_output_for_previous_input_present:
                if input_queue_entry not in queue_lookup_json["input_queue"]:
                    print(f"{func}: Current input is not present in queue, push to queue.")
                    queue_lookup_json["input_queue"].append(input_queue_entry)

                skip_input = True

            else:
                print(f"{func}: Flow output for previous input is present. Start processing next input.")

                if len(queue_lookup_json["input_queue"]) > 0:
                    print(f"{func}: Next input to process is: {queue_lookup_json['input_queue'][0]}")
                    if input_queue_entry == queue_lookup_json["input_queue"][0]:
                        print(f"{func}: Current input is to be processed next.")
                        skip_input = False
                    else:
                        print(f"{func}: Current input is not to be processed next.")
                        skip_input = True
                        if input_queue_entry not in queue_lookup_json["input_queue"]:
                            print(f"{func}: Current input is not present in queue, push to queue.")
                            queue_lookup_json["input_queue"].append(input_queue_entry)
                else:
                    print(f"{func}: There are no items in queue input. Current input is to be processed next.")
                    skip_input = False
                    print(f"{func}: Current input is not present in queue, push to queue.")
                    queue_lookup_json["input_queue"].append(input_queue_entry)

            print(f"{func}: Save any updated to queue lookup file")
            json.dump(queue_lookup_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))

            return skip_input


        def fetch_or_create_queue_lookup_file(bucket):
            '''
            Check if the queue lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global QUEUE_LOOKUP_FILE
            global S3_CLIENT
            global TEMP_QUEUE_LOOKUP_FILE

            func = "fetch_or_create_queue_lookup_file"

            try:
                print(
                    f"{func}: Checking if queue lookup file exists on S3 path {bucket}/{QUEUE_LOOKUP_FILE}")

                S3_CLIENT.head_object(Bucket=bucket, Key=QUEUE_LOOKUP_FILE)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Queue lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = {"first": None, "prev": None, "input_queue": []}
                json.dump(internal_id_to_unique_id_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))

                print(f"{func}: Initiated queue lookup")
            else:
                try:
                    print(f"{func}: Download available queue lookup file from S3")

                    S3_CLIENT.download_file(bucket, QUEUE_LOOKUP_FILE, TEMP_QUEUE_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{QUEUE_LOOKUP_FILE}")
                    raise e


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
          step_function_arn: !GetAtt stepFunction.Arn
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: input/
                - Name: suffix
                  Value: .json
        MyLogGroupTrigger:
          Type: CloudWatchLogs
          Properties:
            LogGroupName: !Ref enrichWithTechniqueLogGroup
            FilterPattern: update queue lookup
  skipInputLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${skipInput}
  skipInputLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref skipInput
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  splunkInput:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_splunk_input
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: splunkInput
      InlineCode: |
        '''
        Chunk input from splunk to create input for VRL transform.
        Merge back chunks for single input file back to one and send as input to pipeline.

        Input: Input from splunk Add-on Amazon S3 Uploader for Splunk (json format)
        '''

        import os
        import sys
        import subprocess
        import json
        import glob
        import urllib
        import boto3
        from itertools import islice
        import gc

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        subprocess.call('pip install ijson==3.3.0 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import ijson

        CHUNK_SIZE = int(os.getenv("chunk_size"))

        missing_variables = []
        if CHUNK_SIZE is None:
            missing_variables.append("chunk_size")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        BUF_SIZE = 1*1024*1024

        TEMP_INPUT_DIR = "/tmp/input/"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)


        def lambda_handler(event, context):
            '''
            Read input file and split in chunks
            '''
            global S3_CLIENT
            global CHUNK_SIZE

            func = "lambda_handler"

            input_file_object = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            clear_temp_dir()

            print(f"{func}: Input file object: {input_file_object}")

            prefix = "/".join(input_file_object.split("/")[:2])
            if prefix == "splunk_input/input":
                process_splunk_input(event, context)
            else:
                merge_transformed_input(event, context)


        def merge_transformed_input(event, context):
            '''
            Read chunk transformed input file and merge it to be input to pipeline
            '''
            global S3_CLIENT
            func = "merge_transformed_input"
            bucket = event["Records"][0]["s3"]["bucket"]["name"]
            input_file_object = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            input_file = "/tmp/chunk_input.json"

            clear_temp_dir()

            try:
                print(f"{func}: Download input file: {bucket}/{input_file_object}")
                S3_CLIENT.download_file(bucket, input_file_object, input_file)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{input_file_object}")
                raise e

            print(f"{func}: Check if all splits are processed")
            input_file_name = input_file_object.split("/")[-2]
            input_file_splits_length_json_filename = f"splunk_input/{SCRATCH_DIR}/count/{input_file_name}/split_length.json"
            temp_input_file_splits_length_json_filename = f"/tmp/split_length.json"
            try:
                print(f"{func}: Download length file: {bucket}/{input_file_splits_length_json_filename}")
                S3_CLIENT.download_file(bucket, input_file_splits_length_json_filename, temp_input_file_splits_length_json_filename)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{input_file_splits_length_json_filename}")
                raise e

            length_json = json.load(open(temp_input_file_splits_length_json_filename, "r"))

            prefix = f"splunk_input/{SCRATCH_DIR}/transformed/{input_file_name}/"
            print(f"{func}: Prefix to search: {prefix}")

            paginator = S3_CLIENT.get_paginator('list_objects_v2')
            page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)

            completed_chunk_keys = []

            print(f"{func}: Iterate through each page")
            for page in page_iterator:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        completed_chunk_keys.append(obj['Key'])

            print(f"{func}: Total files in prefix: {len(completed_chunk_keys)}")

            if len(completed_chunk_keys) != length_json["count"]:
                print(f"{func}: All chunk did not complete processing. Skip merge")
                return

            print(f"{func}: All chunks completed processing. Merge chunks and upload to S3")

            input_to_pipeline_filename = "/tmp/input_pipe.json"
            with open(input_to_pipeline_filename, "w") as f_write:
                f_write.write("""{"input": [""")
                first_line_present = False

                for chunk_key in completed_chunk_keys:
                    completed_chunk_filename = f"{TEMP_INPUT_DIR}chunk.json"
                    try:
                        print(f"{func}: Download input file: {bucket}/{chunk_key}")
                        S3_CLIENT.download_file(bucket, chunk_key, completed_chunk_filename)
                    except Exception as e:
                        print(f"{func}: Failed to download file from S3: {bucket}/{chunk_key}")
                        raise e

                    print("read and add chunk contents")
                    with open(completed_chunk_filename, "r") as f_read:
                        for line in f_read:
                            if first_line_present:
                                f_write.write(",\n" + line.strip("\n"))
                            else:
                                f_write.write("\n" + line.strip("\n"))
                                first_line_present = True

                f_write.write("]}")

            print("upload output file")
            output_file_object = f"input/{input_file_name}.json"

            try:
                S3_CLIENT.upload_file(input_to_pipeline_filename, bucket, output_file_object)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{output_file_object}")
                raise e


        def process_splunk_input(event, context):
            '''
            Read input file and split in chunks
            '''
            global S3_CLIENT
            global CHUNK_SIZE

            func = "process_splunk_input"

            bucket = event["Records"][0]["s3"]["bucket"]["name"]
            input_file_object = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            input_file = "/tmp/input.json"

            clear_temp_dir()

            try:
                print(f"{func}: Download input file: {bucket}/{input_file_object}")
                S3_CLIENT.download_file(bucket, input_file_object, input_file)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{input_file_object}")
                raise e
            
            print(f"{func}: Download completed")

            chunks_of_large_input_file = create_chunk_for_large_input_file(input_file)

            input_file_name = input_file_object.split("/")[-1].split(".")[0]

            input_file_splits_length_json = {"count": len(chunks_of_large_input_file)}

            input_file_splits_length_json_filename = f"splunk_input/{SCRATCH_DIR}/count/{input_file_name}/split_length.json"
            temp_input_file_splits_length_json_filename = f"/tmp/split_length.json"
            json.dump(input_file_splits_length_json, open(temp_input_file_splits_length_json_filename, "w"))

            try:
                S3_CLIENT.upload_file(temp_input_file_splits_length_json_filename, bucket, input_file_splits_length_json_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{input_file_splits_length_json_filename}")
                raise e

            print(f"{func}: Upload chunks. Number of chunks: {len(chunks_of_large_input_file)}")
            for i, chunk_of_large_input_file in enumerate(chunks_of_large_input_file, start=1):
                output_file_object = f"splunk_input/{SCRATCH_DIR}/chunk/{input_file_name}/split_{i}.json"
                try:
                    S3_CLIENT.upload_file(chunk_of_large_input_file, bucket, output_file_object)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{output_file_object}")
                    raise e

            print(f"{func}: Upload completed")


        def create_chunk_for_large_input_file(temp_input_file):
            '''
            Create chunks for a large input file
            Returns:
                List of split file names
            '''
            global CHUNK_SIZE
            global BUF_SIZE
            global TEMP_INPUT_DIR

            split_files = []
            with open(temp_input_file, 'rb') as f:
                split_files = []
                item = 1
                input_list_iter = ijson.items(f, 'item', use_float=True, buf_size=BUF_SIZE)
                while True:
                    input_list = list(islice(input_list_iter, CHUNK_SIZE))
                    split_filename = TEMP_INPUT_DIR + "input" + '_' + str(item) + '.json'
                    if not input_list:
                        break

                    with open(split_filename, 'w') as outfile:
                        for alert in input_list:
                            outfile.write(json.dumps(alert) + "\n")
                        split_files.append(split_filename)

                    item += 1
                    del input_list
                    gc.collect()

            del input_list_iter
            gc.collect()

            return split_files


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          chunk_size: '5000'
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: splunk_input/input/
                - Name: suffix
                  Value: .json
        S3ObjectCreatedTransformed:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: splunk_input/scratch/transformed/
                - Name: suffix
                  Value: .json
  splunkInputLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${splunkInput}
  splunkInputLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref splunkInput
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  vrlLambda:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_vrl_lambda
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: vrlLambda
      ImageUri: !Ref VRLLambdaImage
      PackageType: Image
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Environment:
        Variables:
          vrl_program_bucket: !Ref BucketName
          vrl_program_s3_key: splunk_input/program/program.vrl
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: splunk_input/scratch/chunk/
                - Name: suffix
                  Value: .json
  vrlLambdaLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${vrlLambda}
  vrlLambdaLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref vrlLambda
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  enrichWithTechnique:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_enrich_with_technique
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: enrichWithTechnique
      InlineCode: |
        '''
        Chunk input file, sanititze in required format, encode attributes, encode node_features if present, generate internal id.
        Create batch transform job for technique classification.

        Input: User input file
        '''

        import os
        import sys
        import subprocess
        import gc
        import re
        from datetime import datetime, timezone
        from dateutil import parser

        import urllib
        import uuid
        import json
        import glob
        from itertools import islice
        import sqlite3
        from contextlib import closing

        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd
        import numpy as np

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        os.makedirs("/tmp/nltk_download", exist_ok=True)
        subprocess.call('pip install ijson==3.3.0 nltk==3.8.1 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import ijson
        import nltk
        from nltk.corpus import words

        TECHNIQUE_MODEL_NAME = os.getenv("technique_model_name")

        BATCH_TRANSFORM_JOB_INSTANCE_TYPE = os.getenv("batch_transform_job_instance_type")

        BATCH_TRANSFORM_JOB_MAX_RETRIES = os.getenv("batch_transform_job_max_retries")

        BATCH_TRANSFORM_JOB_TIMEOUT = os.getenv("batch_transform_job_timeout")

        BATCH_TRANSFORM_JOB_MAX_PAYLOAD = os.getenv("batch_transform_job_max_payload")

        BATCH_TRANSFORM_JOB_CONCURRENT_TRANSFORM = os.getenv("batch_transform_job_concurrent_transform")

        BATCH_TRANSFORM_JOB_INSTANCE_COUNT = os.getenv("batch_transform_job_instance_count")

        TECHNIQUE_LOOKUP_OBJECT = os.getenv("technique_lookup_object")

        CHUNK_SIZE = os.getenv("chunk_size")

        ENCODE_OTHER_ATTRS = os.getenv("encode_other_attrs")

        MAP_CEF_TO_INTERNAL = os.getenv("map_cef_to_internal")

        ENCODE_NODE_FEATURE = os.getenv("encode_node_feature")

        missing_variables = []

        if TECHNIQUE_MODEL_NAME is None:
            missing_variables.append("technique_model_name")
        if BATCH_TRANSFORM_JOB_INSTANCE_TYPE is None:
            missing_variables.append("batch_transform_job_instance_type")
        if BATCH_TRANSFORM_JOB_MAX_RETRIES is None:
            missing_variables.append("batch_transform_job_max_retries")
        if BATCH_TRANSFORM_JOB_TIMEOUT is None:
            missing_variables.append("batch_transform_job_timeout")
        if BATCH_TRANSFORM_JOB_MAX_PAYLOAD is None:
            missing_variables.append("batch_transform_job_max_payload")
        if BATCH_TRANSFORM_JOB_CONCURRENT_TRANSFORM is None:
            missing_variables.append("batch_transform_job_concurrent_transform")
        if BATCH_TRANSFORM_JOB_INSTANCE_COUNT is None:
            missing_variables.append("batch_transform_job_instance_count")
        if TECHNIQUE_LOOKUP_OBJECT is None:
            missing_variables.append("technique_lookup_object")
        if CHUNK_SIZE is None:
            missing_variables.append("chunk_size")
        if ENCODE_OTHER_ATTRS is None:
            missing_variables.append("encode_other_attrs")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        BATCH_TRANSFORM_JOB_MAX_RETRIES = int(BATCH_TRANSFORM_JOB_MAX_RETRIES)

        BATCH_TRANSFORM_JOB_TIMEOUT = int(BATCH_TRANSFORM_JOB_TIMEOUT)

        BATCH_TRANSFORM_JOB_MAX_PAYLOAD = int(BATCH_TRANSFORM_JOB_MAX_PAYLOAD)

        BATCH_TRANSFORM_JOB_CONCURRENT_TRANSFORM = int(BATCH_TRANSFORM_JOB_CONCURRENT_TRANSFORM)

        BATCH_TRANSFORM_JOB_INSTANCE_COUNT = int(BATCH_TRANSFORM_JOB_INSTANCE_COUNT)

        CHUNK_SIZE = int(CHUNK_SIZE)

        ENCODE_OTHER_ATTRS = ENCODE_OTHER_ATTRS.lower() == "true"

        if MAP_CEF_TO_INTERNAL is not None:
            MAP_CEF_TO_INTERNAL = (MAP_CEF_TO_INTERNAL == "true")
        else:
            MAP_CEF_TO_INTERNAL = True

        if ENCODE_NODE_FEATURE is not None:
            ENCODE_NODE_FEATURE = (ENCODE_NODE_FEATURE == "true")
        else:
            ENCODE_NODE_FEATURE = True

        S3_CLIENT = boto3.client("s3")
        SAGEMAKER = boto3.client(service_name='sagemaker')

        BUF_SIZE = 1*1024*1024

        TEMP_INPUT_DIR = "/tmp/input/"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)

        SCRATCH_DIR = "scratch"

        PATH_TO_SQLITE = f"{SCRATCH_DIR}/cypienta.db"
        TEMP_SQLITE_FILENAME = "/tmp/cypienta.db"

        INTERNAL_ID_TO_USER_ID_OBJECT_KEY = f"{SCRATCH_DIR}/internal_id_to_user_id.json"
        INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY = f"{SCRATCH_DIR}/internal_id_to_unique_id.json"

        QUEUE_LOOKUP_FILE = f"{SCRATCH_DIR}/queue_lookup.json"
        NODE_FEATURE_LOOKUP_FILE = f"{SCRATCH_DIR}/node_feature_lookup.json"

        TEMP_INPUT_FILENAME = "/tmp/input_all.json"
        TEMP_ALERTS_INPUT_FILENAME = f"{TEMP_INPUT_DIR}input.json"
        TEMP_CLUSTER_CONFIG_INPUT_FILENAME = f"{TEMP_INPUT_DIR}cluster_config.json"
        TEMP_CONFIG_INPUT_FILENAME = f"{TEMP_INPUT_DIR}config.json"
        TEMP_NODE_FEATURE_INPUT_FILENAME = f"{TEMP_INPUT_DIR}node_feature.json"
        TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME = f"{TEMP_INPUT_DIR}node_feature_original.json"
        TEMP_USER_FEEDBACK_WEIGHTS = f"{TEMP_INPUT_DIR}user_feedback_weights.json"

        TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE = "/tmp/data.csv"

        TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE = "/tmp/internal_id_to_user_id.json"
        TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE = "/tmp/internal_id_to_unique_id.json"
        TEMP_QUEUE_LOOKUP_FILE = "/tmp/queue_lookup.json"

        TEMP_NODE_FEATURE_LOOKUP_FILE = "/tmp/node_feature_lookup.json"

        TEMP_SKIPPED_EVENTS_FILENAME = "/tmp/skip_events.json"

        INITIAL_WEIGHTS_VALUE = 100

        CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
        CONFIG_EXISTS_IN_REQUEST = False
        NODE_FEATURE_EXISTS_IN_REQUEST = False

        SKIP_ENCODING_ATTRIBUTE = ["priority", "port", "url", "user_agent", "cert", "user_feedback"]

        ENCODE_TABLE = {}

        nltk.data.path.append("/tmp/nltk_download")
        nltk.download("punkt", download_dir="/tmp/nltk_download")
        nltk.download('words', download_dir="/tmp/nltk_download")

        ENGLISH_WORDS = set(words.words())

        BLACK_LIST_KEYS = [
            '_raw',
            '_si',
            'splunk_server',
            'ComputerName',
            'source',
            'tag',
            'pa',
            'ssl_issuer',
            'ssl_subject',
            'query',
            'FriendlyName',
            'service_name',
            'content{}',
            'Mandatory_Label',
            'vendor_definition'
        ]


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global TEMP_INPUT_FILENAME
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE
            global TEMP_CONFIG_INPUT_FILENAME
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global CONFIG_EXISTS_IN_REQUEST
            global MAP_CEF_TO_INTERNAL
            func = "lambda_handler"

            CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
            CONFIG_EXISTS_IN_REQUEST = False
            NODE_FEATURE_EXISTS_IN_REQUEST = False

            bucket = event["bucket"]
            try:
                input_filename = event["key"]

                # delete only the error txt files from output folder if present
                delete_error_log_s3(bucket)

                clear_temp_dir()

                initialize_sqlite(bucket)

                fetch_or_create_queue_lookup_file(bucket)

                try:
                    S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                    print(f"{func}: Input file download completed")
                except Exception as e:
                    print(f"{func}: Failed to download input file")
                    raise e

                fetch_or_create_technique_classification_lookup_file(bucket)
                fetch_or_create_internal_id_to_user_id_lookup_file(bucket)
                fetch_or_create_internal_id_to_unique_id_lookup_file(bucket)

                path_to_global_attribute_weights = f"{SCRATCH_DIR}/attribute_weights.json"
                fetch_or_create_global_attribute_weights(bucket, path_to_global_attribute_weights)

                get_static_cluster_config()

                process_input_file_to_extract_config_and_node_feature()

                try:
                    print(f"{func}: Create chunk for large input file.")
                    chunks_of_large_input_file = create_chunk_for_large_input_file()
                    if len(chunks_of_large_input_file) < 1:
                        raise ValueError(f"Number of chunks created for input file: {len(chunks_of_large_input_file)}. There should be atleast 1 chunk created. Check your input file.")
                except Exception as e:
                    print("Failure during creating chunks for input file.")
                    raise e

                print(f"{func}: Number of chunks created for large input file: {len(chunks_of_large_input_file)}")

                process_chunks = chunks_of_large_input_file
                if MAP_CEF_TO_INTERNAL:
                    cef_to_internal_mappings = get_cef_to_internal_mappings()

                    print(f"{func}: Map events to internal structure")
                    process_chunks = insert_event_sql(bucket, chunks_of_large_input_file, cef_to_internal_mappings)

                print(f"{func}: Chunks to process: {process_chunks}")

                queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

                if not process_chunks:
                    # need to send update queue log to remove current input from input queue.
                    print(f"{func}: Input file does not have any tranformed events that could be processed. Skip input file.")
                    payload_json = {"prev": queue_lookup_json["prev"]}
                    log_json = {"message": "update queue lookup", "payload": payload_json}
                    print(f"{json.dumps(log_json)}")

                    if len(queue_lookup_json["input_queue"]) > 1:
                        print(f"{func}: Next input file to process: {queue_lookup_json['input_queue'][1]}")
                        try:
                            print(f"{func}: Upload next input file in S3")
                            next_input_queue_entry = queue_lookup_json["input_queue"][1]
                            next_input_bucket = next_input_queue_entry.split("/")[0]
                            next_input_key = "/".join(next_input_queue_entry.split("/")[1:])
                            copy_source = {
                                'Bucket': next_input_bucket,
                                'Key': next_input_key
                            }
                            S3_CLIENT.copy_object(CopySource=copy_source, Bucket=next_input_bucket, Key=next_input_key,
                                                  Metadata={'UpdatedAt': str(datetime.now())}, MetadataDirective='REPLACE')
                        except Exception as e:
                            print(f"{func}: Failed to save file to S3: {next_input_bucket}/{next_input_key}")
                            raise e

                    lambda_response = {"TransformJobName": "skipped_input"}
                    return lambda_response

                unique_ids = [str(uuid.uuid4()) for _ in process_chunks]

                print(f"Unique id(s) generated for the chunk(s): {unique_ids}")

                if NODE_FEATURE_EXISTS_IN_REQUEST and ENCODE_NODE_FEATURE:
                    process_node_features()
                    print(f"{func}: Upload node feature lookup file to S3")
                    try:
                        S3_CLIENT.upload_file(TEMP_NODE_FEATURE_LOOKUP_FILE, bucket, NODE_FEATURE_LOOKUP_FILE)
                    except Exception as e:
                        print(f"{func}: Failed to save file to S3: {bucket}/{NODE_FEATURE_LOOKUP_FILE}")
                        raise e

                start_batch_transform_job = False
                lambda_response = {"TransformJobName": 'transform-job-cluster', "bucket": bucket, "input_first_unique_id": unique_ids[0]}

                for i, chunk_of_large_input_file in enumerate(process_chunks):

                    unique_id = unique_ids[i]

                    next_unique_id = None
                    if (i+1) < len(unique_ids):
                        next_unique_id = unique_ids[i+1]

                    if i > 0:
                        previous_unique_id = unique_ids[i-1]
                    else:
                        previous_unique_id = queue_lookup_json["prev"]
                        # previous_unique_id = None

                    # first - unique id of first batch of current input
                    # prevous - unique id of previous batch that needs to be run (inter file handled)
                    # next - unique id of next batch that needs to be run (last batch of current input will always be null)
                    queue_json = {
                        "first": unique_ids[0],
                        "previous": previous_unique_id,
                        "next": next_unique_id
                    }

                    print(f"{func}: Processing chunk file: {chunk_of_large_input_file}; unique ID for processing chunk file: {unique_id}")

                    print(f"{func}: Starting batch job for enriching alerts with techniques")
                    path_to_input_for_batch_technique_classification = f"{SCRATCH_DIR}/response/classification_in/{unique_ids[0]}/input_classification_{unique_id}.json"
                    response = enrich_alerts_with_techniques(bucket, chunk_of_large_input_file, unique_id, unique_ids[0], queue_json, path_to_input_for_batch_technique_classification)

                    if response:
                        start_batch_transform_job = True

                if start_batch_transform_job:
                    try:
                        print(f"{func}: Create batch transforom job for Technique classification. Unique id: {unique_ids[0]}")
                        batch_transform_job = setup_batch_transform_job_for_enrich_alerts_with_techniques(bucket, unique_ids[0])
                        SAGEMAKER.create_transform_job(**batch_transform_job)
                        print(f"{func}: Created batch transform job")
                        lambda_response = {"TransformJobName": f'transform-job-tech-{unique_ids[0]}'}
                    except Exception as e:
                        print(f"{func}: Failed to create batch transform job. Unique id: {unique_ids[0]}")
                        raise e

                update_queue_lookup(bucket, unique_ids)
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def setup_batch_transform_job_for_enrich_alerts_with_techniques(bucket, unique_id):
            '''
            Configure batch transform job for Technique detection model
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
                input_filename: S3 input filename to batch tranform job
            Returns:
                batch_transform_job: Configuration to create batch transform job
            '''
            transform_job_name = f'transform-job-tech-{unique_id}'

            transform_input = {
                "DataSource": {
                    "S3DataSource": {
                        "S3DataType": "S3Prefix",
                        "S3Uri": f"s3://{bucket}/{SCRATCH_DIR}/response/classification_in/{unique_id}/"
                    }
                },
                "ContentType": "text/plain",
                "SplitType": "Line"
            }
            transform_output = {
                "S3OutputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/classification_out/{unique_id}/",
                "KmsKeyId": "",
                "AssembleWith": "Line"
            }
            transform_resources = {
                "InstanceType": BATCH_TRANSFORM_JOB_INSTANCE_TYPE,
                "InstanceCount": BATCH_TRANSFORM_JOB_INSTANCE_COUNT
            }
            model_client_config = {
                'InvocationsTimeoutInSeconds': BATCH_TRANSFORM_JOB_TIMEOUT,
                'InvocationsMaxRetries': BATCH_TRANSFORM_JOB_MAX_RETRIES
            }

            batch_transform_job = {
                'BatchStrategy': "MutliRecord",
                'TransformJobName': transform_job_name,
                'ModelName': TECHNIQUE_MODEL_NAME,
                'TransformInput': transform_input,
                'TransformOutput': transform_output,
                'MaxPayloadInMB': BATCH_TRANSFORM_JOB_MAX_PAYLOAD,
                'MaxConcurrentTransforms': BATCH_TRANSFORM_JOB_CONCURRENT_TRANSFORM,
                "TransformResources": transform_resources,
                "ModelClientConfig": model_client_config
            }
            return batch_transform_job


        def enrich_alerts_with_techniques(bucket, chunk_of_large_input_file, unique_id, first_unique_id, queue_json, path_to_input_for_batch_technique_classification):
            '''
            Enriches alerts with techniques. Filter out previously classified text. Sanitize input. create batch transform job for technique classification
            Args:
                bucket: Bucket name
                chunk_of_large_input_file: Chunk of input file
                unique_id: The unique identifier for the data chunk,
                first_unique_id: first unique id for chunk of input
                queue_json: JSON object containing queue information
                path_to_input_for_batch_technique_classification: path to store input for classification
            Returns: False if no new alerts to classify, else True if there are alerts to classfiy.
            '''

            global SCRATCH_DIR
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE
            global ENCODE_TABLE
            global ENCODE_OTHER_ATTRS
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global CONFIG_EXISTS_IN_REQUEST
            global S3_CLIENT
            global TEMP_CONFIG_INPUT_FILENAME
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE
            global INTERNAL_ID_TO_USER_ID_OBJECT_KEY
            global TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE
            global INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY
            global SAGEMAKER
            global SKIP_ENCODING_ATTRIBUTE
            global TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME

            func = "enrich_alerts_with_techniques"

            path_to_intermediate_input_before_technique_classification = f"{SCRATCH_DIR}/intermediate/{unique_id}/input.json"
            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            path_to_cluster_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_config.json"
            path_to_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/config.json"
            path_to_node_feature_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/node_feature.json"
            path_to_node_feature_original_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/node_feature_original.json"

            technique_classification_lookup_df = pd.read_csv(TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE)

            try:
                print(f"{func}: Save original chunk")
                path_to_original_chunk_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/original_input.json"
                S3_CLIENT.upload_file(chunk_of_large_input_file, bucket, path_to_original_chunk_input)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_config_from_input}")
                raise e

            # From the input, filter alerts that do not have recognized techniques in the technique lookup table

            # drop dulicates from chunk input
            chunk_input_df = pd.read_json(chunk_of_large_input_file)
            chunk_input_df = chunk_input_df.rename(columns={"name": "alerts"})
            chunk_input_df = chunk_input_df.drop_duplicates("alerts")

            # if tech is present in the input chunk then rename it to techniques, else initialize to None
            if "tech" in chunk_input_df.columns:
                chunk_input_df = chunk_input_df.rename(columns={"tech": "techniques"})
            else:
                chunk_input_df["techniques"] = None

            # If there are any empty techniques in the input chunk, make it None.
            chunk_input_df['techniques'] = chunk_input_df['techniques'].apply(lambda x: None if x == [] else x)
            chunk_input_df = chunk_input_df[["alerts", "techniques"]]

            # Get only those rows from chunk input which did not have tech field, or it was empty.
            filtered_chunk_input_df = chunk_input_df[chunk_input_df["techniques"].isna()]

            # merge the technique lookup table with filtered chunk input.
            # merge on right means it will get None for the alerts not present in the lookup table
            merged_technique_classification_df = pd.merge(technique_classification_lookup_df, filtered_chunk_input_df, on="alerts", how="right")

            # Get all the alerts that are not present in the lookup table by filtering techniques from lookup table as None
            unique_alerts_to_classify = merged_technique_classification_df[merged_technique_classification_df["techniques_x"].isna()]
            unique_alerts_to_classify = unique_alerts_to_classify.drop_duplicates("alerts")
            unique_alerts_to_classify = unique_alerts_to_classify[unique_alerts_to_classify["alerts"].notna() & (unique_alerts_to_classify["alerts"] != "")]
            unique_alerts_to_classify = unique_alerts_to_classify["alerts"].to_list()

            # create dictionary for alerts: recognized techniques
            merged_technique_classification_df["techniques_x"] = merged_technique_classification_df["techniques_x"].fillna("[]")
            merged_technique_classification_df["techniques_y"] = merged_technique_classification_df["techniques_x"]
            merged_technique_classification_df = merged_technique_classification_df[["alerts", "techniques_y"]]

            merged_technique_classification_df = merged_technique_classification_df[merged_technique_classification_df["alerts"].notna() &
                                                                                    (merged_technique_classification_df["alerts"] != "")]
            merged_technique_classification_dict = merged_technique_classification_df.set_index("alerts").to_dict()["techniques_y"]

            print(f"{func}: Start enriching alerts with techniques")

            chunk_input_json = json.load(open(chunk_of_large_input_file, "r"))
            batch_request_for_technique_classification_list = []

            # sanitize input. encode other_attributes_dict and update recognized techniques
            for chunk_input_alert in chunk_input_json:
                if "other_attributes_dict" not in chunk_input_alert or len(chunk_input_alert["other_attributes_dict"].keys()) == 0:
                    chunk_input_alert["other_attributes_dict"] = {"empty": True}
                else:
                    if ENCODE_OTHER_ATTRS:
                        # it will also encode special other_Attributes liek priority
                        for key in chunk_input_alert['other_attributes_dict'].keys():
                            if key in SKIP_ENCODING_ATTRIBUTE:
                                continue
                            encoded, ENCODE_TABLE = custom_encoder(chunk_input_alert['other_attributes_dict'][key], ENCODE_TABLE)
                            chunk_input_alert["other_attributes_dict"][key] = encoded
                    chunk_input_alert["other_attributes_dict"]["empty"] = False

                if "label" not in chunk_input_alert:
                    chunk_input_alert["label"] = -1

                alert_text = chunk_input_alert["name"]

                # if alert text is empty, continue. If tech field does not exist, initialize empty list
                if alert_text.strip() == "":
                    if "tech" not in chunk_input_alert:
                        chunk_input_alert["tech"] = []
                    continue

                # if the tech is not in input, put tech recognized in lookup table if present
                if "tech" not in chunk_input_alert:
                    chunk_input_alert["tech"] = []
                    if alert_text in merged_technique_classification_dict:
                        tech = json.loads(merged_technique_classification_dict[alert_text])
                        chunk_input_alert["tech"] = tech

            print(f"{func}: Save intermediate inputs to S3.")
            if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                try:
                    S3_CLIENT.upload_file(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, bucket, path_to_cluster_config_from_input)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_cluster_config_from_input}")
                    raise e

            if CONFIG_EXISTS_IN_REQUEST:
                try:
                    S3_CLIENT.upload_file(TEMP_CONFIG_INPUT_FILENAME, bucket, path_to_config_from_input)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_config_from_input}")
                    raise e

            if NODE_FEATURE_EXISTS_IN_REQUEST:
                try:
                    S3_CLIENT.upload_file(TEMP_NODE_FEATURE_INPUT_FILENAME, bucket, path_to_node_feature_from_input)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_node_feature_from_input}")
                    raise e

                try:
                    S3_CLIENT.upload_file(TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME, bucket, path_to_node_feature_original_from_input)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_node_feature_original_from_input}")
                    raise e

            chunk_intermediate_input_local_filename = f"{chunk_of_large_input_file}_intermediate.json"

            json.dump(chunk_input_json, open(chunk_intermediate_input_local_filename, "w"))
            preprocess_chunk_input(unique_id, chunk_intermediate_input_local_filename)
            chunk_input_json = json.load(open(chunk_intermediate_input_local_filename, "r"))
            try:
                S3_CLIENT.upload_file(chunk_intermediate_input_local_filename, bucket, path_to_intermediate_input_before_technique_classification)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_intermediate_input_before_technique_classification}")
                raise e

            # Save internal id to user id mapping, internal id to unique id mapping
            try:
                S3_CLIENT.upload_file(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, bucket, INTERNAL_ID_TO_USER_ID_OBJECT_KEY)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{INTERNAL_ID_TO_USER_ID_OBJECT_KEY}")
                raise e

            try:
                S3_CLIENT.upload_file(TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE, bucket, INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY}")
                raise e

            # Save queue for interbatch
            chunk_intermediate_queue_filename = f"{chunk_of_large_input_file}_queue.json"
            json.dump(queue_json, open(chunk_intermediate_queue_filename, "w"))

            try:
                S3_CLIENT.upload_file(chunk_intermediate_queue_filename, bucket, path_to_intermediate_queue)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_intermediate_queue}")
                raise e

            if not unique_alerts_to_classify:
                print(f"{func}: No new alerts to classify. Skip batch transform job for {unique_id}")
                skip_creating_batch_transform_job(bucket, unique_id, first_unique_id, chunk_input_json)
                return False
            else:
                # create request for technique classification model
                for request_id, alert_text in enumerate(unique_alerts_to_classify, start=1):
                    individual_request_body_for_technique_classification_batch = {"request_id": str(request_id), "input": alert_text}

                    batch_request_for_technique_classification_list.append(json.dumps(individual_request_body_for_technique_classification_batch))

                batch_request_for_technique_classification_jsonl = "\n".join(batch_request_for_technique_classification_list)

                temp_batch_request_for_technique_classification_jsonl_filename = f"{chunk_of_large_input_file}_classification.txt"

                print(f"{func}: Save batch request for technique classification")
                with open(temp_batch_request_for_technique_classification_jsonl_filename, "w") as file:
                    file.writelines(batch_request_for_technique_classification_jsonl)
                try:
                    S3_CLIENT.upload_file(temp_batch_request_for_technique_classification_jsonl_filename,
                                          bucket, path_to_input_for_batch_technique_classification)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_input_for_batch_technique_classification}")
                    raise e

                return True


        def insert_event_sql(bucket, chunks_of_large_input_file, cef_to_internal_mappings):
            '''
            Map chunk of input from CEF to internal and insert to event table.
            save skipped events to s3.
            Args:
                bucket: Bucket name
                chunks_of_large_input_file: Chunks of input file
                cef_to_internal_mappings: cef to internal mappings list
            Returns:
                list of chunks to process
            '''
            global S3_CLIENT
            global TEMP_SKIPPED_EVENTS_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME
            func = "insert_event_sql"

            process_chunks = []
            node_feature = {}
            for chunk_of_large_input_file in chunks_of_large_input_file:

                tranformed_events = []
                skipped_events = []

                print(f"{func}: Read cef mapped chunk events")
                events = json.load(open(chunk_of_large_input_file, "r"))

                print(f"{func}: Transform cef mapped chunk events")
                for event in events:
                    tranformed_event, cef_mapped_keys_to_event, event_node_feature = map_cef_to_internal(event, cef_to_internal_mappings)

                    # gather skipped events
                    if tranformed_event is None:
                        skipped_events.append(event)
                        continue

                    # skip insert to table for now

                    tranformed_events.append(tranformed_event)

                    # update node feature
                    for key, value in event_node_feature.items():
                        if key in node_feature:
                            # if its existing node, update node features
                            node_feature[key].update(value)
                        else:
                            # if its new node. get node features
                            node_feature[key] = value

                if len(skipped_events) > 0:
                    print(f"{func}: Save skipped events")
                    json.dump(skipped_events, open(TEMP_SKIPPED_EVENTS_FILENAME, "w"))

                    print(f"{func}: Upload skipped events to S3")
                    current_timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %z")
                    skipped_chunk_object_key = f"skipped_events/{current_timestamp}/input.json"
                    try:
                        S3_CLIENT.upload_file(TEMP_SKIPPED_EVENTS_FILENAME, bucket, skipped_chunk_object_key)
                    except Exception as e:
                        print(f"{func}: Failed to save file to S3: {bucket}/{skipped_chunk_object_key}")
                        print(e)

                if len(tranformed_events) > 0:
                    print(f"{func}: save transformed cef mapped chunk events")
                    json.dump(tranformed_events, open(chunk_of_large_input_file, "w"))
                    process_chunks.append(chunk_of_large_input_file)
                else:
                    print(f"{func}: No events in transformed events, skip batch")

                try:
                    del events
                    del tranformed_events
                    del skipped_events
                    gc.collect()
                except Exception as e:
                    print(f"{func}: Failed to clear local variables. Continue anyways.")

            # save node features
            if node_feature.keys():
                print(f"{func}: Save node features")
                NODE_FEATURE_EXISTS_IN_REQUEST = True
                json.dump(node_feature, open(TEMP_NODE_FEATURE_INPUT_FILENAME, "w"))
                json.dump(node_feature, open(TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME, "w"))

            return process_chunks


        def get_3_words(dict_obj):
            """
            Extracts keys and values from a dictionary based on having three words and returns a concatenated string of values.

            Parameters:
            dict_obj (dict): The input dictionary containing key-value pairs to be processed.
            black_list (list): A list of keys to be ignored during processing.
            english_words (list): A list of valid English words to be used for counting.

            Returns:
            str: A concatenated string of values from `dict_obj` that meet the specified conditions.
            """
            global BLACK_LIST_KEYS
            global ENGLISH_WORDS

            keys = []
            output_str = ""
            for key in dict_obj.keys():
                if key in BLACK_LIST_KEYS:
                    continue

                target_str  = str(dict_obj[key]).lower()

                if len(target_str) < 4:
                    continue

                # Split the sentence into words
                words = re.findall(r'(?<=\s)\w+|\w+(?=\s)|^\w+|\w+$', target_str)

                # Count English words
                english_word_count = sum(1 for word in words if word in ENGLISH_WORDS)

                if english_word_count >= 3:
                    if output_str:
                        output_str += ' '
                    output_str += f'{target_str}'
                    keys.append([key,dict_obj[key]])
            return output_str


        def map_cef_to_internal(event, cef_to_internal_mappings):
            '''
            Map cef fields to internal
            Args:
                event: event from chunk of input
                cef_to_internal_mappings: cef to internal mappings list
            Returns:
                tranformed event mapped from cef to internal, cef mapped keys to internal event, event_node_feature
            '''

            self_loop = None
            self_loop_key = None

            transformed_event = {
                "id": "",
                "src": "",
                "dst": "",
                "time": "",
                "name": "",
                "other_attributes_dict": {}
            }

            other_attrs = {}
            event_node_feature = {}

            cef_mapped_keys_to_event = {
                "time": "",
                "src": "",
                "dst": "",
                "name": ""
            }

            for mapping in cef_to_internal_mappings:
                src_field = mapping["source_field"]
                dst_field = mapping["dest_field"]
                src_field_value = event[src_field]
                if src_field_value and src_field_value != "":
                    if dst_field in ["id"]:
                        transformed_event["id"] = str(src_field_value)
                    elif dst_field in ["dst"]:
                        transformed_event["dst"] = src_field_value
                        cef_mapped_keys_to_event["dst"] = src_field
                    elif dst_field in ["src"]:
                        transformed_event["src"] = src_field_value
                        cef_mapped_keys_to_event["src"] = src_field
                    elif dst_field in ["time"]:
                        transformed_event["time"] = src_field_value
                        cef_mapped_keys_to_event["time"] = src_field
                    elif dst_field in ["name"]:
                        transformed_event["name"] = str(src_field_value)
                        cef_mapped_keys_to_event["name"] = src_field

                    elif dst_field in ["self_loop_src (when both dst & src r empty) "]:
                        self_loop = src_field_value
                        self_loop_key = src_field

                    elif dst_field in ["event_ftr_IMPRTNT_priority"]:
                        try:
                            src_field_value = int(src_field_value)
                            other_attrs["priority"] = src_field_value
                        except Exception as e:
                            pass
                    elif dst_field in ["src_port", "dst_port"]:
                        try:
                            src_field_value = int(src_field_value)
                            other_attrs["port"] = src_field_value
                        except Exception as e:
                            pass
                    elif dst_field in ["event_ftr", "event_ftr_IMPRTNT"]:
                        other_attrs[src_field] = src_field_value

            transformed_event["other_attributes_dict"].update(other_attrs)

            # if none of the src, dst is present, then use self loop
            if transformed_event["src"] and not transformed_event["dst"]:
                transformed_event["dst"] = transformed_event["src"]
                cef_mapped_keys_to_event["dst"] = cef_mapped_keys_to_event["src"]

            elif not transformed_event["src"] and transformed_event["dst"]:
                transformed_event["src"] = transformed_event["dst"]
                cef_mapped_keys_to_event["src"] = cef_mapped_keys_to_event["dst"]

            elif not transformed_event["src"] and not transformed_event["dst"]:
                # self loop get the cef field for the self loop
                if self_loop is not None:
                    transformed_event["src"] = self_loop
                    transformed_event["dst"] = self_loop
                    cef_mapped_keys_to_event["src"] = self_loop_key
                    cef_mapped_keys_to_event["dst"] = self_loop_key
                else:
                    # if self loop is absent too, then required fields src, dest cannot be mapped. skip event
                    return None, cef_mapped_keys_to_event, event_node_feature

            # if required fields are empty, skip event:
            if not transformed_event["time"]:
                return None, cef_mapped_keys_to_event, event_node_feature

            # get name

            if not transformed_event["name"]:
                transformed_event["name"] = get_3_words(event)

            # convert to unix timestamp
            original_time = transformed_event["time"]
            unix_timestamp = convert_to_unix_timestamp(transformed_event["time"])
            if unix_timestamp is None:
                return None, cef_mapped_keys_to_event, event_node_feature
            transformed_event["time"] = unix_timestamp

            self_loop_exists = (transformed_event["src"] == transformed_event["dst"])

            # create node feature

            event_node_feature[transformed_event["src"]] = {}
            event_node_feature[transformed_event["dst"]] = {}

            for mapping in cef_to_internal_mappings:
                src_field = mapping["source_field"]
                dst_field = mapping["dest_field"]

                if dst_field not in ["dst_node_ftr", "src_node_ftr", "self_loop_node_ftr"]:
                    continue

                src_field_value = event[src_field]
                if src_field_value != "":
                    if dst_field in ["dst_node_ftr"]:
                        event_node_feature[transformed_event["dst"]][src_field] = src_field_value
                    elif dst_field in ["src_node_ftr"]:
                        event_node_feature[transformed_event["src"]][src_field] = src_field_value
                    elif dst_field in ["self_loop_node_ftr"]:
                        if self_loop_exists:
                            event_node_feature[transformed_event["src"]][src_field] = src_field_value

            # get id
            source_type = str(event['_sourceType'])
            source_type = source_type.replace("'", "\"")
            source_type = source_type.replace("\\", "\\\\")

            source = str(event['_source'])
            source = source.replace("'", "\"")
            source = source.replace("\\", "\\\\")

            event_type = str(event['_eventType'])
            event_type = event_type.replace("'", "\"")

            event_cd = str(event['_cd'])
            event_cd = event_cd.replace("'", "\"")

            event_uuid = str(uuid.uuid4())

            event_time = str(event["_time"])

            transformed_event["id"] = f'''"{transformed_event['src']}" "{transformed_event['dst']}"'''

            if event_time != "":
                transformed_event["id"] = f'''{transformed_event["id"]} _time="{event_time}"'''

            if source != "":
                transformed_event["id"] = f'''{transformed_event["id"]} source="{source}"'''

            if source_type != "":
                transformed_event["id"] = f'''{transformed_event["id"]} sourcetype="{source_type}"'''

            transformed_event["id"] = f'''{transformed_event["id"]} id:{event_uuid}'''

            return transformed_event, cef_mapped_keys_to_event, event_node_feature


        def preprocess_chunk_input(unique_id, chunk_of_large_input_file):
            '''
            Generate internal id and update chunk input
            Args:
                unique_id: The unique identifier for the data chunk.
                chunk_of_large_input_file: Chunk of input file
            '''

            chunk_input_df = pd.read_json(chunk_of_large_input_file)

            start_id, end_id = create_internal_id(unique_id, chunk_input_df["id"].to_list())

            chunk_input_df["id"] = list(range(start_id, end_id + 1))

            chunk_input_df.to_json(chunk_of_large_input_file, orient="records")


        def create_internal_id(unique_id, user_id_list):
            '''
            Create mapping from internal id to user id and internal id to unique id
            Args:
                unique_id: The unique identifier for the data chunk.
                user_id_list: List of user ids to map to internal id.
            Returns:
                Tuple containing the start and end internal id.
            '''
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE
            global TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE

            func = "create_internal_id"

            print(f"{func}: Create mapping from internal id to user id")
            temp_internal_id_to_user_id_json = json.load(open(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, "r"))

            start_id = len(temp_internal_id_to_user_id_json)
            temp_internal_id_to_user_id_json.extend(user_id_list)
            end_id = len(temp_internal_id_to_user_id_json) - 1

            print(f"{func}: Save mapping from internal id to user id")
            json.dump(temp_internal_id_to_user_id_json, open(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, "w"))

            print(f"{func}: Create mapping from internal id to unique id")
            temp_internal_id_to_unique_id_json = json.load(open(TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE, "r"))

            temp_internal_id_to_unique_id_json = temp_internal_id_to_unique_id_json + [unique_id] * len(user_id_list)

            print(f"{func}: Save mapping from internal id to unique id")
            json.dump(temp_internal_id_to_unique_id_json, open(TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE, "w"))

            return start_id, end_id


        def skip_creating_batch_transform_job(bucket, unique_id, first_unique_id, chunk_input_json):
            '''
            Skip creating a batch transform job and upload enriched alerts with techniques as input to cluster detection.
            Args:
                bucket: The S3 bucket name.
                unique_id: The unique id for classification.
                first_unique_id: first unique id of batch of input
                chunk_input_json: JSON data for chunk input.
            '''
            global SCRATCH_DIR
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_USER_FEEDBACK_WEIGHTS
            global S3_CLIENT

            func = "skip_creating_batch_transform_job"

            skip_batch_dir = "/tmp/skip_batch/"
            os.makedirs(skip_batch_dir, exist_ok=True)
            temp_enriched_alerts_with_techniques_output_filename = f"{skip_batch_dir}input.json"

            path_to_enriched_alerts_with_techniques = f"{SCRATCH_DIR}/output/classification/{first_unique_id}/input_{unique_id}.json"

            user_feedback_weights_json = json.load(open(TEMP_USER_FEEDBACK_WEIGHTS, "r"))
            edge_weights = user_feedback_weights_json["event"]

            with open(temp_enriched_alerts_with_techniques_output_filename, "w") as of:
                req = {"request_id": "1", "input": chunk_input_json, "user_feedback_weights": edge_weights}

                if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                    req["cluster_config"] = json.load(open(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, "r"))

                if NODE_FEATURE_EXISTS_IN_REQUEST:
                    req["node_feature"] = json.load(open(TEMP_NODE_FEATURE_INPUT_FILENAME, "r"))
                json.dump(req, of)

            try:
                print(f"{func}: Save enriched alerts with techniques as input to cluster detection: {bucket}/{path_to_enriched_alerts_with_techniques}")
                S3_CLIENT.upload_file(temp_enriched_alerts_with_techniques_output_filename, bucket, path_to_enriched_alerts_with_techniques)
            except Exception as e:
                print(f"{func}: Failed to save enriched alert with technique: {bucket}/{path_to_enriched_alerts_with_techniques}")
                raise e


        def fetch_or_create_technique_classification_lookup_file(bucket):
            '''
            Check if the technique classification lookup file for alerts exists on S3. Create if it does not exist.
            Args:
                bucket: Bucket name
            '''
            global TECHNIQUE_LOOKUP_OBJECT
            global S3_CLIENT
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE

            func = "fetch_or_create_technique_classification_lookup_file"
            try:
                print(
                    f"{func}: Checking if technique classification lookup file for alerts exists on S3 path {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")

                S3_CLIENT.head_object(Bucket=bucket, Key=TECHNIQUE_LOOKUP_OBJECT)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Technique classification lookup file for alerts does not exist on S3. Create empty table")

                technique_classification_lookup_df = pd.DataFrame(columns=["alerts", "techniques"])
                technique_classification_lookup_df.to_csv(TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE, index=False)

                print(f"{func}: Empty table created")
            else:
                try:
                    print(f"{func}: Download available technique classification lookup file for alerts from S3")

                    S3_CLIENT.download_file(bucket, TECHNIQUE_LOOKUP_OBJECT, TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup table from S3: {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")
                    raise e


        def fetch_or_create_internal_id_to_user_id_lookup_file(bucket):
            '''
            Check if the internal id to user id lookup file exists on S3. Create if it does not
            Args:
                bucket: Bucket name
            '''
            global INTERNAL_ID_TO_USER_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE

            func = "fetch_or_create_internal_id_to_user_id_lookup_file"

            try:
                print(
                    f"{func}: Checking if internal id to user id lookup file exists on S3 path {bucket}/{INTERNAL_ID_TO_USER_ID_OBJECT_KEY}")

                S3_CLIENT.head_object(Bucket=bucket, Key=INTERNAL_ID_TO_USER_ID_OBJECT_KEY)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Internal id to user id lookup file does not exist on S3. Create empty table")

                internal_id_to_user_id_json = []
                json.dump(internal_id_to_user_id_json, open(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, "w"))

                print(f"{func}: Empty table created")
            else:
                try:
                    print(f"{func}: Download available internal id to user id lookup file from S3")

                    S3_CLIENT.download_file(bucket, INTERNAL_ID_TO_USER_ID_OBJECT_KEY, TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{INTERNAL_ID_TO_USER_ID_OBJECT_KEY}")
                    raise e


        def fetch_or_create_internal_id_to_unique_id_lookup_file(bucket):
            '''
            Check if the internal id to unique id lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE

            func = "fetch_or_create_internal_id_to_unique_id_lookup_file"

            try:
                print(
                    f"{func}: Checking if internal id to unique id lookup file exists on S3 path {bucket}/{INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY}")

                S3_CLIENT.head_object(Bucket=bucket, Key=INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Internal id to unique id lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = []
                json.dump(internal_id_to_unique_id_json, open(TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE, "w"))

                print(f"{func}: Empty table created")
            else:
                try:
                    print(f"{func}: Download available uinternal id to unique id lookup file from S3")

                    S3_CLIENT.download_file(bucket, INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY, TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY}")
                    raise e


        def fetch_or_create_queue_lookup_file(bucket):
            '''
            Check if the queue lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global QUEUE_LOOKUP_FILE
            global S3_CLIENT
            global TEMP_QUEUE_LOOKUP_FILE

            func = "fetch_or_create_queue_lookup_file"

            try:
                print(
                    f"{func}: Checking if queue lookup file exists on S3 path {bucket}/{QUEUE_LOOKUP_FILE}")

                S3_CLIENT.head_object(Bucket=bucket, Key=QUEUE_LOOKUP_FILE)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Queue lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = {"first": None, "prev": None, "input_queue": []}
                json.dump(internal_id_to_unique_id_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))

                print(f"{func}: Initiated queue lookup")
            else:
                try:
                    print(f"{func}: Download available queue lookup file from S3")

                    S3_CLIENT.download_file(bucket, QUEUE_LOOKUP_FILE, TEMP_QUEUE_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{QUEUE_LOOKUP_FILE}")
                    raise e


        def fetch_or_create_global_attribute_weights(bucket, path_to_global_attribute_weights):
            '''
            Get or initialize the global attribute weights file
            Args:
                path_to_global_attribute_weights: global attribute weights object key
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_USER_FEEDBACK_WEIGHTS
            func = "fetch_or_create_global_attribute_weights"

            try:
                print(
                    f"{func}: Checking if file exists on S3 path {bucket}/{path_to_global_attribute_weights}")

                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_global_attribute_weights)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: File does not exist on S3. Initialize file")

                global_attribute_weights_json = {
                    "event": {
                        "technique": INITIAL_WEIGHTS_VALUE,
                        "tactic": INITIAL_WEIGHTS_VALUE,
                        "stage": INITIAL_WEIGHTS_VALUE,
                        "count": INITIAL_WEIGHTS_VALUE,
                        "priority": INITIAL_WEIGHTS_VALUE,
                        "port": INITIAL_WEIGHTS_VALUE,
                        "url": INITIAL_WEIGHTS_VALUE,
                        "user_agent": INITIAL_WEIGHTS_VALUE,
                        "cert": INITIAL_WEIGHTS_VALUE
                    },
                    "node": {
                        "os": INITIAL_WEIGHTS_VALUE,
                        "risk": INITIAL_WEIGHTS_VALUE,
                        "user": INITIAL_WEIGHTS_VALUE,
                        "domain": INITIAL_WEIGHTS_VALUE,
                        "subnet": INITIAL_WEIGHTS_VALUE,
                        "usergroup": INITIAL_WEIGHTS_VALUE,
                        "geolocation": INITIAL_WEIGHTS_VALUE
                    }
                }
                json.dump(global_attribute_weights_json, open(TEMP_USER_FEEDBACK_WEIGHTS, "w"))
                print(f"{func}: Initiated file")
                try:
                    print(f"{func}: Upload file to S3: {bucket}/{path_to_global_attribute_weights}")
                    S3_CLIENT.upload_file(TEMP_USER_FEEDBACK_WEIGHTS, bucket, path_to_global_attribute_weights)
                except Exception as ex:
                    print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_global_attribute_weights}")
                    raise ex
            else:
                try:
                    print(f"{func}: Download available file from S3")

                    S3_CLIENT.download_file(bucket, path_to_global_attribute_weights, TEMP_USER_FEEDBACK_WEIGHTS)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{path_to_global_attribute_weights}")
                    raise e


        def update_queue_lookup(bucket, unique_ids):
            '''
            Update queue lookup file
            Args:
                bucket: Bucket name
                unique_ids: Unique ids generated for the current input file
            '''
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE
            global S3_CLIENT
            func = "update_queue_lookup"

            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            payload_json = {}

            if queue_lookup_json["first"] is None:
                payload_json["first"] = unique_ids[0]
                queue_lookup_json["first"] = unique_ids[0]

            queue_lookup_json["prev"] = unique_ids[-1]
            payload_json["prev"] = unique_ids[-1]

            log_json = {"message": "update queue lookup", "payload": payload_json}
            print(f"{json.dumps(log_json)}")


        def create_chunk_for_large_input_file():
            '''
            Create chunks for a large input file
            Returns:
                List of split file names
            '''
            global CHUNK_SIZE
            global BUF_SIZE
            global TEMP_INPUT_DIR
            global TEMP_INPUT_FILENAME

            split_files = []
            with open(TEMP_INPUT_FILENAME, 'rb') as f:
                split_files = []
                item = 0
                input_list_iter = ijson.items(f, 'input.item', use_float=True, buf_size=BUF_SIZE)
                while True:
                    input_list = list(islice(input_list_iter, CHUNK_SIZE))
                    split_filename = TEMP_INPUT_DIR + "input" + '_' + str(item) + '.json'
                    if not input_list:
                        break

                    with open(split_filename, 'w') as outfile:
                        json.dump(input_list, outfile)
                        split_files.append(split_filename)

                    item += 1
                    del input_list
                    gc.collect()

            del input_list_iter
            gc.collect()

            return split_files


        def process_input_file_to_extract_config_and_node_feature():
            '''
            Process the input file to extract config and node feature
            '''
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global CONFIG_EXISTS_IN_REQUEST
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global BUF_SIZE
            global TEMP_INPUT_FILENAME
            global TEMP_CONFIG_INPUT_FILENAME
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME

            func = "process_input_file_to_extract_config_and_node_feature"

            with open(TEMP_INPUT_FILENAME, 'rb') as f:
                cluster_config_json_object = next(ijson.items(f, 'cluster_config', use_float=True, buf_size=BUF_SIZE), None)
                if cluster_config_json_object:
                    print(f"{func}: Cluster config exists in request")
                    CLUSTER_CONFIG_EXISTS_IN_REQUEST = True
                    if not isinstance(cluster_config_json_object, dict):
                        raise ValueError("The 'cluster_config' object in input file must be a json object")
                    json.dump(cluster_config_json_object, open(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, "w"))

            del cluster_config_json_object
            gc.collect()

            with open(TEMP_INPUT_FILENAME, 'rb') as f:
                config_json_object = next(ijson.items(f, 'config', use_float=True, buf_size=BUF_SIZE), None)
                if config_json_object:
                    print(f"{func}: Config exists in request")
                    CONFIG_EXISTS_IN_REQUEST = True
                    if not isinstance(config_json_object, dict):
                        raise ValueError("The 'config' object in input file must be a json object")
                    json.dump(config_json_object, open(TEMP_CONFIG_INPUT_FILENAME, "w"))

            del config_json_object
            gc.collect()

            with open(TEMP_INPUT_FILENAME, 'rb') as f:
                node_feature = next(ijson.items(f, 'node_feature', use_float=True, buf_size=BUF_SIZE), None)
                if node_feature:
                    print(f"{func}: node feature exists in request")
                    NODE_FEATURE_EXISTS_IN_REQUEST = True
                    if not isinstance(node_feature, dict):
                        raise ValueError("The 'node_feature' object in input file must be a json object")
                    json.dump(node_feature, open(TEMP_NODE_FEATURE_INPUT_FILENAME, "w"))
                    json.dump(node_feature, open(TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME, "w"))

            del node_feature
            gc.collect()


        def initialize_sqlite(bucket):
            '''
            Initialize sqlite if database is not present
            Args:
                bucket: Bucket name where sqlite will be stored
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global PATH_TO_SQLITE
            global TEMP_SQLITE_FILENAME

            func = "initialize_sqlite"

            try:
                print(f"{func}: Checking if sqlite exists on S3 path {bucket}/{PATH_TO_SQLITE}")

                S3_CLIENT.head_object(Bucket=bucket, Key=PATH_TO_SQLITE)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: sqlite not exist on S3. Initialize database.")

                create_cluster_output_table = """
                CREATE TABLE IF NOT EXISTS cluster_output (
                    cluster_id INTEGER PRIMARY KEY,
                    campaign_id INTEGER UNIQUE,
                    cluster_starttime REAL,
                    cluster_endtime REAL,
                    cluster_srcips TEXT,
                    cluster_dstips TEXT,
                    cluster_techs TEXT,
                    cluster_tacs TEXT,
                    cluster_stages TEXT
                );
                """

                create_cluster_ticket_output_table = """
                CREATE TABLE IF NOT EXISTS cluster_ticket_output (
                    cluster_id  INTEGER PRIMARY KEY,
                    ticket_id INTEGER UNIQUE,
                    metrics TEXT,
                    FOREIGN KEY (cluster_id) REFERENCES cluster_output(cluster_id)
                );
                """

                create_event_table = """
                CREATE TABLE IF NOT EXISTS event (
                    alert_id TEXT PRIMARY KEY,
                    cluster_id INTEGER,
                    cluster_added_to_ui INTEGER DEFAULT 0,
                    flow_added_to_ui INTEGER DEFAULT 0,
                    src TEXT,
                    dst TEXT,
                    time REAL,
                    name TEXT,
                    tech TEXT,
                    tac TEXT,
                    stage TEXT,
                    other_attributes TEXT,
                    kv_src TEXT,
                    kv_dst TEXT,
                    kv_time TEXT,
                    kv_name TEXT,
                    FOREIGN KEY (cluster_id) REFERENCES cluster_output(cluster_id)
                );
                """

                create_flow_output_table = """
                CREATE TABLE IF NOT EXISTS flow_output (
                    flow_id  INTEGER PRIMARY KEY,
                    campaign_id INTEGER UNIQUE,
                    cluster_prob TEXT,
                    alert_ids TEXT
                );
                """

                create_operation_on_cluster_table = """
                CREATE TABLE IF NOT EXISTS operation_on_cluster (
                    operation_id  INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_id INTEGER,
                    alert_ids TEXT,
                    operation_type TEXT,
                    FOREIGN KEY (cluster_id) REFERENCES cluster_output(cluster_id)
                );
                """

                create_operation_on_flow_table = """
                CREATE TABLE IF NOT EXISTS operation_on_flow (
                    operation_id  INTEGER PRIMARY KEY AUTOINCREMENT,
                    flow_id INTEGER,
                    alert_ids TEXT,
                    operation_type TEXT,
                    FOREIGN KEY (flow_id) REFERENCES flow_output(flow_id)
                );
                """

                create_global_feature_table = """
                CREATE TABLE IF NOT EXISTS global_feature (
                    feature TEXT NOT NULL,
                    feature_type TEXT NOT NULL,
                    added_to_ui INTEGER,
                    CONSTRAINT unique_feature_type UNIQUE (feature, feature_type)
                );
                """

                try:
                    # Connect to the database
                    with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                        with closing(conn.cursor()) as cursor:

                            # Execute a query
                            cursor.execute(create_cluster_output_table)
                            cursor.execute(create_cluster_ticket_output_table)
                            cursor.execute(create_event_table)
                            cursor.execute(create_flow_output_table)
                            cursor.execute(create_operation_on_cluster_table)
                            cursor.execute(create_operation_on_flow_table)
                            cursor.execute(create_global_feature_table)
                        conn.commit()

                    print(f"{func}: Database initialized")
                except Exception as exc:
                    print(f"{func}: Failed to create tables in database.")
                    raise exc

                try:
                    print(f"{func}: Upload sqlite to S3")

                    S3_CLIENT.upload_file(TEMP_SQLITE_FILENAME, bucket, PATH_TO_SQLITE)

                    print(f"{func}: Upload completed")
                except Exception as e:
                    print(f"{func}: Failed to upload sqlite from S3: {bucket}/{PATH_TO_SQLITE}")
                    raise e
            else:
                try:
                    print(f"{func}: Download available sqlite")

                    S3_CLIENT.download_file(bucket, PATH_TO_SQLITE, TEMP_SQLITE_FILENAME)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download sqlite from S3: {bucket}/{PATH_TO_SQLITE}")
                    raise e


        def process_node_features():
            '''
            Encode node features
            '''
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_USER_FEEDBACK_WEIGHTS
            global TEMP_NODE_FEATURE_LOOKUP_FILE
            func = "process_node_features"

            node_feature = json.load(open(TEMP_NODE_FEATURE_INPUT_FILENAME, "r"))
            user_feedback_weights = json.load(open(TEMP_USER_FEEDBACK_WEIGHTS, "r"))
            node_weights = user_feedback_weights["node"]

            node_feature_lookup = {}

            for key in node_feature.keys():
                if type(node_feature[key]) is dict:
                    for key_2 in node_feature[key]:
                        if key_2 not in node_feature_lookup:
                            node_feature_lookup[key_2] = len(node_feature_lookup)
                else:
                    print(f"{func}: Skip node feature: {key}")

            new_node_feature = {}
            encode_table = {}
            for key in node_feature.keys():
                if type(node_feature[key]) is dict:
                    new_node_feature_embedding = [0] * len(node_feature_lookup)
                    for key_2 in node_feature[key]:
                        if node_feature[key][key_2]:
                            encoded, encode_table = custom_encoder_node(node_feature[key][key_2], encode_table)
                        else:
                            encoded = 0
                        if key_2 in node_weights:
                            encoded = encoded * (node_weights[key_2] / 100.0)
                        new_node_feature_embedding[node_feature_lookup[key_2]] = encoded
                    new_node_feature[key] = new_node_feature_embedding

            json.dump(new_node_feature, open(TEMP_NODE_FEATURE_INPUT_FILENAME, "w"))
            json.dump(node_feature_lookup, open(TEMP_NODE_FEATURE_LOOKUP_FILE, "w"))


        def custom_encoder_node(data, encode_table):
            '''
            Simple encoder
            Args:
                data: data to encode
                encode_table: history
            Returns:
                (encoded data, updated history)
            '''
            if str(data) not in encode_table:
                encode_table[str(data)] = len(encode_table)
            return encode_table[str(data)], encode_table


        def custom_encoder(data, encode_table):
            '''
            Simple encoder
            Args:
                data: data to encode
                encode_table: history
            Returns:
                (encoded data, updated history)
            '''
            if str(data) not in encode_table:
                encode_table[str(data)] = len(encode_table)
            return [encode_table[str(data)]], encode_table


        def get_cef_to_internal_mappings():
            '''
            Create cef to internal mappings dataframe
            Returns:
                dataframe of cef to internal mappings
            '''
            cef_to_internal_mappings = [
                {
                    "source_field": "accessGroup",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientLogonId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientMachineName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddress",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddress",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddressIPv6",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceCustomString1",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceHostName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceNtHost",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "domain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "group",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "groupDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "groupTypeChange",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "hostname",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonAccount",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonAccount",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberDn",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberNtDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "primaryDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "primaryUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectAccountDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectAccountName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectDomainName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectLogonId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectLogonId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectSecurityId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "suser",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "user",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userGroup",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userGroupId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userType",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "workstation",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "workstationName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "workstationName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "acl",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "act",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "act",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachDisposition",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachFileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachSize",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachSizeDecoded",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachTransferEncoding",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "authMethod",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "awsAccountId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "body",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bucket",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytesIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytesOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerComputerName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerDomain",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerLogonId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerMachineName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerUserName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cat",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cat",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "categoryString",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cd",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeClass",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeDescription",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaMessageId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaUser",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaVendorAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn1",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn2",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn3",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn4",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn5",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn6",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "command",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "commProto",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "content",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "count",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "creatorProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs1",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs1Label",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs2",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs3",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs4",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs5",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs6",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceDirection",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceProduct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceProduct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceReceiptTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVendor",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVendor",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVersion",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVersion",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "duration",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationHour",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationMinute",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "errorCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventHour",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventMinute",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventMonth",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventSubtype",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventTypeColor",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventWeekDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventYear",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventZone",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "exitStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "filePath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "filePath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileSize",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "flowId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fragmentCount",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "hashCodes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "icmpCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "icmpType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "id",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "idsType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "imageFileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "index",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "indexTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "initialRtt",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "interfaceId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "keywords",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "kv",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "laction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "lineCount",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logLevel",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "messageId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "messageType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "name",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "name",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newAccountName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newDomain",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "object",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectAttrs",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "opCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "outcome",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packets",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packetsIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packetsOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcess",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessPath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "preMsg",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "privilegeList",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "product",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "punct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "query",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "query",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "queryType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "raw",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "recordNumber",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "region",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryPath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryValueName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryValueType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "replyCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "replyCodeId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestClientApplication",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestMethod",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestURL",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "responseTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "rule",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ruleId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "securityId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serial",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "service",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serviceId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serviceName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sessionId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "si",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "signatureId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "source",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceContent",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceNetworkAddress",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceSgInfo",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "splunkServer",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "splunkServerGroup",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sslIsValid",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "status",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "subject",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "subSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "suppliedRealmName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tag",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagApp",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagEventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagObjectCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "taskCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "timeTaken",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tokenElevationType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tokenElevationType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tos",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalBytes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalPacketsIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalPacketsOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalResponseTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "transactionId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "transport",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ttl",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "type",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "type",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "valuesFlowId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "version",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "vpcFlowAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winSecurityCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationAddress",
                    "dest_field": "dst",
                    "priority": 6.0
                },
                {
                    "source_field": "destinationNtHost",
                    "dest_field": "dst",
                    "priority": 6.0
                },
                {
                    "source_field": "destinationAddress",
                    "dest_field": "dst",
                    "priority": 5.0
                },
                {
                    "source_field": "destinationAddressIPv6",
                    "dest_field": "dst",
                    "priority": 4.0
                },
                {
                    "source_field": "destinationHostName",
                    "dest_field": "dst",
                    "priority": 3.0
                },
                {
                    "source_field": "destinationTranslatedAddress",
                    "dest_field": "dst",
                    "priority": 1.0
                },
                {
                    "source_field": "destContent",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destHost",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationInterface",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationInterfaceId",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationMacAddress",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationNtDomain",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetAccountId",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationPriority",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetAccountName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationZone",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destSgInfo",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "duser",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetDomain",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetDomainName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetProcessName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetServerName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetServerName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetUserName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetUserName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "description",
                    "dest_field": "name",
                    "priority": 5.0
                },
                {
                    "source_field": "name",
                    "dest_field": "name",
                    "priority": 4.0
                },
                {
                    "source_field": "ruleName",
                    "dest_field": "name",
                    "priority": 3.0
                },
                {
                    "source_field": "description",
                    "dest_field": "name",
                    "priority": 2.0
                },
                {
                    "source_field": "msg",
                    "dest_field": "name",
                    "priority": 2.0
                },
                {
                    "source_field": "message",
                    "dest_field": "name",
                    "priority": 1.0
                },
                {
                    "source_field": "destPublicPort",
                    "dest_field": "dst_port",
                    "priority": 3.0
                },
                {
                    "source_field": "destinationTranslatedPort",
                    "dest_field": "dst_port",
                    "priority": 2.0
                },
                {
                    "source_field": "destinationPort",
                    "dest_field": "dst_port",
                    "priority": 1.0
                },
                {
                    "source_field": "process",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processCommandLine",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processExec",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processId",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processId",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processName",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processName",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processPath",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "proto",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoCode",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoFullName",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoId",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoStack",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoVersion",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "sourceType",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "sourceType",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendor",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorAccount",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorAction",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorClass",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorDefinition",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorPrivilege",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorSeverity",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "severity",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "severityId",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "severityLevel",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "computerName",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 6.0
                },
                {
                    "source_field": "hostAddr",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 5.0
                },
                {
                    "source_field": "assignedIp",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 4.0
                },
                {
                    "source_field": "clientAddress",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 3.0
                },
                {
                    "source_field": "ip",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 2.0
                },
                {
                    "source_field": "ipAddress",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 1.0
                },
                {
                    "source_field": "sourceAddressIPv6",
                    "dest_field": "src",
                    "priority": 6.0
                },
                {
                    "source_field": "sourceTranslatedAddress",
                    "dest_field": "src",
                    "priority": 5.0
                },
                {
                    "source_field": "sourceAddress",
                    "dest_field": "src",
                    "priority": 4.0
                },
                {
                    "source_field": "sourceAddress",
                    "dest_field": "src",
                    "priority": 3.0
                },
                {
                    "source_field": "sourceNtHost",
                    "dest_field": "src",
                    "priority": 2.0
                },
                {
                    "source_field": "sourceHostName",
                    "dest_field": "src",
                    "priority": 1.0
                },
                {
                    "source_field": "sourceHost",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceInterface",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceInterfaceId",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceMacAddress",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceNtDomain",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourcePriority",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceUserName",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceWorkstation",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceZone",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "suser",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourcePublicPort",
                    "dest_field": "src_port",
                    "priority": 3.0
                },
                {
                    "source_field": "sourceTranslatedPort",
                    "dest_field": "src_port",
                    "priority": 2.0
                },
                {
                    "source_field": "sourcePort",
                    "dest_field": "src_port",
                    "priority": 1.0
                },
                {
                    "source_field": "endTime",
                    "dest_field": "time",
                    "priority": 5.0
                },
                {
                    "source_field": "endTime",
                    "dest_field": "time",
                    "priority": 4.0
                },
                {
                    "source_field": "startTime",
                    "dest_field": "time",
                    "priority": 3.0
                },
                {
                    "source_field": "timestamp",
                    "dest_field": "time",
                    "priority": 2.0
                },
                {
                    "source_field": "time",
                    "dest_field": "time",
                    "priority": 1.0
                }
            ]

            # cef_to_internal_mappings_df = pd.DataFrame(cef_to_internal_mappings)

            return cef_to_internal_mappings


        def convert_to_unix_timestamp(time_str):
            """
            Convert any date/time format to a UNIX timestamp.
            Args:
                time_str: The date/time string or UNIX timestamp to convert.
            Returns
                return: The UNIX timestamp.
            """
            try:
                # Check if the input is already a UNIX timestamp
                if isinstance(time_str, (int, float)):
                    return time_str

                # Attempt to convert string to a float or int
                try:
                    timestamp = float(time_str)
                    return timestamp
                except ValueError:
                    pass

                # Parse the date/time string into a datetime object
                dt = parser.parse(time_str)
                # Convert the datetime object to a UNIX timestamp
                unix_timestamp = int(dt.timestamp())
                return unix_timestamp
            except Exception as e:
                print(f"Error: {e}")
                return None


        def get_static_cluster_config():
            '''
            Get static cluster config for clustering model
            '''
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST

            CLUSTER_CONFIG_EXISTS_IN_REQUEST = True
            cluster_config = {
                "set_space": 256,
                "max_cluster_size": 64,
                "eps_ds": 45,
                "lambd": 0.05,
                "beta": 0.4,
                "mu": 7,
                "eps_ds_DBSCAN": 25,
                "min_samples_ds": 7,
                "secondary_clustering": False,
                "contrastive_loss": "js",
                "triplet_margin": 0.1,
                "max_time": None,
                "time_feat_dim": 8,
                "channel_embedding_dim": 128,
                "direct_edge_weight": 1000,
                "technique": 1000000,
                "tactic": 100,
                "stage": 1,
                "count": 10,
                "priority": 10000,
                "port": 1,
                "url": 10,
                "user_agent": 1000000,
                "cert": 10000,
                "time": 1,
                "user_feedback": 100,
                "learning_rate": 3e-05,
                "num_epochs": 30,
                "batch_size": 500,
                "embedding_type": "avg",
                "dropout": 0.05,
                "patch_size": 1,
                "max_input_sequence_length": 64,
                "num_neighbors": 30,
                "seed": 1,
                "aggalerts_flag": False
            }

            json.dump(cluster_config, open(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, "w"))


        def delete_error_log_s3(bucket):
            '''
            Delete contents of output folder in root of S3 bucket
            '''
            global S3_CLIENT

            func = "delete_error_log_s3"

            error_log_key = "output/error_log.txt"

            # Delete the objects
            response = S3_CLIENT.delete_object(
                Bucket=bucket,
                Key=error_log_key
            )

            print(f"{func}: Deleted objects: {response}")


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          batch_transform_job_instance_type: !Ref EnrichTechBatchInstanceType
          batch_transform_job_max_retries: '1'
          batch_transform_job_timeout: '3600'
          batch_transform_job_max_payload: '10'
          batch_transform_job_concurrent_transform: '1'
          batch_transform_job_instance_count: !Ref EnrichTechBatchInstanceCount
          technique_model_name: !Ref TechniqueModelName
          technique_lookup_object: !Ref TechniqueLookupObject
          chunk_size: !Ref ChunkSize
          encode_other_attrs: 'true'
          map_cef_to_internal: 'true'
          encode_node_feature: 'true'
  enrichWithTechniqueLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${enrichWithTechnique}
  enrichWithTechniqueLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref enrichWithTechnique
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  processEnrichedWithTechnique:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_process_enriched_with_technique
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: processEnrichedWithTechnique
      InlineCode: |
        '''
        Process response from technique classification model and enrich alerts with technique. Create input for the Temporal clustering model.

        Input: Response from Technique classification model
        '''

        import os
        import urllib
        import json
        import glob
        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        BUCKET = os.getenv("bucket")

        missing_variables = []

        if BUCKET is None:
            missing_variables.append("bucket")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        TEMP_INPUT_DIR = "/tmp/input/"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)

        SCRATCH_DIR = "scratch"

        TEMP_INPUT_FILENAME = "/tmp/classification_response.json"
        TEMP_ALERTS_INPUT_FILENAME = f"{TEMP_INPUT_DIR}input.json"
        TEMP_QUEUE_FILENAME = f"{TEMP_INPUT_DIR}queue.json"
        TEMP_CLUSTER_CONFIG_INPUT_FILENAME = f"{TEMP_INPUT_DIR}cluster_config.json"
        TEMP_NODE_FEATURE_INPUT_FILENAME = f"{TEMP_INPUT_DIR}node_feature.json"
        TEMP_USER_FEEDBACK_WEIGHTS = f"{TEMP_INPUT_DIR}user_feedback_weights.json"

        CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
        NODE_FEATURE_EXISTS_IN_REQUEST = False


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_INPUT_FILENAME
            global SCRATCH_DIR
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global NODE_FEATURE_EXISTS_IN_REQUEST

            func = "lambda_handler"

            CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
            NODE_FEATURE_EXISTS_IN_REQUEST = False

            # bucket = event["Records"][0]["s3"]["bucket"]["name"]
            # input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            # bucket, input_filename = get_output_object_key(event)

            print(event)

            bucket = BUCKET
            input_filename = event["Key"]

            try:

                clear_temp_dir()

                print(f"{func}: Download file to process from S3 bucket: {bucket}/{input_filename}")

                try:
                    S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                    print(f"{func}: File to process download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file to process")
                    raise e

                first_unique_id = input_filename.split("/")[-2]

                unique_id = input_filename.split("/")[-1].split("_")[-1].split(".")[0]

                print(f"{func}: Download intermediate files from S3")
                try:
                    download_intermediate_input_files(bucket, unique_id)

                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                print(f"{func}: Processing chunk file and enriching with techniques. unique ID: {unique_id}")
                path_to_enriched_alerts_with_techniques = f"{SCRATCH_DIR}/output/classification/{first_unique_id}/input_{unique_id}.json"
                lambda_response = process_classification_output(bucket, unique_id, path_to_enriched_alerts_with_techniques)

                print(f"{func}: Enriched with techniques: {unique_id}")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def process_classification_output(bucket, unique_id, path_to_enriched_alerts_with_techniques):
            '''
            Enrich alerts with technique. Start next technique classification batch transform job from queue.
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
                path_to_enriched_alerts_with_techniques: S3 path to save enriched alerts
            Returns:
                Lambda response with next job to run
            '''
            global TEMP_INPUT_FILENAME
            global TEMP_ALERTS_INPUT_FILENAME
            global TEMP_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_USER_FEEDBACK_WEIGHTS
            global S3_CLIENT

            func = "process_classification_output"

            print(f"{func}: Load classification response file")
            classification_response_lines = open(TEMP_INPUT_FILENAME, "r").readlines()

            print(f"{func}: Load chunk input file")
            chunk_input_json = json.load(open(TEMP_ALERTS_INPUT_FILENAME, "r"))

            print(f"{func}: Gather alerts and classified techniques")

            classification_response_list = []
            for response_lines in classification_response_lines:
                response = json.loads(response_lines)
                if "labels" in response and "techniques" in response["labels"]:
                    classification_response_dict = {"alerts": response["raw_alert"], "techniques": json.dumps(response["labels"]["techniques"])}
                else:
                    classification_response_dict = {"alerts": response["raw_alert"], "techniques": json.dumps([])}

                classification_response_list.append(classification_response_dict)

            classification_response_df = pd.DataFrame(classification_response_list, columns=["alerts", "techniques"])

            # Enrich input with recognized techniques from response from technique detection model
            if len(classification_response_df) > 0:
                print(f"{func}: Enrich alerts with techniques")

                for chunk_input_alert in chunk_input_json:

                    if len(chunk_input_alert["tech"]) > 0:
                        continue

                    alert_text = chunk_input_alert["name"]
                    alert_text_in_classification_response_df = classification_response_df["alerts"].eq(alert_text).any()

                    if alert_text_in_classification_response_df:
                        tech = classification_response_df[classification_response_df["alerts"] == alert_text].iloc[0]["techniques"]
                        tech = json.loads(tech)
                        chunk_input_alert["tech"] = tech

            temp_enriched_alerts_with_techniques_output_filename = "/tmp/input.json"

            user_feedback_weights_json = json.load(open(TEMP_USER_FEEDBACK_WEIGHTS, "r"))
            edge_weights = user_feedback_weights_json["event"]

            # Create request for temporal clustering model
            with open(temp_enriched_alerts_with_techniques_output_filename, "w") as of:
                req = {"request_id": "1", "input": chunk_input_json, "user_feedback_weights": edge_weights}

                if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                    req["cluster_config"] = json.load(open(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, "r"))

                if NODE_FEATURE_EXISTS_IN_REQUEST:
                    req["node_feature"] = json.load(open(TEMP_NODE_FEATURE_INPUT_FILENAME, "r"))
                json.dump(req, of)

            try:
                print(f"{func}: Save enriched alerts with techniques as input to cluster detection: {bucket}/{path_to_enriched_alerts_with_techniques}")
                S3_CLIENT.upload_file(temp_enriched_alerts_with_techniques_output_filename, bucket, path_to_enriched_alerts_with_techniques)
            except Exception as e:
                print(f"Failed to save enriched alerts with technique to S3: {bucket}/{path_to_enriched_alerts_with_techniques}")
                raise e

            update_lookup_table(bucket, unique_id, classification_response_df)

            lambda_response = {}
            current_queue_json = json.load(open(TEMP_QUEUE_FILENAME, "r"))
            lambda_response["TransformJobName"] = "transform-job-cluster"
            lambda_response["bucket"] = bucket
            lambda_response["input_first_unique_id"] = current_queue_json["first"]

            return lambda_response


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download chunk input file, queue, config if exists, and node features if exists
            Args:
                bucket: Bucket name
                path_to_intermediate_input_before_technique_classification: S3 path to chunk input file before technique enrichment
                path_to_intermediate_queue: S3 path to queue for chunk
                path_to_config_from_input: S3 path to config
                path_to_node_feature_from_input: S3 path to node_feature
            '''
            global TEMP_ALERTS_INPUT_FILENAME
            global TEMP_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_USER_FEEDBACK_WEIGHTS

            func = "download_intermediate_input_files"

            path_to_intermediate_input_before_technique_classification = f"{SCRATCH_DIR}/intermediate/{unique_id}/input.json"
            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            path_to_cluster_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_config.json"
            path_to_node_feature_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/node_feature.json"
            path_to_global_attribute_weights = f"{SCRATCH_DIR}/attribute_weights.json"

            S3_CLIENT.download_file(bucket, path_to_intermediate_input_before_technique_classification, TEMP_ALERTS_INPUT_FILENAME)
            S3_CLIENT.download_file(bucket, path_to_intermediate_queue, TEMP_QUEUE_FILENAME)
            S3_CLIENT.download_file(bucket, path_to_global_attribute_weights, TEMP_USER_FEEDBACK_WEIGHTS)

            try:
                print(f"{func}: Checking if cluster config file exists on S3 path {bucket}/{path_to_cluster_config_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_cluster_config_from_input)
                CLUSTER_CONFIG_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Cluster config file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_cluster_config_from_input, TEMP_CLUSTER_CONFIG_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_cluster_config_from_input}")
                    raise e

            try:
                print(f"{func}: Checking if node feaures file exists on S3 path {bucket}/{path_to_node_feature_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_node_feature_from_input)
                NODE_FEATURE_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Node feaures file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_node_feature_from_input, TEMP_NODE_FEATURE_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_node_feature_from_input}")
                    raise e


        def update_lookup_table(bucket, unique_id, classification_response_df):
            '''
            Create input to update technique classification lookup table. Trigger lookup table update.
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
                classification_response_df: dataframe to add to lookup table
            '''
            global SCRATCH_DIR
            global S3_CLIENT

            func = "update_lookup_table"

            classification_response_s3_key = f"{SCRATCH_DIR}/queue/{unique_id}/classification_response.csv"

            temp_message_payload_dir = "/tmp/message_payload/"
            os.makedirs(temp_message_payload_dir, exist_ok=True)
            temp_message_payload_filename = f"{temp_message_payload_dir}classification_response.csv"

            if len(classification_response_df) < 1:
                return
            classification_response_df.to_csv(temp_message_payload_filename, index=False)

            try:
                S3_CLIENT.upload_file(temp_message_payload_filename, bucket, classification_response_s3_key)
            except Exception as e:
                print(f"{func}: Failed to upload classification response message payload file to: {bucket}/{classification_response_s3_key}")
                raise e

            message_body = {
                "s3_bucket": bucket,
                "s3_key": classification_response_s3_key
            }

            print(json.dumps({"event": "update lookup from message", "message": message_body}))


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
  processEnrichedWithTechniqueLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${processEnrichedWithTechnique}
  processEnrichedWithTechniqueLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref processEnrichedWithTechnique
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  createEmbedding:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_create_embedding
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: createEmbedding
      InlineCode: |
        '''
        Create batch transform job for Temporal Clustering model

        Input: input alerts enriched with techniques
        '''

        import json
        import os
        import urllib
        import zipfile
        import glob

        import boto3
        from botocore.exceptions import ClientError

        EMBEDDING_MODEL_NAME = os.getenv("embedding_model_name")

        BATCH_TRANSFORM_JOB_INSTANCE_TYPE = os.getenv("batch_transform_job_instance_type")

        BATCH_TRANSFORM_JOB_MAX_RETRIES = os.getenv("batch_transform_job_max_retries")

        BATCH_TRANSFORM_JOB_TIMEOUT = os.getenv("batch_transform_job_timeout")

        BATCH_TRANSFORM_JOB_SUFFIX = os.getenv("batch_transform_job_suffix")

        BATCH_TRANSFORM_JOB_INSTANCE_COUNT = os.getenv("batch_transform_job_instance_count")

        missing_variables = []
        if EMBEDDING_MODEL_NAME is None:
            missing_variables.append("embedding_model_name")
        if BATCH_TRANSFORM_JOB_INSTANCE_TYPE is None:
            missing_variables.append("batch_transform_job_instance_type")
        if BATCH_TRANSFORM_JOB_MAX_RETRIES is None:
            missing_variables.append("batch_transform_job_max_retries")
        if BATCH_TRANSFORM_JOB_TIMEOUT is None:
            missing_variables.append("batch_transform_job_timeout")
        if BATCH_TRANSFORM_JOB_INSTANCE_COUNT is None:
            missing_variables.append("batch_transform_job_instance_count")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        if BATCH_TRANSFORM_JOB_SUFFIX is None:
            BATCH_TRANSFORM_JOB_SUFFIX = ""

        BATCH_TRANSFORM_JOB_MAX_RETRIES = int(BATCH_TRANSFORM_JOB_MAX_RETRIES)
        BATCH_TRANSFORM_JOB_TIMEOUT = int(BATCH_TRANSFORM_JOB_TIMEOUT)
        BATCH_TRANSFORM_JOB_INSTANCE_COUNT = int(BATCH_TRANSFORM_JOB_INSTANCE_COUNT)

        S3_CLIENT = boto3.client("s3")
        SAGEMAKER = boto3.client(service_name='sagemaker')

        SCRATCH_DIR = "scratch"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global SAGEMAKER
            func = "lambda_handler"

            bucket = event["bucket"]
            try:

                clear_temp_dir()

                unique_id = event['input_first_unique_id']

                print(f"{func}: Start batch transform job for aggregate and cluster and alerts")

                try:
                    batch_transform_job = setup_batch_transform_job_for_embedding_alerts(bucket, unique_id)
                    response = SAGEMAKER.create_transform_job(**batch_transform_job)
                except Exception as e:
                    print(f"{func}: Failed to create batch transform job. Unique id: {unique_id}")
                    raise e
                print(f"{func}: Created batch transform job for aggregate and cluster alerts: {response['TransformJobArn']}")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            lambda_response = {"TransformJobName": f'transform-job-embedding-{unique_id}'}
            return lambda_response


        def setup_batch_transform_job_for_embedding_alerts(bucket, unique_id):
            '''
            Configure batch transform job for Temporal clustering part 1
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
            Returns:
                batch_transform_job: Configuration to create batch transform job
            '''
            transform_job_name = f'transform-job-embedding-{unique_id}'
            if BATCH_TRANSFORM_JOB_SUFFIX:
                transform_job_name += f"-{BATCH_TRANSFORM_JOB_SUFFIX}"

            transform_input = {
                "DataSource": {
                    "S3DataSource": {
                        "S3DataType": "S3Prefix",
                        "S3Uri": f"s3://{bucket}/{SCRATCH_DIR}/output/classification/{unique_id}/"
                    }
                },
                "ContentType": "application/zip",
                "SplitType": "None"
            }
            transform_output = {
                "S3OutputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/embedding_out/{unique_id}/",
                "KmsKeyId": ""
            }
            transform_resources = {
                "InstanceType": BATCH_TRANSFORM_JOB_INSTANCE_TYPE,
                "InstanceCount": BATCH_TRANSFORM_JOB_INSTANCE_COUNT
            }
            model_client_config = {
                'InvocationsTimeoutInSeconds': BATCH_TRANSFORM_JOB_TIMEOUT,
                'InvocationsMaxRetries': BATCH_TRANSFORM_JOB_MAX_RETRIES
            }

            batch_transform_job = {
                'TransformJobName': transform_job_name,
                'ModelName': EMBEDDING_MODEL_NAME,
                'TransformInput': transform_input,
                'TransformOutput': transform_output,
                'MaxPayloadInMB': 0,
                'MaxConcurrentTransforms': 1,
                "TransformResources": transform_resources,
                "ModelClientConfig": model_client_config
            }
            return batch_transform_job


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          embedding_model_name: !Ref ClusterModelName1
          batch_transform_job_instance_type: !Ref ClusterBatchInstanceType1
          batch_transform_job_max_retries: '1'
          batch_transform_job_timeout: '3600'
          batch_transform_job_instance_count: !Ref ClusterBatchBatchInstanceCount1
  createEmbeddingLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${createEmbedding}
  createEmbeddingLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref createEmbedding
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  processEmbedding:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_process_embedding
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: processEmbedding
      InlineCode: |
        '''
        Process response from embedding model and create input for temporal clustering model.

        Input: Response from temporal clustering model part 1
        '''

        import os
        import urllib
        import json
        import glob
        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        BUCKET = os.getenv("bucket")

        missing_variables = []

        if BUCKET is None:
            missing_variables.append("bucket")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global BUCKET
            global SCRATCH_DIR

            func = "lambda_handler"

            # bucket = event["Records"][0]["s3"]["bucket"]["name"]
            # input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            # bucket, input_filename = get_output_object_key(event)

            print(event)

            bucket = BUCKET
            input_filename = event["Key"]

            try:

                clear_temp_dir()

                first_unique_id = input_filename.split("/")[-2]

                unique_id = input_filename.split("/")[-1].split("_")[-1].split(".")[0]

                print(f"{func}: Processing chunk file and creating output for embedding model. unique ID: {unique_id}")
                path_to_embedding_output = f"{SCRATCH_DIR}/output/embedding/{unique_id}/input.gz"
                process_embedding_output(bucket, path_to_embedding_output, input_filename)

                print(f"{func}: Processed chunk: {unique_id}")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            lambda_response = {
                "TransformJobName": "transform-job-cluster",
                "bucket": bucket,
                "input_first_unique_id": first_unique_id
            }

            return lambda_response


        def process_embedding_output(bucket, path_to_embedding_output, input_filename):
            '''
            Copy response from embedding output to corresponding output path
            Args:
                bucket: Bucket name
                path_to_embedding_output: S3 path to save output for embedding model
                input_filename: S3 path to response from embedding model
            '''
            global S3_CLIENT

            func = "process_classification_output"

            copy_source = {
                'Bucket': bucket,
                'Key': input_filename
            }

            try:
                print(f"{func}: Upload embedding model output in S3")
                S3_CLIENT.copy(copy_source, bucket, path_to_embedding_output)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_embedding_output}")
                raise e


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
  processEmbeddingLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${processEmbedding}
  processEmbeddingLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref processEmbedding
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  createCluster:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_create_cluster
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: createCluster
      InlineCode: |
        '''
        Create batch transform job for Temporal Clustering model

        Input: input alerts enriched with techniques
        '''

        import json
        import os
        import urllib
        import zipfile
        import glob

        import boto3
        from botocore.exceptions import ClientError

        CLUSTER_MODEL_NAME = os.getenv("cluster_model_name")

        BATCH_TRANSFORM_JOB_INSTANCE_TYPE = os.getenv("batch_transform_job_instance_type")

        BATCH_TRANSFORM_JOB_MAX_RETRIES = os.getenv("batch_transform_job_max_retries")

        BATCH_TRANSFORM_JOB_TIMEOUT = os.getenv("batch_transform_job_timeout")

        BATCH_TRANSFORM_JOB_SUFFIX = os.getenv("batch_transform_job_suffix")

        missing_variables = []
        if CLUSTER_MODEL_NAME is None:
            missing_variables.append("cluster_model_name")
        if BATCH_TRANSFORM_JOB_INSTANCE_TYPE is None:
            missing_variables.append("batch_transform_job_instance_type")
        if BATCH_TRANSFORM_JOB_MAX_RETRIES is None:
            missing_variables.append("batch_transform_job_max_retries")
        if BATCH_TRANSFORM_JOB_TIMEOUT is None:
            missing_variables.append("batch_transform_job_timeout")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        if BATCH_TRANSFORM_JOB_SUFFIX is None:
            BATCH_TRANSFORM_JOB_SUFFIX = ""

        BATCH_TRANSFORM_JOB_MAX_RETRIES = int(BATCH_TRANSFORM_JOB_MAX_RETRIES)
        BATCH_TRANSFORM_JOB_TIMEOUT = int(BATCH_TRANSFORM_JOB_TIMEOUT)

        S3_CLIENT = boto3.client("s3")
        SAGEMAKER = boto3.client(service_name='sagemaker')

        SCRATCH_DIR = "scratch"

        CLUSTER_CONFIG_EXISTS_IN_REQUEST = False

        TEMP_INPUT_FILENAME = "/tmp/input.gz"
        TEMP_INTERMEDIATE_QUEUE_FILENAME = "/tmp/queue.json"

        TEMP_CLUSTER_CONFIG_INPUT_FILENAME = "/tmp/cluster_config.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global TEMP_INPUT_FILENAME
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            func = "lambda_handler"

            CLUSTER_CONFIG_EXISTS_IN_REQUEST = False

            bucket = event["bucket"]
            try:
                input_filename = f"{SCRATCH_DIR}/output/embedding/{event['input_first_unique_id']}/input.gz"

                clear_temp_dir()

                print(f"{func}: Download input file from S3 bucket: {bucket}/{input_filename}")

                try:
                    S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                    print(f"{func}: Input file download completed")
                except Exception as e:
                    print(f"{func}: Failed to download input file")
                    raise e

                unique_id = input_filename.split("/")[-2]

                print(f"{func}: Download intermediate files from S3")
                try:
                    download_intermediate_input_files(bucket, unique_id)

                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                if check_if_skip_creating_tranform_job(bucket, unique_id):
                    return

                print(f"{func}: Start batch transform job for aggregate and cluster and alerts")
                batch_transform_job_creation_response = aggregate_and_cluster_alerts(bucket, unique_id)
                print(f"{func}: Created batch transform job for aggregate and cluster alerts: {batch_transform_job_creation_response['TransformJobArn']}")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            lambda_response = {"TransformJobName": f'transform-job-cluster-{unique_id}'}
            return lambda_response


        def setup_batch_transform_job_for_aggregate_and_cluster_alerts(bucket, unique_id, input_filename):
            '''
            Configure batch transform job for Temporal clustering
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
                input_filename: S3 input filename to batch tranform job
            Returns:
                batch_transform_job: Configuration to create batch transform job
            '''
            transform_job_name = f'transform-job-cluster-{unique_id}'
            if BATCH_TRANSFORM_JOB_SUFFIX:
                transform_job_name += f"-{BATCH_TRANSFORM_JOB_SUFFIX}"

            transform_input = {
                "DataSource": {
                    "S3DataSource": {
                        "S3DataType": "S3Prefix",
                        "S3Uri": f"s3://{bucket}/{input_filename}"
                    }
                },
                "ContentType": "application/zip",
                "SplitType": "None"
            }
            transform_output = {
                "S3OutputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/cluster_out/{unique_id}/",
                "KmsKeyId": ""
            }
            transform_resources = {
                "InstanceType": BATCH_TRANSFORM_JOB_INSTANCE_TYPE,
                "InstanceCount": 1
            }
            model_client_config = {
                'InvocationsTimeoutInSeconds': BATCH_TRANSFORM_JOB_TIMEOUT,
                'InvocationsMaxRetries': BATCH_TRANSFORM_JOB_MAX_RETRIES
            }

            batch_transform_job = {
                'TransformJobName': transform_job_name,
                'ModelName': CLUSTER_MODEL_NAME,
                'TransformInput': transform_input,
                'TransformOutput': transform_output,
                'MaxPayloadInMB': 0,
                'MaxConcurrentTransforms': 1,
                "TransformResources": transform_resources,
                "ModelClientConfig": model_client_config
            }
            return batch_transform_job


        def check_if_skip_creating_tranform_job(bucket, unique_id):
            '''
            Check if batch transform job should be created for the unique id based on queue
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of the input
            Returns:
                boolean: True, if the transform job creation should be skipped. False, otherwise
            '''

            global TEMP_INPUT_FILENAME
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_INTERMEDIATE_QUEUE_FILENAME

            queue_json = json.load(open(TEMP_INTERMEDIATE_QUEUE_FILENAME, "r"))
            print(f"Contents of the queue for creating transform job: {queue_json}")

            if queue_json["previous"] is None:
                print(f"No previous unique id found. Create batch transform job for {unique_id}")
                return False

            previous_unique_id = queue_json["previous"]
            print(f"Previous unique id found: {previous_unique_id}. Check if the cluster detection is completed.")

            try:
                path_to_cluster_output_of_previous = f"{SCRATCH_DIR}/response/cluster_out/{previous_unique_id}/input.zip.out"
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_cluster_output_of_previous)
                print(f"Will create batch transform job for {unique_id}")
                return False
            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e
                print(f"Clustering did not complete for {previous_unique_id}. Skip create batch transform job for {unique_id}")
                return True


        def aggregate_and_cluster_alerts(bucket, unique_id):
            '''
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of the input
            Returns:
                response: Response of sagemaker client batch transform job creation
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global SAGEMAKER
            global TEMP_INPUT_FILENAME
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST

            func = "aggregate_and_cluster_alerts"

            temp_previous_cluster_output_zip = "/tmp/previous_output.zip"
            temp_enriched_alerts_with_techniques_output_filename = "input.gz"
            temp_cluster_config_filename = "cluster_config.json"

            queue_json = json.load(open(TEMP_INTERMEDIATE_QUEUE_FILENAME, "r"))

            try:
                if queue_json["previous"] is not None:
                    previous_unique_id = queue_json["previous"]
                    path_to_cluster_output_of_previous = f"{SCRATCH_DIR}/response/cluster_out/{previous_unique_id}/input.zip.out"
                    S3_CLIENT.download_file(bucket, path_to_cluster_output_of_previous, temp_previous_cluster_output_zip)
            except Exception as e:
                print(f"{func}: Unable to download file from S3: {bucket}/{path_to_cluster_output_of_previous}")
                raise e

            try:
                with zipfile.ZipFile(temp_previous_cluster_output_zip, "a") as z:
                    z.write(TEMP_INPUT_FILENAME, arcname=temp_enriched_alerts_with_techniques_output_filename)
                    if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                        z.write(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, arcname=temp_cluster_config_filename)
            except Exception as e:
                print(f"{func}: Unable to write input file to zip while creating the zip file")
                raise e

            path_to_embedding_output = f"{SCRATCH_DIR}/output/embedding/{unique_id}/input.zip"

            try:
                print(f"{func}: Save zip input to flow detection: {bucket}/{path_to_embedding_output}")
                S3_CLIENT.upload_file(temp_previous_cluster_output_zip, bucket, path_to_embedding_output)
            except Exception as e:
                print(f"{func}: Failed to upload zip file to S3: {bucket}/{path_to_embedding_output}")
                raise e

            try:
                batch_transform_job = setup_batch_transform_job_for_aggregate_and_cluster_alerts(bucket, unique_id, path_to_embedding_output)
                response = SAGEMAKER.create_transform_job(**batch_transform_job)
            except Exception as e:
                print(f"{func}: Failed to create batch transform job. Unique id: {unique_id}")
                raise e

            return response


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download chunk input file, queue, config if exists, and node features if exists
            Args:
                bucket: Bucket name
                path_to_intermediate_input_before_technique_classification: S3 path to chunk input file before technique enrichment
                path_to_intermediate_queue: S3 path to queue for chunk
                path_to_config_from_input: S3 path to config
                path_to_node_feature_from_input: S3 path to node_feature
            '''
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME

            func = "download_intermediate_input_files"

            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            path_to_cluster_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_config.json"

            try:
                S3_CLIENT.download_file(bucket, path_to_intermediate_queue, TEMP_INTERMEDIATE_QUEUE_FILENAME)
                print(f"{func}: Queue file download completed")
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{path_to_intermediate_queue}")
                raise e

            try:
                print(f"{func}: Checking if cluster config file exists on S3 path {bucket}/{path_to_cluster_config_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_cluster_config_from_input)
                CLUSTER_CONFIG_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Cluster config file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_cluster_config_from_input, TEMP_CLUSTER_CONFIG_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_cluster_config_from_input}")
                    raise e


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          batch_transform_job_instance_type: !Ref ClusterBatchInstanceType
          batch_transform_job_max_retries: '1'
          batch_transform_job_timeout: '3600'
          cluster_model_name: !Ref ClusterModelName
  createClusterLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${createCluster}
  createClusterLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref createCluster
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  processCluster:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_process_cluster
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: processCluster
      InlineCode: |
        '''
        Create input for next batch if in queue. Extract internal aggregated alerts, cluster json. Create input for flow model.
        Input: response from the clustering model
        '''

        import json
        import os
        import urllib
        import zipfile
        import glob
        import sqlite3
        from contextlib import closing

        import pandas as pd
        import boto3
        from botocore.exceptions import ClientError

        pd.options.mode.chained_assignment = None

        CLUSTER_MODEL_NAME = os.getenv("cluster_model_name")

        BATCH_TRANSFORM_JOB_INSTANCE_TYPE = os.getenv("batch_transform_job_instance_type")

        BATCH_TRANSFORM_JOB_MAX_RETRIES = os.getenv("batch_transform_job_max_retries")

        BATCH_TRANSFORM_JOB_TIMEOUT = os.getenv("batch_transform_job_timeout")

        FLOW_INPUT_WINDOW_SIZE = os.getenv("flow_input_window_size")

        missing_variables = []

        if CLUSTER_MODEL_NAME is None:
            missing_variables.append("cluster_model_name")
        if BATCH_TRANSFORM_JOB_INSTANCE_TYPE is None:
            missing_variables.append("batch_transform_job_instance_type")
        if BATCH_TRANSFORM_JOB_MAX_RETRIES is None:
            missing_variables.append("batch_transform_job_max_retries")
        if BATCH_TRANSFORM_JOB_TIMEOUT is None:
            missing_variables.append("batch_transform_job_timeout")
        if FLOW_INPUT_WINDOW_SIZE is None:
            missing_variables.append("flow_input_window_size")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        BATCH_TRANSFORM_JOB_MAX_RETRIES = int(BATCH_TRANSFORM_JOB_MAX_RETRIES)
        BATCH_TRANSFORM_JOB_TIMEOUT = int(BATCH_TRANSFORM_JOB_TIMEOUT)
        FLOW_INPUT_WINDOW_SIZE = int(FLOW_INPUT_WINDOW_SIZE)

        S3_CLIENT = boto3.client("s3")
        SAGEMAKER = boto3.client(service_name='sagemaker')

        SCRATCH_DIR = "scratch"

        PATH_TO_SQLITE = f"{SCRATCH_DIR}/cypienta.db"
        TEMP_SQLITE_FILENAME = "/tmp/cypienta.db"

        TEMP_INTERMEDIATE_QUEUE_FILENAME = "/tmp/queue.json"

        TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME = "/tmp/aggregate_model_output.zip"

        TEMP_AGGREGATED_ALERTS_JSON_FILENAME = "/tmp/aggregated_alerts_output.json"
        TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME = "/tmp/cluster_output.json"

        TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME = "/tmp/input_cluster_id_list.json"

        TEMP_INPUT_TO_SEQUENCE_ALERTS_JSON_FILENAME = "/tmp/input_sequence.json"

        TEMP_CONFIG_INPUT_FILENAME = "/tmp/config.json"
        CONFIG_EXISTS_IN_REQUEST = False

        TEMP_CLUSTER_CONFIG_INPUT_FILENAME = "/tmp/cluster_config.json"
        CLUSTER_CONFIG_EXISTS_IN_REQUEST = False

        INTERNAL_ID_TO_USER_ID_OBJECT_KEY = f"{SCRATCH_DIR}/internal_id_to_user_id.json"
        TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE = "/tmp/internal_id_to_user_id.json"

        TICKET_ID_OBJECT_KEY = f"{SCRATCH_DIR}/ticket_id.json"
        TEMP_TICKET_ID_LOOKUP_FILE = "/tmp/ticket_id.json"

        TEMP_INPUT_CLUSTER_TICKET_OUTPUT_FILENAME = "/tmp/input_cluster_ticket_output.json"

        TEMP_GLOBAL_METRICS_FILENAME = "/tmp/global_metrics.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME
            global TEMP_SQLITE_FILENAME
            global PATH_TO_SQLITE
            global CONFIG_EXISTS_IN_REQUEST
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST

            func = "lambda_handler"

            CONFIG_EXISTS_IN_REQUEST = False
            CLUSTER_CONFIG_EXISTS_IN_REQUEST = False

            bucket, aggregate_and_cluster_alerts_output = get_output_object_key(event)

            # bucket = event['Records'][0]['s3']['bucket']['name']
            # aggregate_and_cluster_alerts_output = event['Records'][0]['s3']['object']['key']

            try:

                clear_temp_dir()

                try:
                    print(f"{func}: input file: {bucket}/{aggregate_and_cluster_alerts_output}")
                    S3_CLIENT.download_file(bucket, aggregate_and_cluster_alerts_output, TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME)
                    print(f"{func}: Downloaded input file")
                except Exception as e:
                    print(f"{func}: Failed to download input file")
                    raise e

                unique_id = aggregate_and_cluster_alerts_output.split("/")[-2]

                print(f"{func}: Unique ID: {unique_id}")

                input_to_sequence_alerts = f"{SCRATCH_DIR}/output/cluster/{unique_id}/input_flow.json"

                print(f"{func}: Download intermediate files from S3 bucket")
                try:
                    download_intermediate_input_files(bucket, unique_id)

                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                fetch_or_create_ticket_id_lookup_file(bucket)

                print(f"{func}: Starting processing cluster output")
                lambda_response = process_output_of_aggregate_and_cluster_alerts(bucket, unique_id, input_to_sequence_alerts)
                print(f"{func}: Completed processing")

                try:
                    print(f"{func}: Upload sqlite to S3")

                    S3_CLIENT.upload_file(TEMP_SQLITE_FILENAME, bucket, PATH_TO_SQLITE)

                    print(f"{func}: Upload completed")
                except Exception as e:
                    print(f"{func}: Failed to upload sqlite from S3: {bucket}/{PATH_TO_SQLITE}")
                    raise e
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def setup_batch_transform_job_for_aggregate_and_cluster_alerts(bucket, unique_id, input_filename):
            '''
            Configure batch transform job for Temporal clustering model
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
                input_filename: S3 input filename to batch tranform job
            Returns:
                batch_transform_job: Configuration to create batch transform job
            '''
            global BATCH_TRANSFORM_JOB_INSTANCE_TYPE
            global BATCH_TRANSFORM_JOB_TIMEOUT
            global BATCH_TRANSFORM_JOB_MAX_RETRIES
            global CLUSTER_MODEL_NAME
            global SCRATCH_DIR

            transform_job_name = f'transform-job-cluster-{unique_id}'

            transform_input = {
                "DataSource": {
                    "S3DataSource": {
                        "S3DataType": "S3Prefix",
                        "S3Uri": f"s3://{bucket}/{input_filename}"
                    }
                },
                "ContentType": "application/zip",
                "SplitType": "None"
            }
            transform_output = {
                "S3OutputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/cluster_out/{unique_id}/",
                "KmsKeyId": ""
            }
            transform_resources = {
                "InstanceType": BATCH_TRANSFORM_JOB_INSTANCE_TYPE,
                "InstanceCount": 1
            }
            model_client_config = {
                'InvocationsTimeoutInSeconds': BATCH_TRANSFORM_JOB_TIMEOUT,
                'InvocationsMaxRetries': BATCH_TRANSFORM_JOB_MAX_RETRIES
            }

            batch_transform_job = {
                'TransformJobName': transform_job_name,
                'ModelName': CLUSTER_MODEL_NAME,
                'TransformInput': transform_input,
                'TransformOutput': transform_output,
                'MaxPayloadInMB': 0,
                'MaxConcurrentTransforms': 1,
                "TransformResources": transform_resources,
                "ModelClientConfig": model_client_config
            }
            return batch_transform_job


        def process_output_of_aggregate_and_cluster_alerts(bucket, unique_id, input_to_sequence_alerts):
            '''
            Process output of temporal clustering model, extract aggregated alerts, and clusters internally.
            create cluster ticket output for batch, global cluster output
            Create next batch transform job if in queue. else save input file for flow model
            Args:
                bucket: Bucket name
                unique_id: Unique id of current chunk
                input_to_sequence_alerts: S3 path to save input for flow model
            '''

            global S3_CLIENT
            global TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME
            global TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_INPUT_TO_SEQUENCE_ALERTS_JSON_FILENAME
            global TEMP_CONFIG_INPUT_FILENAME
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global CONFIG_EXISTS_IN_REQUEST

            func = "process_output_of_aggregate_and_cluster_alerts"

            print(f"{func}: Extract cluster and alert output from zip")

            with zipfile.ZipFile(TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME, "r") as z:
                with z.open("cluster_output.json", "r") as f:
                    clusters_of_aggregated_alerts = json.load(f)

                with z.open("alert_output.json", "r") as f:
                    aggregated_alerts = json.load(f)

            with open(TEMP_INTERMEDIATE_QUEUE_FILENAME, "r") as f:
                queue_json = json.load(f)

            print(f"{func}: Sort output")

            aggregated_alerts.sort(key=sort_by_alert_id)
            clusters_of_aggregated_alerts.sort(key=sort_by_cluster_id)

            print(f"{func}: Save raw output")

            json.dump(clusters_of_aggregated_alerts, open(TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME, "w"))
            json.dump(aggregated_alerts, open(TEMP_AGGREGATED_ALERTS_JSON_FILENAME, "w"))

            print(f"{func}: Update cluster id in input, get current batch cluster id list")

            batch_cluster_id_list = get_batch_cluster_ids_update_input_cluster_ids(bucket, unique_id)

            # save output from cluster model to S3
            print(f"{func}: save output from cluster model to S3")
            save_aggreate_cluster_json_output(bucket, unique_id)

            # update event sqlite table with current batch output
            print(f"{func}: update event table with current batch output")
            update_event_sql(bucket, unique_id)

            # update cluster output sqlite table with operations and current batch output
            print(f"{func}: update cluster output table with current batch output. get partial metrics dict")
            metrics = update_cluster_output_sql(batch_cluster_id_list)

            # update cluster ticket output sqlite for current batch output
            print(f"{func}: update cluster ticket output table with current batch output. create per per cluster metrics")
            update_cluster_ticket_output_sql(bucket, batch_cluster_id_list, metrics)

            lambda_response = {}

            if queue_json["next"] is None:
                print(f"{func}: Start flow detection as this is last chunk of input")

                print(f"{func}: Get global metrics for all clusters currently in sql")
                get_global_metrics_from_cluster_ticket_output_sql(bucket, unique_id)

                print(f"{func}: create cluster ticket output for input")
                create_input_cluster_ticket_output_sql(bucket, unique_id)

                print(f"{func}: Save input to sequence alerts to: {input_to_sequence_alerts}")

                print(f"{func}: Get last {FLOW_INPUT_WINDOW_SIZE} clusters as input to flow model")
                flow_input = get_last_n_clusters()
                flow_input = [dict(row) for row in flow_input]

                print(f"{func}: Process data for flow model input")
                for row in flow_input:
                    row["cluster_srcips"] = json.loads(row["cluster_srcips"])
                    row["cluster_dstips"] = json.loads(row["cluster_dstips"])
                    row["cluster_ips"] = list(set(row["cluster_srcips"] + row["cluster_dstips"]))
                    row["cluster_techs"] = json.loads(row["cluster_techs"])
                    row["cluster_tacs"] = json.loads(row["cluster_tacs"])
                    row["cluster_tacs"] = [int(t) for t in row["cluster_tacs"]]
                    row["cluster_stages"] = json.loads(row["cluster_stages"])
                    row["cluster_stages"] = [int(t) for t in row["cluster_stages"]]

                print(f"{func}: Save input for flow model")
                with open(TEMP_INPUT_TO_SEQUENCE_ALERTS_JSON_FILENAME, "w") as of:
                    req = {"request_id": "1", "input": flow_input}
                    if CONFIG_EXISTS_IN_REQUEST:
                        print(f"{func}: Flow model config added to request")
                        req["config"] = json.load(open(TEMP_CONFIG_INPUT_FILENAME, "r"))
                    json.dump(req, of)

                try:
                    S3_CLIENT.upload_file(TEMP_INPUT_TO_SEQUENCE_ALERTS_JSON_FILENAME, bucket, input_to_sequence_alerts)
                    print(f"{func}: Input to sequence alerts saved")
                except Exception as e:
                    print(f"{func}: Failed to upload file to S3: {bucket}/{input_to_sequence_alerts}")
                    raise e

                lambda_response["TransformJobName"] = "transform-job-flow"
                lambda_response["bucket"] = bucket
                lambda_response["input_path"] = input_to_sequence_alerts

            else:
                print(f"{func}: Start batch transform job for next aggregate and cluster and alerts")
                batch_transform_job_creation_response = aggregate_and_cluster_alerts(bucket, queue_json["next"])
                if batch_transform_job_creation_response is not None:
                    print(f"{func}: Created batch transform job for next aggregate and cluster alerts: {batch_transform_job_creation_response['TransformJobArn']}")
                    lambda_response["TransformJobName"] = f'transform-job-cluster-{queue_json["next"]}'
                else:
                    print(f"{func}: Skip creating next cluster batch transform job")

            return lambda_response


        def aggregate_and_cluster_alerts(bucket, unique_id):
            '''
            Create input and start next batch transform job from the queue
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk used for next batch transform job
            Returns:
                response: Response from sagemaker client create transform job
            '''

            global SCRATCH_DIR
            global S3_CLIENT
            global SAGEMAKER
            global TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME

            func = "aggregate_and_cluster_alerts"

            next_input_filename = f"{SCRATCH_DIR}/output/embedding/{unique_id}/input.gz"

            temp_input_dir = "/tmp/input/"
            os.makedirs(temp_input_dir, exist_ok=True)

            temp_next_input_filename = f"{temp_input_dir}input.gz"

            print(f"{func}: Download next input file: {bucket}/{next_input_filename}")

            try:
                S3_CLIENT.download_file(bucket, next_input_filename, temp_next_input_filename)
                print(f"{func}: Next input file download completed")
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{next_input_filename}")
                raise e

            print(f"{func}: Create next zip for clustering")

            temp_enriched_alerts_with_techniques_output_filename = "input.gz"
            temp_cluster_config_filename = "cluster_config.json"

            with zipfile.ZipFile(TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME, "a") as z:
                z.write(temp_next_input_filename, arcname=temp_enriched_alerts_with_techniques_output_filename)
                if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                    z.write(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, arcname=temp_cluster_config_filename)
                print(f"{func}: Next zip input contents: {z.namelist()}")

            path_to_embedding_output = f"{SCRATCH_DIR}/output/embedding/{unique_id}/input.zip"

            try:
                print(f"{func}: Save next zip input to clustering: {bucket}/{path_to_embedding_output}")
                S3_CLIENT.upload_file(TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME, bucket, path_to_embedding_output)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_embedding_output}")
                raise e

            try:
                batch_transform_job = setup_batch_transform_job_for_aggregate_and_cluster_alerts(bucket, unique_id, path_to_embedding_output)
                response = SAGEMAKER.create_transform_job(**batch_transform_job)
            except Exception as e:
                print(f"{func}: Failed to create batch transform job for temporal clustering. Unique id: {unique_id}")
                raise e

            return response


        def get_batch_cluster_ids_update_input_cluster_ids(bucket, unique_id):
            '''
            Get cluster ids in current batch and update cluster ids in input
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk
            Returns:
                list of cluster ids in current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME

            func = "get_batch_cluster_ids_update_input_cluster_ids"

            queue_json = json.load(open(TEMP_INTERMEDIATE_QUEUE_FILENAME, "r"))

            # if its first batch of current input. initialize cluster id list. else download from previous batch
            if queue_json["first"] == unique_id:
                print(f"{func}: This is first batch of current input. Initialize cluster id list for input")
                json.dump([], open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "w"))
            else:
                path_to_prev_cluster_id_list = f"{SCRATCH_DIR}/intermediate/{queue_json['previous']}/input_cluster_id_list.json"
                try:
                    S3_CLIENT.download_file(bucket, path_to_prev_cluster_id_list, TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME)
                    print(f"{func}: Download file from S3: {bucket}/{path_to_prev_cluster_id_list}")
                except Exception as e:
                    print(f"{func}: Failed to donwnload file from S3: {bucket}/{path_to_prev_cluster_id_list}")
                    raise e

            print(f"{func}: Update cluster id list for input")
            cluster_id_list_json = json.load(open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "r"))
            cluster_id_list_set = set(cluster_id_list_json)

            batch_cluster_df = pd.read_json(TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME)
            batch_cluster_id_list = batch_cluster_df["cluster_id"].to_list()
            batch_cluster_id_list.sort()

            cluster_id_list_list = list(cluster_id_list_set.union(batch_cluster_id_list))

            json.dump(cluster_id_list_list, open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "w"))

            path_to_cluster_id_list = f"{SCRATCH_DIR}/intermediate/{unique_id}/input_cluster_id_list.json"
            print(f"{func}: Save file to S3: {bucket}/{path_to_cluster_id_list}")

            try:
                S3_CLIENT.upload_file(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, bucket, path_to_cluster_id_list)
                print(f"{func}: Uploaded file to S3: {bucket}/{path_to_cluster_id_list}")
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_cluster_id_list}")
                raise e

            return batch_cluster_id_list


        def save_aggreate_cluster_json_output(bucket, unique_id):
            '''
            Save aggregated alerts and current batch cluster output to S3
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk
            '''

            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_AGGREGATED_ALERTS_JSON_FILENAME

            func = "save_aggreate_cluster_json_output"
            aggregate_and_cluster_alerts_json_output_prefix = f"{SCRATCH_DIR}/intermediate/{unique_id}/"

            cluster_of_aggregated_alerts_json_filename = f"{aggregate_and_cluster_alerts_json_output_prefix}cluster_output.json"
            aggregated_alerts_json_filename = f"{aggregate_and_cluster_alerts_json_output_prefix}aggregated_alerts_output.json"

            print(f"{func}: Save aggregated alerts and clusters json output: {aggregate_and_cluster_alerts_json_output_prefix}")
            try:
                S3_CLIENT.upload_file(TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME, bucket,
                                      cluster_of_aggregated_alerts_json_filename)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{cluster_of_aggregated_alerts_json_filename}")
                raise e

            try:
                S3_CLIENT.upload_file(TEMP_AGGREGATED_ALERTS_JSON_FILENAME, bucket, aggregated_alerts_json_filename)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{aggregated_alerts_json_filename}")
                raise e


        def update_event_sql(bucket, unique_id):
            '''
            Update event table in sqlite from current batch cluster output.
            Save current batch cluster output with current batch user given alert ids associated to it
            Args:
                bucket: Bucket name
                unique_id: Unique id of the current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_SQLITE_FILENAME
            global TEMP_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE

            func = "update_event_sql"

            print(f"{func}: Read aggregate alert output, cluster output from current batch")
            agg_df = pd.read_json(TEMP_AGGREGATED_ALERTS_JSON_FILENAME)
            cluster_output_df = pd.read_json(TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME)

            internal_id_to_user_id_json = json.load(open(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, "r"))

            # get all user alert ids for cluster
            print(f"{func}: Get user alert ids in aggregate alert output")
            internal_id_exploded_agg_df = agg_df.explode("ids")
            internal_id_exploded_agg_df = internal_id_exploded_agg_df.apply(lambda row: get_user_ids(row,
                                                                                                     internal_id_to_user_id_json),
                                                                            axis=1)

            # get all original alerts
            print(f"{func}: Get original input alerts for current batch")
            chunk_input_file = f"{SCRATCH_DIR}/intermediate/{unique_id}/original_input.json"

            temp_chunk_input_file = "/tmp/original_input.json"

            try:
                S3_CLIENT.download_file(bucket, chunk_input_file, temp_chunk_input_file)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{chunk_input_file}")
                raise e

            original_alerts_df = pd.read_json(temp_chunk_input_file)
            if "other_attributes_dict" not in original_alerts_df:
                original_alerts_df["other_attributes_dict"] = [{}] * len(original_alerts_df)
            else:
                original_alerts_df["other_attributes_dict"] = original_alerts_df["other_attributes_dict"].apply(lambda x: {} if pd.isna(x) else x)
            if "tech" in original_alerts_df:
                original_alerts_df.drop(columns=["tech"], inplace=True)

            original_alert_keep_cols = original_alerts_df.columns.to_list()
            original_alert_keep_cols.extend(["tech", "tac", "stage"])

            print(f"{func}: Original alert keep columns: {original_alert_keep_cols}")

            # filter aggregate alert df with alerts for current batch only
            print(f"{func}: Filter aggregated alert with alerts from current batch")
            internal_id_exploded_agg_df = internal_id_exploded_agg_df[internal_id_exploded_agg_df["user_id"].isin(original_alerts_df["id"])]

            to_merge_internal_id_exploded_agg_df = internal_id_exploded_agg_df[["tech", "tac", "stage", "user_id"]]

            print(f"{func}: Length of original alerts before merge: {len(original_alerts_df)}")
            # enrich original alerts with tech, tac, stage
            print(f"{func}: Enrich original alerts with tech, tac, stage")
            original_alerts_df = original_alerts_df.merge(to_merge_internal_id_exploded_agg_df,
                                                          how="left",
                                                          left_on="id",
                                                          right_on="user_id")
            print(f"{func}: Original alert columns after enriching: {original_alerts_df.columns.to_list()}")
            print(f"{func}: Length of original alerts after merge: {len(original_alerts_df)}")
            original_alerts_df = original_alerts_df[original_alert_keep_cols]

            cluster_aggalertids_exploded_cluster_output_df = cluster_output_df.explode("cluster_aggalertids")

            # filter aggregate ids in cluster output for only current batch aggregated alerts
            print(f"{func}: Filter cluster output with only current batch alerts")
            cluster_aggalertids_exploded_cluster_output_df = cluster_aggalertids_exploded_cluster_output_df[
                cluster_aggalertids_exploded_cluster_output_df["cluster_aggalertids"].isin(internal_id_exploded_agg_df["aggalert_id"])]

            to_merge_internal_id_exploded_agg_df = internal_id_exploded_agg_df[["aggalert_id", "ids"]]

            # get internal ids in batch cluster output
            print(f"{func}: Get internal ids in current batch cluster output")
            merge_cluster_output_agg_output = cluster_aggalertids_exploded_cluster_output_df.merge(to_merge_internal_id_exploded_agg_df,
                                                                                                   left_on="cluster_aggalertids",
                                                                                                   right_on="aggalert_id",
                                                                                                   how="left")

            internal_id_cluster_output = merge_cluster_output_agg_output.groupby("cluster_id")["ids"].apply(list).reset_index()

            add_internal_id_to_cluster_output_df = internal_id_cluster_output.merge(cluster_output_df, how="inner")

            print(f"{func}: Columns for add_internal_id_to_cluster_output_df: {add_internal_id_to_cluster_output_df.columns.to_list()}")

            # get involved event ids in cluster_output
            print(f"{func}: Get user alert ids in cluster output for current batch")
            add_internal_id_to_cluster_output_df["ids"] = \
                add_internal_id_to_cluster_output_df["ids"].apply(lambda ids: [internal_id_to_user_id_json[id] for id in ids])

            add_internal_id_to_cluster_output_df.to_json(TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME, orient="records")

            insert_event = """
            INSERT INTO event(
                alert_id,
                src,
                dst,
                time,
                name,
                tech,
                tac,
                stage,
                other_attributes
            ) VALUES(
                :alert_id,
                :src,
                :dst,
                :time,
                :name,
                :tech,
                :tac,
                :stage,
                :other_attributes
            );
            """

            insert_global_feature = """
            INSERT OR IGNORE INTO global_feature(
                feature,
                feature_type,
                added_to_ui
            ) VALUES(
                :feature,
                :feature_type,
                :added_to_ui
            );
            """

            insert_or_ignore_cluster = """
            INSERT OR IGNORE INTO cluster_output(
                cluster_id
            )
            VALUES(
                :cluster_id
            )
            ;
            """

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:

                        insert_event_params = []
                        unique_other_attributes_key_set = set()
                        for _, row_original_alerts in original_alerts_df.iterrows():
                            # Execute a query
                            params = {
                                "alert_id": row_original_alerts['id'],
                                "src": row_original_alerts['src'],
                                "dst": row_original_alerts['dst'],
                                "time": row_original_alerts['time'],
                                "name": row_original_alerts['name'],
                                "tech": json.dumps(row_original_alerts['tech']),
                                "tac": json.dumps(list(map(str, row_original_alerts['tac']))),
                                "stage": json.dumps(list(map(str, row_original_alerts['stage']))),
                                "other_attributes": json.dumps(row_original_alerts['other_attributes_dict'])
                            }
                            insert_event_params.append(params)

                            unique_other_attributes_key_set = unique_other_attributes_key_set.union(list(row_original_alerts['other_attributes_dict'].keys()))

                        insert_global_feature_params = []
                        for other_attrs_key in unique_other_attributes_key_set:

                            global_feature_params = {
                                "feature": other_attrs_key,
                                "feature_type": "event",
                                "added_to_ui": 0
                            }
                            insert_global_feature_params.append(global_feature_params)

                        print(f"{func}: Add current batch alert to event table")
                        cursor.executemany(insert_event, insert_event_params)

                        print(f"{func}: Add event features to table")
                        cursor.executemany(insert_global_feature, insert_global_feature_params)

                        insert_or_ignore_cluster_params = []
                        update_cluster_id_in_event_queries = []
                        for _, row_cluster_output in add_internal_id_to_cluster_output_df.iterrows():
                            params = {
                                "cluster_id": row_cluster_output["cluster_id"]
                            }
                            insert_or_ignore_cluster_params.append(params)

                            user_alert_id_list = "','".join(map(str, row_cluster_output['ids']))
                            user_alert_id_list = f"('{user_alert_id_list}')"
                            update_event_query = f"""
                            UPDATE event
                            SET cluster_id = {row_cluster_output["cluster_id"]}
                            WHERE alert_id IN {user_alert_id_list}
                            ;
                            """
                            update_cluster_id_in_event_queries.append(update_event_query)

                        # insert or ignore cluster output
                        print(f"{func}: Add or ignore current batch cluster to cluster output table")
                        cursor.executemany(insert_or_ignore_cluster, insert_or_ignore_cluster_params)

                        # update cluster id for alerts
                        print(f"{func}: Update cluster id for event")
                        for query in update_cluster_id_in_event_queries:
                            cursor.execute(query)

                    conn.commit()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_cluster_output_sql(batch_cluster_id_list):
            '''
            Update cluster output table in sqlite
            Args:
                batch_cluster_id_list: cluster id in current batch
            Returns:
                metrics dictionary for cluster id (contains count of unique tech, tac, stage per cluster)
            '''
            global TEMP_SQLITE_FILENAME

            func = "update_cluster_output_sql"

            update_cluster_query = """
            UPDATE cluster_output
            SET cluster_starttime = :cluster_starttime,
                cluster_endtime = :cluster_endtime,
                cluster_srcips = :cluster_srcips,
                cluster_dstips = :cluster_dstips,
                cluster_techs = :cluster_techs,
                cluster_tacs = :cluster_tacs,
                cluster_stages = :cluster_stages
            WHERE cluster_id = :cluster_id
            ;
            """

            metrics = {}

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:

                        print(f"{func}: Update cluster details in cluster output table")
                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"

                        select_events_filter_cluster_query = f"""
                        SELECT
                            cluster_id,
                            MIN(time) AS min_time,
                            MAX(time) AS max_time,
                            GROUP_CONCAT(DISTINCT src) AS unique_src,
                            GROUP_CONCAT(DISTINCT dst) AS unique_dst,
                            GROUP_CONCAT(tech,';') AS combined_tech,
                            GROUP_CONCAT(tac,';') AS combined_tac,
                            GROUP_CONCAT(stage,';') AS combined_stage
                        FROM
                            event
                        WHERE cluster_id IN {cluster_id_list}
                        GROUP BY cluster_id
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_events_filter_cluster_query)
                        result = cursor.fetchall()

                        update_cluster_output_params = []
                        for row in result:
                            row = dict(row)
                            src_ips = row['unique_src']
                            src_ips = src_ips.split(",")

                            dst_ips = row['unique_dst']
                            dst_ips = dst_ips.split(",")

                            combined_tech = row['combined_tech']
                            combined_tech = combined_tech.split(";")
                            tech_list = []
                            for techs in combined_tech:
                                tech_list += json.loads(techs)
                            tech_list = list(set(tech_list))

                            combined_tac = row['combined_tac']
                            combined_tac = combined_tac.split(";")
                            tac_list = []
                            for tacs in combined_tac:
                                tac_list += json.loads(tacs)
                            tac_list = list(set(tac_list))

                            combined_stage = row['combined_stage']
                            combined_stage = combined_stage.split(";")
                            stage_list = []
                            for stage in combined_stage:
                                stage_list += json.loads(stage)
                            stage_list = list(set(stage_list))

                            metrics[row['cluster_id']] = {
                                "tech": {"count": len(tech_list)},
                                "tac": {"count": len(tac_list)},
                                "stage": {"count": len(stage_list)}
                            }

                            # if cluster id is present in table. Update fields for existing row.
                            params = {
                                "cluster_starttime": row['min_time'],
                                "cluster_endtime": row['max_time'],
                                "cluster_srcips": json.dumps(src_ips),
                                "cluster_dstips": json.dumps(dst_ips),
                                "cluster_techs": json.dumps(tech_list),
                                "cluster_tacs": json.dumps(tac_list),
                                "cluster_stages": json.dumps(stage_list),
                                "cluster_id": row['cluster_id']
                            }

                            update_cluster_output_params.append(params)

                        cursor.executemany(update_cluster_query, update_cluster_output_params)

                    conn.commit()
                return metrics
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_cluster_ticket_output_sql(bucket, batch_cluster_id_list, metrics):
            '''
            Update cluster ticket output table and add per cluster metrics
            Args:
                bucket: Bucket name
                batch_cluster_id_list: cluster id in current batch
                metrics: dict of metric for cluster id (contains count of unique tech, tac, stage per cluster)
            '''
            global TEMP_SQLITE_FILENAME
            global TEMP_TICKET_ID_LOOKUP_FILE

            func = "update_cluster_ticket_output_sql"

            metrics_cols = ["tech", "tac", "stage"]

            ticket_id_json = json.load(open(TEMP_TICKET_ID_LOOKUP_FILE, "r"))
            ticket_id_start = ticket_id_json["last_ticket_id"]

            insert_cluster_ticket_output_query = """
            INSERT INTO cluster_ticket_output (
                cluster_id,
                ticket_id,
                metrics
            )
            VALUES (
                :cluster_id,
                :ticket_id,
                :metrics
            )
            ;
            """

            update_cluster_ticket_output_query = """
            UPDATE cluster_ticket_output
            SET 
                metrics = :metrics
            WHERE cluster_id = :cluster_id
            ;
            """

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:
                        print(f"{func}: Get events for clusters in current batch")
                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"
                        select_event_query = f"""
                        SELECT *
                        FROM event
                        WHERE cluster_id IN {cluster_id_list}
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_event_query)
                        results = cursor.fetchall()
                        results = [dict(row) for row in results]
                        events_df = pd.DataFrame(results)

                        print(f"{func}: Get existing cluster ticket output for clusters in current batch")
                        select_cluster_ticket_output_query = f"""
                        SELECT *
                        FROM cluster_ticket_output
                        WHERE cluster_id IN {cluster_id_list}
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_cluster_ticket_output_query)
                        results = cursor.fetchall()
                        results = [dict(row) for row in results]
                        cluster_ticket_output_df = pd.DataFrame(results)

                        cluster_ticket_dict = {}
                        if len(cluster_ticket_output_df):
                            cluster_ticket_dict = cluster_ticket_output_df.set_index('cluster_id')['ticket_id'].to_dict()

                        print(f"{func}: Get per cluster metrics and update cluster ticket output")
                        insert_cluster_ticket_output_params = []
                        update_cluster_ticket_output_params = []
                        for cluster_id in batch_cluster_id_list:
                            filtered_events = events_df[events_df["cluster_id"] == cluster_id]
                            cluster_metrics = per_cluster_metrics_events(filtered_events)

                            for metric_col in metrics_cols:
                                cluster_metrics[metric_col]["count"] = float(metrics[cluster_id][metric_col]["count"])

                            if cluster_id not in cluster_ticket_dict:
                                ticket_id_start += 1
                                params = {
                                    "cluster_id": cluster_id,
                                    "ticket_id": ticket_id_start,
                                    "metrics": json.dumps(cluster_metrics)
                                }
                                insert_cluster_ticket_output_params.append(params)
                            else:
                                params = {
                                    "cluster_id": cluster_id,
                                    "metrics": json.dumps(cluster_metrics)
                                }
                                update_cluster_ticket_output_params.append(params)
                        if insert_cluster_ticket_output_params:
                            cursor.executemany(insert_cluster_ticket_output_query, insert_cluster_ticket_output_params)
                        if update_cluster_ticket_output_params:
                            cursor.executemany(update_cluster_ticket_output_query, update_cluster_ticket_output_params)

                    conn.commit()

                print(f"{func}: Save ticket id lookup file")
                ticket_id_json["last_ticket_id"] = ticket_id_start
                json.dump(ticket_id_json, open(TEMP_TICKET_ID_LOOKUP_FILE, "w"))
                save_ticket_id_lookup_file(bucket)
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def create_input_cluster_ticket_output_sql(bucket, unique_id):
            '''
            Create cluster ticket output for current input and save to S3
            Args:
                bucket: Bucket name
                unique_id: Unique id of chunk of input
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_SQLITE_FILENAME
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global TEMP_INPUT_CLUSTER_TICKET_OUTPUT_FILENAME

            func = "create_input_cluster_ticket_output"

            input_cluster_id_list = json.load(open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "r"))

            try:
                with open(TEMP_INPUT_CLUSTER_TICKET_OUTPUT_FILENAME, "w") as f:
                    with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                        conn.row_factory = sqlite3.Row
                        conn.execute("PRAGMA foreign_keys = ON;")
                        with closing(conn.cursor()) as cursor:
                            cluster_id_list = ",".join(map(str, input_cluster_id_list))
                            cluster_id_list = f"({cluster_id_list})"
                            select_query = f"""
                            SELECT
                                co.cluster_id,
                                co.cluster_starttime,
                                co.cluster_endtime,
                                co.cluster_srcips,
                                co.cluster_dstips,
                                co.cluster_techs,
                                co.cluster_tacs,
                                co.cluster_stages,
                                cto.ticket_id,
                                cto.metrics,
                                e.alert_id,
                                e.src AS event_src,
                                e.dst AS event_dst,
                                e.time AS event_time,
                                e.name AS event_name,
                                e.tech AS event_tech,
                                e.tac AS event_tac,
                                e.stage AS event_stage,
                                e.other_attributes AS event_other_attributes
                            FROM cluster_ticket_output cto
                            INNER JOIN cluster_output co ON co.cluster_id = cto.cluster_id
                            INNER JOIN event e ON e.cluster_id = cto.cluster_id
                            WHERE cto.cluster_id IN {cluster_id_list}
                            ORDER BY cto.cluster_id
                            ;
                            """

                            cursor.execute(select_query)

                            row = cursor.fetchone()
                            prev_cluster = None

                            involved_events = []

                            while row:
                                row = dict(row)

                                curr_cluster = row
                                if prev_cluster is None:
                                    prev_cluster = curr_cluster

                                if curr_cluster["cluster_id"] != prev_cluster["cluster_id"]:
                                    row_cluster_ticket_output = {
                                        "ticket_id": prev_cluster["ticket_id"],
                                        "cluster_id": prev_cluster["cluster_id"],
                                        "involved_events": involved_events,
                                        "start_time": prev_cluster["cluster_starttime"],
                                        "end_time": prev_cluster["cluster_endtime"],
                                        "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                        "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                        "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                        "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                        "metrics": json.loads(prev_cluster["metrics"]),
                                    }
                                    f.write(json.dumps(row_cluster_ticket_output) + "\n")
                                    involved_events = []

                                events = {
                                    "id": row["alert_id"],
                                    "src": row["event_src"],
                                    "dst": row["event_dst"],
                                    "time": row["event_time"],
                                    "name": row["event_name"],
                                    "tech": json.loads(row["event_tech"]),
                                    "tac": json.loads(row["event_tac"]),
                                    "stage": json.loads(row["event_stage"]),
                                    "other_attributes_dict": json.loads(row["event_other_attributes"]),
                                }
                                involved_events.append(events)

                                prev_cluster = curr_cluster
                                row = cursor.fetchone()

                            row_cluster_ticket_output = {
                                "ticket_id": prev_cluster["ticket_id"],
                                "cluster_id": prev_cluster["cluster_id"],
                                "involved_events": involved_events,
                                "start_time": prev_cluster["cluster_starttime"],
                                "end_time": prev_cluster["cluster_endtime"],
                                "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                "metrics": json.loads(prev_cluster["metrics"]),
                            }
                            f.write(json.dumps(row_cluster_ticket_output) + "\n")

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            try:
                path_to_batch_input_cluster_ticket_output = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_ticket_output.json"
                S3_CLIENT.upload_file(TEMP_INPUT_CLUSTER_TICKET_OUTPUT_FILENAME, bucket, path_to_batch_input_cluster_ticket_output)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_batch_input_cluster_ticket_output}")
                raise e


        def per_cluster_metrics_events(involved_events_df):
            '''
            Get metrics per cluster
            Args:
                involved_events_df: involved events of one cluster
            Returns:
                metrics per cluster
            '''

            metrics_cols = ["tech", "tac", "stage"]

            metrics = {}

            for metric_col in metrics_cols:
                involved_events_df[f'{metric_col}'] = involved_events_df[f'{metric_col}'].apply(json.loads)
                involved_events_df[f'{metric_col}_count'] = involved_events_df[f'{metric_col}'].apply(len)

                metrics[metric_col] = {
                    "avg": float(involved_events_df[f"{metric_col}_count"].mean()),
                    "min": float(involved_events_df[f"{metric_col}_count"].min()),
                    "max": float(involved_events_df[f"{metric_col}_count"].max()),
                    "median": float(involved_events_df[f"{metric_col}_count"].median())
                }

            return metrics


        def get_global_metrics_from_cluster_ticket_output_sql(bucket, unique_id):
            '''
            Get global metrics from an SQLite database.
            Args:
                bucket: Bucket name
                unique_id: Unique id of chunk of input
            Returns:
                metrics dictionary
            '''
            global TEMP_SQLITE_FILENAME
            global TEMP_GLOBAL_METRICS_FILENAME
            global SCRATCH_DIR
            global S3_CLIENT

            func = "get_global_metrics_from_cluster_ticket_output_sql"

            metrics = {}
            metrics_cols = ["tech", "tac", "stage", "event", "entity"]
            mapped_metric_cols = ["cluster_techs", "cluster_tacs", "cluster_stages", "cluster_events", "cluster_entities"]

            # Initialize counters for metrics
            metrics_data = {col: [] for col in metrics_cols}
            time_ranges = []

            event_count = 0

            select_query = """
            SELECT
                co.cluster_id,
                co.cluster_starttime,
                co.cluster_endtime,
                co.cluster_srcips,
                co.cluster_dstips,
                co.cluster_techs,
                co.cluster_tacs,
                co.cluster_stages,
                COUNT(e.cluster_id) AS cluster_events
            FROM
                cluster_output co
            INNER JOIN
                event e ON co.cluster_id = e.cluster_id
            GROUP BY
                co.cluster_id
            ;
            """

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    with closing(conn.cursor()) as cursor:
                        cursor.execute(select_query)

                        row = cursor.fetchone()
                        while row:
                            row = dict(row)
                            record = {
                                "cluster_id": row["cluster_id"],
                                "cluster_starttime": row["cluster_starttime"],
                                "cluster_endtime": row["cluster_endtime"],
                                "cluster_techs": json.loads(row["cluster_techs"]),
                                "cluster_tacs": json.loads(row["cluster_tacs"]),
                                "cluster_stages": json.loads(row["cluster_stages"]),
                                "cluster_events": row["cluster_events"],
                                "cluster_entities": list(set(json.loads(row["cluster_dstips"]) + json.loads(row["cluster_srcips"])))
                            }

                            for metric_col, mapped_metric_col in zip(metrics_cols, mapped_metric_cols):

                                if metric_col == "event":
                                    count = record[mapped_metric_col]
                                    event_count += count
                                    metrics_data[metric_col].append(count)
                                else:
                                    count = float(len(record[mapped_metric_col]))
                                    metrics_data[metric_col].append(count)

                            start_time = pd.to_datetime(record['cluster_starttime'], unit='s')
                            end_time = pd.to_datetime(record['cluster_endtime'], unit='s')
                            time_range = (end_time - start_time).total_seconds()
                            time_ranges.append(time_range)

                            row = cursor.fetchone()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            # Calculate statistics for each metric
            for metric_col in metrics_cols:
                if metric_col not in metrics:
                    metrics[metric_col] = {}

                if metrics_data[metric_col]:
                    metrics[metric_col]["avg"] = float(pd.Series(metrics_data[metric_col]).mean())
                    metrics[metric_col]["min"] = float(pd.Series(metrics_data[metric_col]).min())
                    metrics[metric_col]["max"] = float(pd.Series(metrics_data[metric_col]).max())
                    metrics[metric_col]["median"] = float(pd.Series(metrics_data[metric_col]).median())
                    metrics[metric_col]["q1"] = float(pd.Series(metrics_data[metric_col]).quantile(0.25))
                    metrics[metric_col]["q3"] = float(pd.Series(metrics_data[metric_col]).quantile(0.75))
                    if metric_col == "event":
                        metrics[metric_col]["count"] = event_count

            # Calculate statistics for time range
            if time_ranges:
                metrics["time_range"] = {
                    "avg": float(pd.Series(time_ranges).mean()),
                    "min": float(pd.Series(time_ranges).min()),
                    "max": float(pd.Series(time_ranges).max()),
                    "median": float(pd.Series(time_ranges).median()),
                    "q1": float(pd.Series(time_ranges).quantile(0.25)),
                    "q3": float(pd.Series(time_ranges).quantile(0.75)),
                }

            json.dump(metrics, open(TEMP_GLOBAL_METRICS_FILENAME, "w"))

            try:
                print(f"{func}: Upload global metric output file to S3")
                global_metrics_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/global_metrics.json"
                S3_CLIENT.upload_file(TEMP_GLOBAL_METRICS_FILENAME, bucket, global_metrics_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{global_metrics_filename}")
                raise e

            return metrics


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download intermediate files required
            Args:
                bucket: Bucket name
                unique_id: unique id of current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_CONFIG_INPUT_FILENAME
            global CONFIG_EXISTS_IN_REQUEST
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global INTERNAL_ID_TO_USER_ID_OBJECT_KEY
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE
            global PATH_TO_SQLITE
            global TEMP_SQLITE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME

            func = "download_intermediate_input_files"

            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            path_to_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/config.json"
            path_to_cluster_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_config.json"

            try:
                print(f"{func}: Download queue json file from S3")
                S3_CLIENT.download_file(bucket, path_to_intermediate_queue, TEMP_INTERMEDIATE_QUEUE_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{path_to_intermediate_queue}")
                raise e

            try:
                S3_CLIENT.download_file(bucket, INTERNAL_ID_TO_USER_ID_OBJECT_KEY, TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{INTERNAL_ID_TO_USER_ID_OBJECT_KEY}")
                raise e

            try:
                print(f"{func}: Checking if cluster config file exists on S3 path {bucket}/{path_to_cluster_config_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_cluster_config_from_input)
                CLUSTER_CONFIG_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Cluster config file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_cluster_config_from_input, TEMP_CLUSTER_CONFIG_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_cluster_config_from_input}")
                    raise e

            try:
                print(f"{func}: Checking if config file exists on S3 path {bucket}/{path_to_config_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_config_from_input)
                CONFIG_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e
                print(f"{func}: Config file does not exist")
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_config_from_input, TEMP_CONFIG_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_config_from_input}")
                    raise e

            try:
                print(f"{func}: Download sqlite file from S3")
                S3_CLIENT.download_file(bucket, PATH_TO_SQLITE, TEMP_SQLITE_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{PATH_TO_SQLITE}")
                raise e


        def save_ticket_id_lookup_file(bucket):
            '''
            Save ticket id lookup file to S3
            Args:
                bucket: The S3 bucket name
            '''
            global TICKET_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_TICKET_ID_LOOKUP_FILE
            func = "save_ticket_id_lookup_file"
            try:
                print(f"{func}: Upload ticket id lookup file to S3")
                S3_CLIENT.upload_file(TEMP_TICKET_ID_LOOKUP_FILE, bucket, TICKET_ID_OBJECT_KEY)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{TICKET_ID_OBJECT_KEY}")
                raise e


        def fetch_or_create_ticket_id_lookup_file(bucket):
            '''
            Check if the ticket_id lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global TICKET_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_TICKET_ID_LOOKUP_FILE

            func = "fetch_or_create_ticket_id_lookup_file"

            try:
                print(
                    f"{func}: Checking if ticket id lookup file exists on S3 path {bucket}/{TICKET_ID_OBJECT_KEY}")

                S3_CLIENT.head_object(Bucket=bucket, Key=TICKET_ID_OBJECT_KEY)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Ticket id lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = {"last_ticket_id": 0}
                json.dump(internal_id_to_unique_id_json, open(TEMP_TICKET_ID_LOOKUP_FILE, "w"))

                print(f"{func}: Initiated ticket id")
            else:
                try:
                    print(f"{func}: Download available internal id to unique id lookup file from S3")

                    S3_CLIENT.download_file(bucket, TICKET_ID_OBJECT_KEY, TEMP_TICKET_ID_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{TICKET_ID_OBJECT_KEY}")
                    raise e


        def get_user_ids(row, internal_id_to_user_id_json):
            '''
            Get user alert id for a row
            Args:
                row: A row containing internal id
                internal_id_to_user_id_json: A dictionary mapping user ids to internal ids
            Returns:
                The row with the added unique_id field
            '''

            row["user_id"] = internal_id_to_user_id_json[row["ids"]]
            return row


        def sort_by_alert_id(obj):
            '''
            sort by alert id
            Args:
                obj: aggregate alert dictionary object
            Returns:
                aggregate alert id
            '''
            return obj["aggalert_id"]


        def sort_by_cluster_id(obj):
            '''
            sort by cluster id
            Args:
                obj: cluster dictionary object
            Returns:
                cluster id
            '''
            return obj["cluster_id"]


        def get_last_n_clusters():
            '''
            Get last n clusters from cluster output

            Returns:
                list of cluster output rows
            '''
            global TEMP_SQLITE_FILENAME
            global FLOW_INPUT_WINDOW_SIZE

            func = "get_last_n_clusters"

            select_query = """
            SELECT *
            FROM cluster_output
            ORDER BY cluster_id DESC
            LIMIT :limit
            ;
            """
            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    with closing(conn.cursor()) as cursor:
                        params = {"limit": FLOW_INPUT_WINDOW_SIZE}
                        cursor.execute(select_query, params)

                        results = cursor.fetchall()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            return results


        def get_output_object_key(event):
            '''
            Get output object key from input and output s3 path
            Args:
                event: Input to lambda
            Returns:
                (bucket_name, object_key)
            '''
            func = "get_output_object_key"

            input_bucket, input_key = parse_s3_path(event["inputPath"])

            print(f"{func}: Input bucket: {input_bucket}")
            print(f"{func}: Input key: {input_key}")

            output_bucket, output_key = parse_s3_path(event["outputPath"])

            print(f"{func}: Output bucket: {output_bucket}")
            print(f"{func}: Output key: {output_key}")

            if input_bucket is None or input_key is None or output_bucket is None or output_key is None:
                raise ValueError("Failed to get output path of the batch transform job")

            output_file_name = input_key.split("/")[-1] + ".out"
            output_key += output_file_name

            return output_bucket, output_key


        def parse_s3_path(s3_path):
            '''
            Parse an S3 path and extract the bucket name and object key.
            Args:
                s3_path: The S3 path to parse. Should be in the format: s3://bucket-name/path/to/object
            Returns:
                (bucket_name, object_key).
                (None, None) if the path is invalid.
            '''
            if not s3_path.startswith("s3://"):
                return None, None

            # Remove the "s3://" prefix
            path_without_prefix = s3_path[5:]

            # Split the remaining path into parts
            parts = path_without_prefix.split('/', 1)

            # If there's only one part, it means there's no object key
            if len(parts) == 1:
                return parts[0], None

            # If there are two parts, the first is the bucket name, the second is the object key
            return parts[0], parts[1]


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          batch_transform_job_instance_type: !Ref ClusterBatchInstanceType
          batch_transform_job_max_retries: '1'
          batch_transform_job_timeout: '3600'
          cluster_model_name: !Ref ClusterModelName
          flow_input_window_size: !Ref FlowInputMaxClusters
  processClusterLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${processCluster}
  processClusterLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref processCluster
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  createFlow:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_create_flow
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: createFlow
      InlineCode: |
        '''
        Create batch transform job for Flow detection

        Input: cluster output from Temporal clustering
        '''

        import os
        import urllib
        import glob
        import boto3

        FLOW_MODEL_NAME = os.getenv("flow_model_name")

        BATCH_TRANSFORM_JOB_INSTANCE_TYPE = os.getenv("batch_transform_job_instance_type")

        BATCH_TRANSFORM_JOB_MAX_RETRIES = os.getenv("batch_transform_job_max_retries")

        BATCH_TRANSFORM_JOB_TIMEOUT = os.getenv("batch_transform_job_timeout")

        BATCH_TRANSFORM_JOB_SUFFIX = os.getenv("batch_transform_job_suffix")

        missing_variables = []
        if FLOW_MODEL_NAME is None:
            missing_variables.append("flow_model_name")
        if BATCH_TRANSFORM_JOB_INSTANCE_TYPE is None:
            missing_variables.append("batch_transform_job_instance_type")
        if BATCH_TRANSFORM_JOB_MAX_RETRIES is None:
            missing_variables.append("batch_transform_job_max_retries")
        if BATCH_TRANSFORM_JOB_TIMEOUT is None:
            missing_variables.append("batch_transform_job_timeout")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        BATCH_TRANSFORM_JOB_MAX_RETRIES = int(BATCH_TRANSFORM_JOB_MAX_RETRIES)
        BATCH_TRANSFORM_JOB_TIMEOUT = int(BATCH_TRANSFORM_JOB_TIMEOUT)

        if BATCH_TRANSFORM_JOB_SUFFIX is None:
            BATCH_TRANSFORM_JOB_SUFFIX = ""

        SCRATCH_DIR = "scratch"
        S3_CLIENT = boto3.client("s3")


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            func = "lambda_handler"

            sagemaker = boto3.client(service_name='sagemaker')
            bucket = event["bucket"]

            lambda_response = None

            try:
                input_filename = event["input_path"]

                clear_temp_dir()

                print(f"{func}: Input file from S3 bucket: {bucket}/{input_filename}")

                unique_id = input_filename.split("/")[-2]

                try:

                    print(f"{func}: Start batch transform job for sequence alerts")

                    batch_transform_job = setup_batch_transform_job_for_sequence_alerts(bucket, unique_id, input_filename)
                    batch_transform_job_creation_response = sagemaker.create_transform_job(**batch_transform_job)

                    lambda_response = {"TransformJobName": batch_transform_job["TransformJobName"]}

                    print(f"{func}: Created batch transform job for sequence alerts: {batch_transform_job_creation_response['TransformJobArn']}")

                except Exception as e:
                    print(f"{func}: Failed to created transform job for sequence alerts. Unique id: {unique_id}")
                    raise e
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def setup_batch_transform_job_for_sequence_alerts(bucket, unique_id, input_filename):
            '''
            Configure batch transform job for Flow model
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
                input_filename: S3 input filename to batch tranform job
            Returns:
                batch_transform_job: Configuration to create batch transform job
            '''
            global BATCH_TRANSFORM_JOB_INSTANCE_TYPE
            global BATCH_TRANSFORM_JOB_TIMEOUT
            global BATCH_TRANSFORM_JOB_MAX_RETRIES
            global FLOW_MODEL_NAME
            global SCRATCH_DIR

            transform_job_name = f'transform-job-flow-{unique_id}'
            if BATCH_TRANSFORM_JOB_SUFFIX:
                transform_job_name += f"-{BATCH_TRANSFORM_JOB_SUFFIX}"

            transform_input = {
                "DataSource": {
                    "S3DataSource": {
                        "S3DataType": "S3Prefix",
                        "S3Uri": f"s3://{bucket}/{input_filename}"
                    }
                },
                "ContentType": "text/plain",
                "SplitType": "None"
            }
            transform_output = {
                "S3OutputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/flow_out/{unique_id}/",
                "KmsKeyId": ""
            }
            transform_resources = {
                "InstanceType": BATCH_TRANSFORM_JOB_INSTANCE_TYPE,
                "InstanceCount": 1
            }
            model_client_config = {
                'InvocationsTimeoutInSeconds': BATCH_TRANSFORM_JOB_TIMEOUT,
                'InvocationsMaxRetries': BATCH_TRANSFORM_JOB_MAX_RETRIES
            }

            batch_transform_job = {
                'TransformJobName': transform_job_name,
                'ModelName': FLOW_MODEL_NAME,
                'TransformInput': transform_input,
                'TransformOutput': transform_output,
                'MaxPayloadInMB': 0,
                'MaxConcurrentTransforms': 1,
                "TransformResources": transform_resources,
                "ModelClientConfig": model_client_config
            }
            return batch_transform_job


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          batch_transform_job_instance_type: !Ref FlowBatchInstanceType
          batch_transform_job_max_retries: '1'
          batch_transform_job_timeout: '3600'
          flow_model_name: !Ref FlowModelName
  createFlowLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${createFlow}
  createFlowLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref createFlow
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  processFlow:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_process_flow
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: processFlow
      InlineCode: |
        '''
        Process response from flow detection model. Extract cluster, flow. Create enriched_alerts_input

        Input: Response from Flow detection model
        '''

        import os
        import json
        import urllib
        import glob
        from datetime import datetime, timezone
        import sqlite3
        from contextlib import closing
        import copy

        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        FLOW_INPUT_WINDOW_SIZE = os.getenv("flow_input_window_size")

        CAMPAIGN_MAP = os.getenv("campaign_map")

        SKIP_SINGLE_ALERT = os.getenv("skip_single_alert")

        MAP_CEF_TO_INTERNAL = os.getenv("map_cef_to_internal")

        missing_variables = []
        if FLOW_INPUT_WINDOW_SIZE is None:
            missing_variables.append("flow_input_window_size")
        if CAMPAIGN_MAP is None:
            missing_variables.append("campaign_map")
        if SKIP_SINGLE_ALERT is None:
            SKIP_SINGLE_ALERT = "true"

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")
        if CAMPAIGN_MAP not in ["flow", "cluster"]:
            raise ValueError(f"Valid values for 'campaign_map' environemnt variable are 'flow', 'cluster'. Entered value is: '{CAMPAIGN_MAP}'")
        if SKIP_SINGLE_ALERT not in ["true", "false"]:
            raise ValueError(f"Valid values for 'skip_single_alert' environemnt variable are 'true', 'false'. Entered value is: '{SKIP_SINGLE_ALERT}'")

        FLOW_INPUT_WINDOW_SIZE = int(FLOW_INPUT_WINDOW_SIZE)

        if MAP_CEF_TO_INTERNAL is not None:
            MAP_CEF_TO_INTERNAL = (MAP_CEF_TO_INTERNAL == "true")
        else:
            MAP_CEF_TO_INTERNAL = True

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        NODE_FEATURE_EXISTS_IN_REQUEST = False

        PATH_TO_SQLITE = f"{SCRATCH_DIR}/cypienta.db"
        TEMP_SQLITE_FILENAME = "/tmp/cypienta.db"

        TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME = "/tmp/input_cluster_id_list.json"

        TEMP_USER_CLUSTER_OUTPUT_FILENAME = "/tmp/user_cluster_output.json"

        TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME = "/tmp/aggregate_model_output.json"

        TEMP_FLOW_OUTPUT_JSON_FILENAME = "/tmp/flow_output.json"
        TEMP_USER_FLOW_OUTPUT_JSON_FILENAME = "/tmp/user_flow_output.json"
        TEMP_FLOW_TICKET_OUTPUT_FILENAME = "/tmp/flow_ticket_output.json"

        SPLUNK_USER_CLUSTER_JSON_LINES = f"/tmp/splunk_user_cluster_output.json"
        SPLUNK_USER_FLOW_JSON_LINES = f"/tmp/splunk_user_flow_output.json"

        TICKET_ID_OBJECT_KEY = f"{SCRATCH_DIR}/ticket_id.json"
        TEMP_TICKET_ID_LOOKUP_FILE = "/tmp/ticket_id.json"

        QUEUE_LOOKUP_FILE = f"{SCRATCH_DIR}/queue_lookup.json"
        TEMP_QUEUE_LOOKUP_FILE = "/tmp/queue_lookup.json"

        TEMP_NODE_FEATURE_INPUT_FILENAME = "/tmp/node_feature.json"
        NODE_FEATURE_LOOKUP_FILE = f"{SCRATCH_DIR}/node_feature_lookup.json"
        TEMP_NODE_FEATURE_LOOKUP_FILE = "/tmp/node_feature_lookup.json"

        TEMP_FLOW_GLOBAL_METRICS_FILENAME = "/tmp/flow_global_metrics.json"
        TEMP_GLOBAL_FEATURE_FILENAME = "/tmp/global_feature.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME
            global SCRATCH_DIR
            global PATH_TO_SQLITE
            global TEMP_SQLITE_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            func = "lambda_handler"

            NODE_FEATURE_EXISTS_IN_REQUEST = False

            bucket, sequence_alerts_output = get_output_object_key(event)

            # bucket = event['Records'][0]['s3']['bucket']['name']
            # sequence_alerts_output = event['Records'][0]['s3']['object']['key']

            try:

                clear_temp_dir()

                print(f"{func}: Download flow output from S3 bucket: {bucket}/{sequence_alerts_output}")
                try:
                    S3_CLIENT.download_file(bucket, sequence_alerts_output, TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME)

                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                unique_id = sequence_alerts_output.split("/")[-2]

                print(f"{func}: Unique ID: {unique_id}")

                try:
                    print(f"{func}: Download intermediate files from S3 bucket")
                    download_intermediate_input_files(bucket, unique_id)
                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                fetch_or_create_ticket_id_lookup_file(bucket)

                print(f"{func}: Starting processing flow output")
                output_filename = process_output_of_sequence_alerts(bucket, unique_id)
                print(f"{func}: Completed processing flow output")

                try:
                    print(f"{func}: Upload sqlite to S3")

                    S3_CLIENT.upload_file(TEMP_SQLITE_FILENAME, bucket, PATH_TO_SQLITE)

                    print(f"{func}: Upload completed")
                except Exception as e:
                    print(f"{func}: Failed to upload sqlite from S3: {bucket}/{PATH_TO_SQLITE}")
                    raise e
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            lambda_response = {"bucket": bucket, "input_path": output_filename}
            return lambda_response


        def process_output_of_sequence_alerts(bucket, unique_id):
            '''
            Extract cluster, flow. Create flow ticket output.
            Upload user flow, cluster output, flow ticket output
            Args:
                bucket: Bucket name
                unique_id: Unique id of chunk of input
            '''
            global TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME
            global TEMP_FLOW_OUTPUT_JSON_FILENAME
            global SCRATCH_DIR
            global CAMPAIGN_MAP
            global S3_CLIENT
            global TEMP_QUEUE_LOOKUP_FILE
            global NODE_FEATURE_EXISTS_IN_REQUEST

            func = "process_output_of_sequence_alerts"

            sequence_alerts_output_json = json.load(open(TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME))
            sequence_alerts_output_json = sequence_alerts_output_json[0]["output"]
            flow_output = sequence_alerts_output_json["flow_output"]

            json.dump(flow_output, open(TEMP_FLOW_OUTPUT_JSON_FILENAME, "w"))

            print(f"{func}: Save flow output to scratch intermediate path")

            flow_output_json_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow.json"
            try:
                print(f"{func}: Save aggregated flows json output: {flow_output_json_filename}")
                S3_CLIENT.upload_file(TEMP_FLOW_OUTPUT_JSON_FILENAME, bucket, flow_output_json_filename)
            except Exception as e:
                print(f"{func}: Failed to save sequence output to S3: {bucket}/{flow_output_json_filename}")
                raise e

            if NODE_FEATURE_EXISTS_IN_REQUEST:
                print(f"{func}: Update global feature for nodes in sql for current input")
                update_global_feature_node_sql()

            create_global_feature_for_ui()

            print(f"{func}: Save global feature for UI to S3")
            global_feature_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/global_feature.json"
            try:
                S3_CLIENT.upload_file(TEMP_GLOBAL_FEATURE_FILENAME, bucket, global_feature_filename)
            except Exception as e:
                print(f"{func}: Failed to upload to S3: {bucket}/{global_feature_filename}")
                raise e

            print(f"{func}: Create cluster output for current input")
            create_input_cluster_output()

            if len(flow_output) < 1:
                print(f"{func}: No flows were detected for the input. Skip creating flow ticket output.")
            else:

                print(f"{func}: Update flow in sql. Create flow ticket output")
                update_flow_sql(bucket, unique_id)

            print(f"{func}: Process cluster output to user readable structure. Save cluster, flow output")
            upload_cluster_and_flow_output(bucket, unique_id)
            print(f"{func}: Saved cluster, flow output")

            if len(flow_output) > 0:
                output_filename = f"{SCRATCH_DIR}/output/flow/{unique_id}/flow_ticket_output.json"
                source_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_ticket_output.json"
                copy_source = {
                    'Bucket': bucket,
                    'Key': source_filename
                }

                try:
                    print(f"{func}: Upload flow ticket output file to output for flow in S3")
                    S3_CLIENT.copy(copy_source, bucket, output_filename)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{output_filename}")
                    raise e

            output_filename = f"{SCRATCH_DIR}/output/flow/{unique_id}/cluster_ticket_output.json"
            source_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_ticket_output.json"
            copy_source = {
                'Bucket': bucket,
                'Key': source_filename
            }

            try:
                print(f"{func}: Upload cluster ticket output file to output for flow in S3")
                S3_CLIENT.copy(copy_source, bucket, output_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{output_filename}")
                raise e

            print(f"{func}: Saved ticket output")

            print(f"{func}: Check if next input is present to start processing")
            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            if len(queue_lookup_json["input_queue"]) > 0:
                print(f"{func}: Next input file to process: {queue_lookup_json['input_queue'][0]}")
                try:
                    print(f"{func}: Process next input file")
                    next_input_queue_entry = queue_lookup_json["input_queue"][0]
                    next_input_bucket = next_input_queue_entry.split("/")[0]
                    next_input_key = "/".join(next_input_queue_entry.split("/")[1:])
                    copy_source = {
                        'Bucket': next_input_bucket,
                        'Key': next_input_key
                    }
                    S3_CLIENT.copy_object(CopySource=copy_source, Bucket=next_input_bucket, Key=next_input_key,
                                          Metadata={'UpdatedAt': str(datetime.now())}, MetadataDirective='REPLACE')
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {next_input_bucket}/{next_input_key}")
                    raise e

            return output_filename


        def update_global_feature_node_sql():
            '''
            Update global feature table for current input
            '''
            global TEMP_SQLITE_FILENAME
            global TEMP_NODE_FEATURE_LOOKUP_FILE

            func = "update_global_feature_node_sql"

            node_feature_lookup_json = json.load(open(TEMP_NODE_FEATURE_LOOKUP_FILE, "r"))

            insert_global_feature = """
            INSERT OR IGNORE INTO global_feature(
                feature,
                feature_type,
                added_to_ui
            ) VALUES(
                :feature,
                :feature_type,
                :added_to_ui
            );
            """

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:

                        insert_global_feature_params = []

                        for node_key in node_feature_lookup_json.keys():
                            global_feature_params = {
                                "feature": node_key,
                                "feature_type": "node",
                                "added_to_ui": 0
                            }
                            insert_global_feature_params.append(global_feature_params)

                        print(f"{func}: Add node features to table")
                        cursor.executemany(insert_global_feature, insert_global_feature_params)

                    conn.commit()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def create_global_feature_for_ui():
            '''
            Create global feature file to update on UI
            '''
            global TEMP_SQLITE_FILENAME
            global TEMP_GLOBAL_FEATURE_FILENAME

            func = "update_global_feature_node_sql"

            select_global_feature_event = """
            SELECT * FROM global_feature
            WHERE added_to_ui = 0 AND feature_type = 'event'
            ;
            """

            select_global_feature_node = """
            SELECT * FROM global_feature
            WHERE added_to_ui = 0 AND feature_type = 'node'
            ;
            """

            update_global_feature_added_to_ui = """
            UPDATE global_feature
            SET added_to_ui = 1
            ;
            """

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:

                        global_feature = {
                            "event_feature": [],
                            "node_feature": []
                        }

                        print(f"{func}: Select global feature event")
                        cursor.execute(select_global_feature_event)

                        result = cursor.fetchall()
                        result = [dict(row) for row in result]
                        global_feature["event_feature"] = [feat["feature"] for feat in result]

                        print(f"{func}: Select global feature node")
                        cursor.execute(select_global_feature_node)

                        result = cursor.fetchall()
                        result = [dict(row) for row in result]
                        global_feature["node_feature"] = [feat["feature"] for feat in result]

                        print(f"{func}: Save global feature")
                        json.dump(global_feature, open(TEMP_GLOBAL_FEATURE_FILENAME, "w"))

                        print(f"{func}: Update sql for features that are used")
                        cursor.execute(update_global_feature_added_to_ui)

                    conn.commit()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def create_input_cluster_output():
            '''
            Create user cluster output for input
            '''
            global TEMP_USER_CLUSTER_OUTPUT_FILENAME
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global TEMP_SQLITE_FILENAME
            global MAP_CEF_TO_INTERNAL

            func = "create_input_cluster_output"

            input_cluster_id_list = json.load(open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "r"))

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:
                        cluster_id_list = ",".join(map(str, input_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"
                        select_cluster_output_query = f"""
                        SELECT
                            co.cluster_id,
                            co.cluster_starttime,
                            co.cluster_endtime,
                            co.cluster_srcips,
                            co.cluster_dstips,
                            co.cluster_techs,
                            co.cluster_tacs,
                            co.cluster_stages,
                            GROUP_CONCAT(e.alert_id) AS alert_ids
                        FROM
                            cluster_output co
                        INNER JOIN
                            event e ON co.cluster_id = e.cluster_id
                        WHERE co.cluster_id IN {cluster_id_list}
                        GROUP BY
                            co.cluster_id
                        ;
                        """
                        cursor.execute(select_cluster_output_query)
                        rows = cursor.fetchall()

                        user_cluster_output = []

                        for row in rows:
                            row = dict(row)
                            alert_ids_list = row["alert_ids"].split(",")
                            if SKIP_SINGLE_ALERT and len(alert_ids_list) <= 1:
                                continue

                            if MAP_CEF_TO_INTERNAL:
                                alert_ids_list = [alert_id[:-40] for alert_id in alert_ids_list]
                            splunk_query = ") OR (".join(alert_ids_list)
                            splunk_query = f"({splunk_query})"

                            cluster = {
                                "cluster_id": row["cluster_id"],
                                "ids": alert_ids_list,
                                "splunk_query": splunk_query,
                                "start_time": row["cluster_starttime"],
                                "end_time": row["cluster_endtime"],
                                "src": json.loads(row["cluster_srcips"]),
                                "dst": json.loads(row["cluster_dstips"]),
                                "cluster_techs": json.loads(row["cluster_techs"]),
                                "cluster_tacs": json.loads(row["cluster_tacs"]),
                                "cluster_stages": json.loads(row["cluster_stages"])
                            }

                            user_cluster_output.append(cluster)
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            with open(TEMP_USER_CLUSTER_OUTPUT_FILENAME, "w") as f:
                json.dump(user_cluster_output, f)


        def upload_cluster_and_flow_output(bucket, unique_id):
            '''
            Upload user cluster output, flow output to output folder, splunk
            Args:
                bucket: Bucket name
                unique_id: Unique id of current batch
            '''
            global S3_CLIENT
            global TEMP_USER_FLOW_OUTPUT_JSON_FILENAME
            global TEMP_USER_CLUSTER_OUTPUT_FILENAME
            global SPLUNK_USER_FLOW_JSON_LINES
            global SPLUNK_USER_CLUSTER_JSON_LINES
            func = "upload_cluster_and_flow_output"

            current_timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %z")
            path_to_user_cluster_output = f"output/{current_timestamp}/cluster.json"
            path_to_user_flow_output = f"output/{current_timestamp}/flow.json"

            flow_exists = os.path.exists(TEMP_USER_FLOW_OUTPUT_JSON_FILENAME)

            print(f"{func}: Save user cluster and flow output to S3: {path_to_user_cluster_output}, {path_to_user_flow_output}")
            try:
                S3_CLIENT.upload_file(TEMP_USER_CLUSTER_OUTPUT_FILENAME, bucket, path_to_user_cluster_output)
                print(f"{func}: Saved user cluster output to S3")
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_user_cluster_output}")
                raise e

            if flow_exists:
                try:
                    S3_CLIENT.upload_file(TEMP_USER_FLOW_OUTPUT_JSON_FILENAME, bucket, path_to_user_flow_output)
                    print(f"{func}: Saved user flow output to S3")
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_user_flow_output}")
                    raise e

            # upload to splunk
            path_to_splunk_flow_output = f"splunk/flow_{unique_id}.json"
            path_to_splunk_cluster_output = f"splunk/cluster_{unique_id}.json"

            # convert to json lines format

            if flow_exists:
                with open(TEMP_USER_FLOW_OUTPUT_JSON_FILENAME, "r") as f_read, open(SPLUNK_USER_FLOW_JSON_LINES, "w") as f_write:
                    user_cluster_output = json.load(f_read)
                    for cluster in user_cluster_output:
                        f_write.write(json.dumps(cluster) + "\n")
            
            with open(TEMP_USER_CLUSTER_OUTPUT_FILENAME, "r") as f_read, open(SPLUNK_USER_CLUSTER_JSON_LINES, "w") as f_write:
                user_cluster_output = json.load(f_read)
                for cluster in user_cluster_output:
                    f_write.write(json.dumps(cluster) + "\n")

            print(f"{func}: Save flow output to S3 for splunk: {path_to_splunk_flow_output}")

            if flow_exists:
                try:
                    S3_CLIENT.upload_file(SPLUNK_USER_FLOW_JSON_LINES, bucket, path_to_splunk_flow_output)
                    print(f"{func}: Saved flow output to S3")
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_splunk_flow_output}")
                    raise e

            try:
                S3_CLIENT.upload_file(SPLUNK_USER_CLUSTER_JSON_LINES, bucket, path_to_splunk_cluster_output)
                print(f"{func}: Saved cluster output to S3")
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_splunk_cluster_output}")
                raise e


        def update_flow_sql(bucket, unique_id):
            '''
            Update flow output sqlite and create flow ticket output
            Args:
                bucket: Bucket name
                unique_id: Unique id of chunk of input
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_SQLITE_FILENAME
            global TEMP_FLOW_OUTPUT_JSON_FILENAME
            global TEMP_USER_FLOW_OUTPUT_JSON_FILENAME
            global TEMP_FLOW_TICKET_OUTPUT_FILENAME
            global TEMP_TICKET_ID_LOOKUP_FILE
            global TEMP_FLOW_GLOBAL_METRICS_FILENAME

            func = "update_flow_sql"

            flow_output_json = json.load(open(TEMP_FLOW_OUTPUT_JSON_FILENAME, "r"))

            print(f"{func}: Number of flows: {len(flow_output_json)}")

            # create ticket id
            ticket_id_json = json.load(open(TEMP_TICKET_ID_LOOKUP_FILE, "r"))
            ticket_id_start = ticket_id_json["last_ticket_id"]

            print(ticket_id_start)

            metrics_cols = ["tech", "tac", "stage", "event", "entity"]
            mapped_metric_cols = ["involved_techs", "involved_tacs", "involved_stages", "ids", "involved_entities"]

            global_metric_time_ranges = []
            event_count = 0
            metrics_data = {col: [] for col in metrics_cols}
            global_metrics = {}

            insert_flow_output_query = """
            INSERT OR IGNORE INTO flow_output (
                flow_id,
                cluster_prob,
                alert_ids
            )
            VALUES (
                :flow_id,
                :cluster_prob,
                :alert_ids
            )
            ;
            """

            try:
                with open(TEMP_FLOW_TICKET_OUTPUT_FILENAME, "w") as f:
                    with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                        conn.row_factory = sqlite3.Row
                        conn.execute("PRAGMA foreign_keys = ON;")
                        with closing(conn.cursor()) as cursor:
                            insert_flow_output_params = []
                            for flow in flow_output_json:
                                flow["flow_id"] = flow["Flow_id"]
                                flow.pop("Flow_id")
                                flow["ids"] = []

                                ticket_id_start += 1
                                flow_ticket = copy.deepcopy(flow)
                                flow_ticket.pop("cluster_ids")
                                flow_ticket["ticket_id"] = ticket_id_start

                                print(ticket_id_start)

                                cluster_id_list = ",".join(map(str, flow["cluster_ids"]))
                                cluster_id_list = f"({cluster_id_list})"
                                select_event_query = f"""
                                SELECT
                                    cluster_id,
                                    MIN(time) AS start_time,
                                    MAX(time) AS end_time,
                                    GROUP_CONCAT(DISTINCT src) AS unique_src,
                                    GROUP_CONCAT(DISTINCT dst) AS unique_dst,
                                    GROUP_CONCAT(tech,';') AS combined_tech,
                                    GROUP_CONCAT(tac,';') AS combined_tac,
                                    GROUP_CONCAT(stage,';') AS combined_stage,
                                    GROUP_CONCAT(alert_id) AS alert_ids
                                FROM
                                    event
                                WHERE cluster_id IN {cluster_id_list}
                                GROUP BY cluster_id
                                ORDER BY cluster_id
                                ;
                                """
                                cursor.execute(select_event_query)

                                print("select event query executed")

                                clusters = cursor.fetchall()
                                clusters = [dict(cluster) for cluster in clusters]

                                print("create cluster dataframe")

                                cluster_df = pd.DataFrame(clusters)

                                print("created cluster dataframe")

                                flow_ticket["start_time"] = cluster_df["start_time"].min()
                                flow_ticket["end_time"] = cluster_df["end_time"].max()

                                start_time = pd.to_datetime(cluster_df["start_time"].min(), unit='s')
                                end_time = pd.to_datetime(cluster_df["end_time"].max(), unit='s')

                                print("get start and end time")

                                global_metric_time_range = (end_time - start_time).total_seconds()
                                global_metric_time_ranges.append(global_metric_time_range)

                                print("get time range")

                                flow_ticket["involved_entities"] = []
                                flow_ticket["involved_techs"] = []
                                flow_ticket["involved_tacs"] = []
                                flow_ticket["involved_stages"] = []

                                print("create flow ticket")

                                cluster_df = cluster_df.apply(process_row, axis=1)
                                for index, row_cluster in cluster_df.iterrows():

                                    flow_ticket["involved_entities"] = list(set(flow_ticket["involved_entities"] + row_cluster["involved_entities"]))
                                    flow_ticket["involved_techs"] = list(set(flow_ticket["involved_techs"] + row_cluster["involved_techs"]))
                                    flow_ticket["involved_tacs"] = list(set(flow_ticket["involved_tacs"] + row_cluster["involved_tacs"]))
                                    flow_ticket["involved_stages"] = list(set(flow_ticket["involved_stages"] + row_cluster["involved_stages"]))
                                    flow_ticket["ids"] = list(set(flow_ticket["ids"] + row_cluster["involved_events"]))

                                print("get global metrics from cluster")
                                flow_ticket["metrics"] = get_global_metrics_from_cluster_sql(cluster_df)

                                for metric_col, mapped_metric_col in zip(metrics_cols, mapped_metric_cols):

                                    if metric_col == "event":
                                        count = float(len(flow_ticket[mapped_metric_col]))
                                        event_count += count
                                        metrics_data[metric_col].append(count)
                                    else:
                                        count = float(len(flow_ticket[mapped_metric_col]))
                                        metrics_data[metric_col].append(count)

                                f.write(json.dumps(flow_ticket) + "\n")

                                flow["ids"] = flow_ticket["ids"]

                                flow.pop("cluster_ids")

                                params = {
                                    "flow_id": flow["flow_id"],
                                    "cluster_prob": flow["cluster_prob"],
                                    "alert_ids": json.dumps(flow["ids"])
                                }
                                insert_flow_output_params.append(params)

                            print(f"{func}: Insert flow output to table")
                            cursor.executemany(insert_flow_output_query, insert_flow_output_params)

                            print(f"{func}: Save ticket id lookup file")
                            ticket_id_json["last_ticket_id"] = ticket_id_start
                            json.dump(ticket_id_json, open(TEMP_TICKET_ID_LOOKUP_FILE, "w"))
                            save_ticket_id_lookup_file(bucket)
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            # Calculate statistics for each metric
            for metric_col in metrics_cols:
                if metric_col not in global_metrics:
                    global_metrics[metric_col] = {}

                if metrics_data[metric_col]:
                    global_metrics[metric_col]["avg"] = float(pd.Series(metrics_data[metric_col]).mean())
                    global_metrics[metric_col]["min"] = float(pd.Series(metrics_data[metric_col]).min())
                    global_metrics[metric_col]["max"] = float(pd.Series(metrics_data[metric_col]).max())
                    global_metrics[metric_col]["median"] = float(pd.Series(metrics_data[metric_col]).median())
                    global_metrics[metric_col]["q1"] = float(pd.Series(metrics_data[metric_col]).quantile(0.25))
                    global_metrics[metric_col]["q3"] = float(pd.Series(metrics_data[metric_col]).quantile(0.75))
                    if metric_col == "event":
                        global_metrics[metric_col]["count"] = event_count

            # Calculate statistics for time range
            if global_metric_time_ranges:
                global_metrics["time_range"] = {
                    "avg": float(pd.Series(global_metric_time_ranges).mean()),
                    "min": float(pd.Series(global_metric_time_ranges).min()),
                    "max": float(pd.Series(global_metric_time_ranges).max()),
                    "median": float(pd.Series(global_metric_time_ranges).median()),
                    "q1": float(pd.Series(global_metric_time_ranges).quantile(0.25)),
                    "q3": float(pd.Series(global_metric_time_ranges).quantile(0.75)),
                }

            json.dump(global_metrics, open(TEMP_FLOW_GLOBAL_METRICS_FILENAME, "w"))

            print(f"{func}: Save flow output for user")
            json.dump(flow_output_json, open(TEMP_USER_FLOW_OUTPUT_JSON_FILENAME, "w"))

            try:
                print(f"{func}: Upload global metric output file to S3")
                flow_global_metrics_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_global_metrics.json"
                S3_CLIENT.upload_file(TEMP_FLOW_GLOBAL_METRICS_FILENAME, bucket, flow_global_metrics_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{flow_global_metrics_filename}")
                raise e

            try:
                print(f"{func}: Upload flow ticket output file to S3")
                flow_ticket_output_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_ticket_output.json"
                S3_CLIENT.upload_file(TEMP_FLOW_TICKET_OUTPUT_FILENAME, bucket, flow_ticket_output_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{flow_ticket_output_filename}")
                raise e


        def process_row(row):
            combined_tech = row['combined_tech']
            combined_tech = combined_tech.split(";")
            tech_list = []
            for techs in combined_tech:
                tech_list += json.loads(techs)
            tech_list = list(set(tech_list))

            combined_tac = row['combined_tac']
            combined_tac = combined_tac.split(";")
            tac_list = []
            for tacs in combined_tac:
                tac_list += json.loads(tacs)
            tac_list = list(set(tac_list))

            combined_stage = row['combined_stage']
            combined_stage = combined_stage.split(";")
            stage_list = []
            for stage in combined_stage:
                stage_list += json.loads(stage)
            stage_list = list(set(stage_list))

            row["involved_entities"] = list(set(row["unique_src"].split(",") + row["unique_dst"].split(",")))
            row["involved_techs"] = tech_list
            row["involved_tacs"] = tac_list
            row["involved_stages"] = stage_list
            row["involved_events"] = row["alert_ids"].split(",")
            return row


        def get_global_metrics_from_cluster_sql(cluster_df: pd.DataFrame):
            '''
            Get global metrics from cluster
            Args:
                cluster_df: dataframe of cluster output for which metrics is to be calculated
            Returns:
                metrics dictionary
            '''
            func = "get_global_metrics_from_cluster_sql"

            metrics = {}
            metrics_cols = ["tech", "tac", "stage", "event", "entity"]
            mapped_metric_cols = ["involved_techs", "involved_tacs", "involved_stages", "involved_events", "involved_entities"]

            for metric_col, mapped_metric_col in zip(metrics_cols, mapped_metric_cols):
                cluster_df[f'{metric_col}_count'] = cluster_df[mapped_metric_col].apply(lambda x: float(len(x)))

                metrics[metric_col] = {
                    "avg": cluster_df[f"{metric_col}_count"].mean(),
                    "min": cluster_df[f"{metric_col}_count"].min(),
                    "max": cluster_df[f"{metric_col}_count"].max(),
                    "median": cluster_df[f"{metric_col}_count"].median(),
                    "q1": cluster_df[f"{metric_col}_count"].quantile(0.25),
                    "q3": cluster_df[f"{metric_col}_count"].quantile(0.75)
                }

                if metric_col == "event":
                    metrics[metric_col]["count"] = float(cluster_df[f"{metric_col}_count"].sum())

            # time range metric
            cluster_df["start_time"] = pd.to_datetime(cluster_df['start_time'], unit="s")
            cluster_df["end_time"] = pd.to_datetime(cluster_df['end_time'], unit="s")
            cluster_df["time_range"] = (cluster_df["end_time"] - cluster_df["start_time"]).dt.total_seconds()
            metrics["time_range"] = {
                "avg": cluster_df["time_range"].mean(),
                "min": cluster_df["time_range"].min(),
                "max": cluster_df["time_range"].max(),
                "median": cluster_df["time_range"].median(),
                "q1": cluster_df["time_range"].quantile(0.25),
                "q3": cluster_df["time_range"].quantile(0.75)
            }

            # for _, row in cluster_ticket_output_for_flow.iterrows():
            #     unique_other_attributes = row["metrics"].keys()

            return metrics


        def save_ticket_id_lookup_file(bucket):
            '''
            Save ticket id lookup file to S3
            Args:
                bucket: The S3 bucket name
            '''
            global TICKET_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_TICKET_ID_LOOKUP_FILE
            func = "save_ticket_id_lookup_file"
            try:
                print(f"{func}: Upload ticket id lookup file to S3")
                S3_CLIENT.upload_file(TEMP_TICKET_ID_LOOKUP_FILE, bucket, TICKET_ID_OBJECT_KEY)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{TICKET_ID_OBJECT_KEY}")
                raise e


        def fetch_or_create_ticket_id_lookup_file(bucket):
            '''
            Check if the ticket_id lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global TICKET_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_TICKET_ID_LOOKUP_FILE

            func = "fetch_or_create_ticket_id_lookup_file"

            try:
                print(
                    f"{func}: Checking if ticket id lookup file exists on S3 path {bucket}/{TICKET_ID_OBJECT_KEY}")

                S3_CLIENT.head_object(Bucket=bucket, Key=TICKET_ID_OBJECT_KEY)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Ticket id lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = {"last_ticket_id": 0}
                json.dump(internal_id_to_unique_id_json, open(TEMP_TICKET_ID_LOOKUP_FILE, "w"))

                print(f"{func}: Initiated ticket id")
            else:
                try:
                    print(f"{func}: Download available internal id to unique id lookup file from S3")

                    S3_CLIENT.download_file(bucket, TICKET_ID_OBJECT_KEY, TEMP_TICKET_ID_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{TICKET_ID_OBJECT_KEY}")
                    raise e


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download intermediate files for processing.
            Args:
                bucket: Bucket name
                unique_id: unique id for current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global PATH_TO_SQLITE
            global TEMP_SQLITE_FILENAME
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global QUEUE_LOOKUP_FILE
            global TEMP_QUEUE_LOOKUP_FILE
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global NODE_FEATURE_LOOKUP_FILE
            global TEMP_NODE_FEATURE_LOOKUP_FILE

            func = "download_intermediate_input_files"

            try:
                print(f"{func}: Download sqlite file from S3")
                S3_CLIENT.download_file(bucket, PATH_TO_SQLITE, TEMP_SQLITE_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{PATH_TO_SQLITE}")
                raise e

            try:
                print(f"{func}: Download queue lookup file from S3")
                S3_CLIENT.download_file(bucket, QUEUE_LOOKUP_FILE, TEMP_QUEUE_LOOKUP_FILE)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{QUEUE_LOOKUP_FILE}")
                raise e

            path_to_cluster_id_list = f"{SCRATCH_DIR}/intermediate/{unique_id}/input_cluster_id_list.json"
            print(f"{func}: Save file to S3: {bucket}/{path_to_cluster_id_list}")

            try:
                S3_CLIENT.download_file(bucket, path_to_cluster_id_list, TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME)
                print(f"{func}: Uploaded file to S3: {bucket}/{path_to_cluster_id_list}")
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_cluster_id_list}")
                raise e

            try:
                print(f"{func}: Checking if node feautres lookup file exists on S3 path {bucket}/{NODE_FEATURE_LOOKUP_FILE}")
                S3_CLIENT.head_object(Bucket=bucket, Key=NODE_FEATURE_LOOKUP_FILE)
                NODE_FEATURE_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Node feaures file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, NODE_FEATURE_LOOKUP_FILE, TEMP_NODE_FEATURE_LOOKUP_FILE)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{NODE_FEATURE_LOOKUP_FILE}")
                    raise e


        def get_output_object_key(event):
            '''
            Get output object key from input and output s3 path
            Args:
                event: Input to lambda
            Returns:
                (bucket_name, object_key)
            '''
            func = "get_output_object_key"

            input_bucket, input_key = parse_s3_path(event["inputPath"])

            print(f"{func}: Input bucket: {input_bucket}")
            print(f"{func}: Input key: {input_key}")

            output_bucket, output_key = parse_s3_path(event["outputPath"])

            print(f"{func}: Output bucket: {output_bucket}")
            print(f"{func}: Output key: {output_key}")

            if input_bucket is None or input_key is None or output_bucket is None or output_key is None:
                raise ValueError("Failed to get output path of the batch transform job")

            output_file_name = input_key.split("/")[-1] + ".out"
            output_key += output_file_name

            return output_bucket, output_key


        def parse_s3_path(s3_path):
            '''
            Parse an S3 path and extract the bucket name and object key.
            Args:
                s3_path: The S3 path to parse. Should be in the format: s3://bucket-name/path/to/object
            Returns:
                (bucket_name, object_key).
                (None, None) if the path is invalid.
            '''
            if not s3_path.startswith("s3://"):
                return None, None

            # Remove the "s3://" prefix
            path_without_prefix = s3_path[5:]

            # Split the remaining path into parts
            parts = path_without_prefix.split('/', 1)

            # If there's only one part, it means there's no object key
            if len(parts) == 1:
                return parts[0], None

            # If there are two parts, the first is the bucket name, the second is the object key
            return parts[0], parts[1]


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          campaign_map: !Ref ClusterOrFlowMapToCampaign
          flow_input_window_size: !Ref FlowInputMaxClusters
          map_cef_to_internal: 'true'
  processFlowLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${processFlow}
  processFlowLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref processFlow
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  updateLookupTable:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_update_lookup_table
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: updateLookupTable
      InlineCode: |
        '''
        Update technique lookup table
        Input: Row to be added to lookup table
        '''

        import os
        import json
        import glob
        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        TECHNIQUE_LOOKUP_OBJECT = os.getenv("technique_lookup_object")

        if not TECHNIQUE_LOOKUP_OBJECT:
            raise ValueError("Please enter environment variable: 'technique_lookup_object'")

        S3_CLIENT = boto3.client("s3")

        TEMP_INPUT_FILENAME = "/tmp/classification_response.csv"

        TEMP_NEW_LOOKUP_TABLE_FILENAME = "/tmp/new_lookup_table.csv"

        TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE = "/tmp/technique_lookup.csv"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global TEMP_INPUT_FILENAME
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE

            func = "lambda_handler"

            print(f"{func}: Received event: {json.dumps(event)}")

            bucket = event['Records'][0]['s3']['bucket']['name']
            input_filename = event['Records'][0]['s3']['object']['key']

            clear_temp_dir()

            print(f"{func}: Download file to process from S3 bucket: {bucket}/{input_filename}")

            try:
                S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                print(f"{func}: File to process download completed")
            except Exception as e:
                print(f"{func}: Failed to download file to process")
                raise e

            fetch_or_create_technique_classification_lookup_file(bucket)

            print(f"{func}: Update lookup table")

            update_lookup_table(bucket)
            clean_up(bucket, input_filename)

            print(f"{func}: Lookup table updated.")


        def update_lookup_table(bucket):
            '''
            Update lookup table
            Args:
                bucket: Bucket name
            '''

            global TEMP_INPUT_FILENAME
            global TEMP_NEW_LOOKUP_TABLE_FILENAME
            global S3_CLIENT
            global TECHNIQUE_LOOKUP_OBJECT
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE

            func = "update_lookup_table"

            technique_classification_lookup_df = pd.read_csv(TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE)
            before_len_technique_classification_lookup_df = len(technique_classification_lookup_df)

            classification_response_df = pd.read_csv(TEMP_INPUT_FILENAME)

            new_technique_classification_lookup_df = pd.concat([technique_classification_lookup_df, classification_response_df])
            new_technique_classification_lookup_df_no_dupe = new_technique_classification_lookup_df.drop_duplicates(subset=["alerts"], keep="first")
            after_len_technique_classification_lookup_df = len(new_technique_classification_lookup_df_no_dupe)

            new_technique_classification_lookup_df_no_dupe.to_csv(TEMP_NEW_LOOKUP_TABLE_FILENAME, index=False)

            try:
                S3_CLIENT.upload_file(TEMP_NEW_LOOKUP_TABLE_FILENAME, bucket, TECHNIQUE_LOOKUP_OBJECT)
            except Exception as e:
                print(f"{func}: Failed to save lookup table to S3: {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")
                raise e

            print(f"{func}: Added number of rows: {after_len_technique_classification_lookup_df - before_len_technique_classification_lookup_df}")


        def fetch_or_create_technique_classification_lookup_file(bucket):
            '''
            Fetch or create lookup file
            Args:
                bucket: Bucket name
            '''

            global TECHNIQUE_LOOKUP_OBJECT
            global S3_CLIENT
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE

            func = "fetch_or_create_technique_classification_lookup_file"
            try:
                print(
                    f"{func}: Checking if technique classification lookup file for alerts exists on S3 path {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")

                S3_CLIENT.head_object(Bucket=bucket, Key=TECHNIQUE_LOOKUP_OBJECT)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Technique classification lookup file for alerts does not exist on S3. Create empty table")

                technique_classification_lookup_df = pd.DataFrame(columns=["alerts", "techniques"])
                technique_classification_lookup_df.to_csv(TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE, index=False)

                print(f"{func}: Empty table created")
            else:
                try:
                    print(f"{func}: Download available technique classification lookup file for alerts from S3")

                    S3_CLIENT.download_file(bucket, TECHNIQUE_LOOKUP_OBJECT, TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup table from S3: {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")
                    raise e


        def clean_up(bucket, input_filename):
            '''
            Delete input file
            Args:
                bucket: Bucket name
                input_filename: S3 object to delete
            '''
            global S3_CLIENT

            func = "clean_up"

            try:
                print(f"{func}: Delete message payload from S3")
                S3_CLIENT.delete_object(Bucket=bucket, Key=input_filename)
                print(f"{func}: Message payload deleted")
            except Exception as e:
                print(f"{func}: Failed to delete message payload from S3. You can manually delete the object: {bucket}/{input_filename}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          technique_lookup_object: !Ref TechniqueLookupObject
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: scratch/queue/
                - Name: suffix
                  Value: .csv
  updateLookupTableLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${updateLookupTable}
  updateLookupTableLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref updateLookupTable
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  createCampaign:
    Type: AWS::Serverless::Function
    DependsOn:
    - LoadBalancer
    Properties:
      FunctionName: !Sub ${AWS::StackName}_create_campaign
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: createCampaign
      InlineCode: |
        '''
        This lambda function will create campaigns, events on the UI.

        Input: cluster_ticket_output.json
        '''

        import gzip
        import json
        import os
        import time
        import urllib
        import glob
        import sqlite3
        from contextlib import closing

        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd
        import requests

        UI_LB_URL = os.getenv("ui_lb_url")

        EVENT_THRESHOLD = os.getenv("event_threshold")

        TAC_THRESHOLD = os.getenv("tac_threshold")

        UI_USERNAME = os.getenv("ui_username")

        UI_PASSWORD = os.getenv("ui_password")

        CAMPAIGN_MAP = os.getenv("campaign_map")

        missing_variables = []
        if UI_LB_URL is None:
            missing_variables.append("ui_lb_url")
        if EVENT_THRESHOLD is None:
            missing_variables.append("event_threshold")
        if TAC_THRESHOLD is None:
            missing_variables.append("tac_threshold")
        if UI_USERNAME is None:
            missing_variables.append("ui_username")
        if UI_PASSWORD is None:
            missing_variables.append("ui_password")
        if CAMPAIGN_MAP is None:
            missing_variables.append("campaign_map")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        CAMPAIGN_MAP = CAMPAIGN_MAP.lower()
        if CAMPAIGN_MAP not in ["flow", "cluster"]:
            raise ValueError(f"Valid values for 'campaign_map' environemnt variable are 'flow', 'cluster'. Entered value is: '{CAMPAIGN_MAP}'")

        EVENT_THRESHOLD = int(EVENT_THRESHOLD)

        TAC_THRESHOLD = int(TAC_THRESHOLD)

        HOST = f"http://{UI_LB_URL}:8000/"

        S3_CLIENT = boto3.client("s3")

        CONTENT_TYPE = "application/json"

        SCRATCH_DIR = "scratch"

        PATH_TO_SQLITE = f"{SCRATCH_DIR}/cypienta.db"
        TEMP_SQLITE_FILENAME = "/tmp/cypienta.db"

        TEMP_CLUSTER_TICKET_OUTPUT = "/tmp/cluster_ticket_output.json"
        TEMP_FLOW_TICKET_OUTPUT = "/tmp/flow_ticket_output.json"

        FLOW_TICKET_OUTPUT_EXIST = False

        TEMP_FLOW_GLOBAL_METRICS_FILENAME = "/tmp/flow_global_metrics.json"
        TEMP_GLOBAL_METRICS_FILENAME = "/tmp/global_metrics.json"
        TEMP_GLOBAL_FEATURE_FILENAME = "/tmp/global_feature.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global TEMP_CLUSTER_TICKET_OUTPUT
            global TEMP_FLOW_TICKET_OUTPUT
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_GLOBAL_METRICS_FILENAME
            global TEMP_GLOBAL_FEATURE_FILENAME
            global PATH_TO_SQLITE
            global TEMP_SQLITE_FILENAME
            global FLOW_TICKET_OUTPUT_EXIST

            func = "lambda_handler"

            FLOW_TICKET_OUTPUT_EXIST = False

            # bucket = event['Records'][0]['s3']['bucket']['name']
            # input_file = event['Records'][0]['s3']['object']['key']

            bucket = event["bucket"]
            try:
                input_file = event["input_path"]

                clear_temp_dir()
                unique_id = input_file.split("/")[-2]

                print(f"{func}: input file: {bucket}/{input_file}")

                temp_input_file = TEMP_CLUSTER_TICKET_OUTPUT

                try:
                    print(f"{func}: Download the input file")
                    S3_CLIENT.download_file(bucket, input_file, temp_input_file)
                except Exception as e:
                    print(f"{func}: Failed to download file from S3: {bucket}/{input_file}")
                    raise e

                try:
                    print(f"{func}: Download intermediate files from S3 bucket")
                    download_intermediate_input_files(bucket, unique_id)
                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                print("Starting campaign creation")
                create_campaigns()
                print(f"{func}: Complete campaign creation")

                try:
                    print(f"{func}: Upload sqlite to S3")

                    S3_CLIENT.upload_file(TEMP_SQLITE_FILENAME, bucket, PATH_TO_SQLITE)

                    print(f"{func}: Upload completed")
                except Exception as e:
                    print(f"{func}: Failed to upload sqlite from S3: {bucket}/{PATH_TO_SQLITE}")
                    raise e
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex


        def create_campaigns():
            '''
            Create events, campaigns. Where campaigns could be a flow or a cluster.
            '''
            global TEMP_CLUSTER_TICKET_OUTPUT
            global CAMPAIGN_MAP
            global EVENT_THRESHOLD
            global HOST
            global FLOW_TICKET_OUTPUT_EXIST
            func = "create_campaign"

            retry_limit = 3
            retry_delay = 5

            print(f"{func}: Get UI session")
            for attempt in range(retry_limit):
                try:
                    ui_cookies, ui_headers, ui_session = get_ui_session()
                    break
                except Exception as e:
                    print(f"Attempt {attempt + 1} failed: {e}")
                    if attempt < retry_limit - 1:
                        print(f"Retrying in {retry_delay} seconds")
                        time.sleep(retry_delay)
                    else:
                        print("Max retries reached. Failed to get session.")
                        raise

            create_global_metrics(ui_cookies, ui_headers, ui_session)

            create_global_features(ui_cookies, ui_headers, ui_session)

            create_campaign_for_cluster(ui_cookies, ui_headers, ui_session)

            if FLOW_TICKET_OUTPUT_EXIST:
                create_campaign_for_flow(ui_cookies, ui_headers, ui_session)


        def create_global_metrics(ui_cookies, ui_headers, ui_session):
            '''
            Update or upload global metrics to UI
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_GLOBAL_METRICS_FILENAME
            global TEMP_FLOW_GLOBAL_METRICS_FILENAME
            global FLOW_TICKET_OUTPUT_EXIST
            func = "create_global_metrics"

            global_metrics_json = json.load(open(TEMP_GLOBAL_METRICS_FILENAME))
            payload = {"metric": global_metrics_json}

            try:
                response_from_ui = ui_session.post(f"{HOST}api/v2/globalmetric/", json=payload, cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)
                response_from_ui.raise_for_status()
                print(f"{func}: Global metrics added successful")
            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e

            if FLOW_TICKET_OUTPUT_EXIST:
                global_metrics_json = json.load(open(TEMP_FLOW_GLOBAL_METRICS_FILENAME))
                payload = {"metric": global_metrics_json}

                try:
                    response_from_ui = ui_session.post(f"{HOST}api/v2/flow_globalmetric/", json=payload, cookies=ui_cookies, headers=ui_headers)
                    print(f"{func}: Got response", response_from_ui)
                    response_from_ui.raise_for_status()
                    print(f"{func}: Flow Global metrics added successful")
                except requests.exceptions.RequestException as req_err:
                    print("Request error occurred.")
                    raise req_err
                except Exception as e:
                    print(f"{func}: An unexpected error occurred")
                    raise e


        def create_global_features(ui_cookies, ui_headers, ui_session):
            '''
            Update or upload global metrics to UI
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_GLOBAL_FEATURE_FILENAME
            func = "create_global_features"

            global_feature_json = json.load(open(TEMP_GLOBAL_FEATURE_FILENAME))

            if len(global_feature_json["event_feature"]) < 1 and len(global_feature_json["node_feature"]) < 1:
                print(f"{func}: No new global features to add. Skip adding global features")
                return

            try:
                response_from_ui = ui_session.post(f"{HOST}api/v2/globalfeature/bulk_create/", json=global_feature_json, cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)
                response_from_ui.raise_for_status()
                print(f"{func}: Global features added successful")
            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e


        def create_campaign_for_cluster(ui_cookies, ui_headers, ui_session):
            '''
            Create campaign for cluster
            Args:
                ui_cookies: UI cookies
                ui_headers: UI header
                ui_session: UI session
            '''
            global TEMP_CLUSTER_TICKET_OUTPUT
            global CAMPAIGN_MAP
            global EVENT_THRESHOLD
            global HOST
            func = "create_campaign_for_cluster"

            # will have only 1 row as result because of filter on single cluster and group by on same cluster
            select_cluster_event = """
            SELECT
                co.cluster_id,
                co.campaign_id,
                GROUP_CONCAT(e.alert_id) AS alert_ids
            FROM event e
            INNER JOIN cluster_output co
            ON co.cluster_id = e.cluster_id
            WHERE e.cluster_added_to_ui = 0 AND co.cluster_id = :cluster_id
            GROUP BY co.cluster_id
            ;
            """

            update_cluster_output_campaign_id = """
            UPDATE cluster_output
            SET campaign_id = :campaign_id
            WHERE cluster_id = :cluster_id
            ;
            """

            update_event_added_to_ui = """
            UPDATE event
            SET cluster_added_to_ui = 1
            WHERE cluster_id = :cluster_id
            ;
            """

            try:

                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:
                        with open(TEMP_CLUSTER_TICKET_OUTPUT, "r") as f:
                            for line in f:
                                row_cluster = json.loads(line)

                                print(f"{func}: Evaluating for {CAMPAIGN_MAP}: {row_cluster['cluster_id']}")

                                involved_events_df = pd.DataFrame(row_cluster["involved_events"])

                                # filter out involved events that are already added to UI
                                param = {"cluster_id": row_cluster['cluster_id']}
                                cursor.execute(select_cluster_event, param)

                                result = cursor.fetchone()
                                if result is None:
                                    continue
                                row = dict(result)
                                alert_ids_list = row["alert_ids"].split(",")

                                # will have atleast 1 new event to upload to UI
                                involved_events_df = involved_events_df[involved_events_df["id"].isin(alert_ids_list)]

                                involved_events_df['time_df'] = pd.to_datetime(involved_events_df['time'], unit="s")
                                involved_events_df['time_df'] = involved_events_df['time_df'].dt.strftime('%Y-%m-%d %H:%M:%S')

                                events = create_events_list_for_ui(involved_events_df)

                                if len(events) < EVENT_THRESHOLD:
                                    print(
                                        f"{func}: Length of list of events is less than threshold set. Skip creation of campaign. length: {len(events)}")
                                    continue

                                files, payload = create_payload_to_create_campaign_and_events_in_ui(events, row_cluster['cluster_id'])

                                if row["campaign_id"] is None:
                                    print(f"{func}: Create campaign")
                                    try:
                                        response_from_ui = ui_session.post(f"{HOST}api/v2/campaign/", data=payload, files=files, cookies=ui_cookies, headers=ui_headers)

                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                        print(f"{func}: Campaign creation successful")
                                        response_from_ui = json.loads(response_from_ui.content)
                                        campaign_id = response_from_ui["id"]
                                        print("Campaign id: ", campaign_id)

                                        update_cluster_output_campaign_id_param = {
                                            "cluster_id": row_cluster['cluster_id'],
                                            "campaign_id": campaign_id
                                        }
                                        cursor.execute(update_cluster_output_campaign_id, update_cluster_output_campaign_id_param)

                                    except Exception as e:
                                        print(f"{func}: Failed to create campaign")
                                        raise e
                                else:
                                    print(f"{func}: Update campaign: {row['campaign_id']}")
                                    try:
                                        response_from_ui = ui_session.put(f"{HOST}api/v2/campaign/{row['campaign_id']}/", data=payload, files=files, cookies=ui_cookies, headers=ui_headers)
                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                    except Exception as e:
                                        print(f"{func}: Failed to update campaign")
                                        raise e

                                # update only those events that were added per cluster. in case the cluster has 1 event its not added. but it could get more events in next batch.
                                update_event_added_to_ui_param = {"cluster_id": row_cluster['cluster_id']}
                                cursor.execute(update_event_added_to_ui, update_event_added_to_ui_param)

                        conn.commit()

            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e


        def create_campaign_for_flow(ui_cookies, ui_headers, ui_session):
            '''
            Create campaign for flow
            Args:
                ui_cookies: UI cookies
                ui_headers: UI header
                ui_session: UI session
            '''
            global TEMP_FLOW_TICKET_OUTPUT
            global CAMPAIGN_MAP
            global EVENT_THRESHOLD
            global HOST
            func = "create_campaign_for_flow"

            update_flow_output_campaign_id = """
            UPDATE flow_output
            SET campaign_id = :campaign_id
            WHERE flow_id = :flow_id
            ;
            """

            try:

                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:
                        with open(TEMP_FLOW_TICKET_OUTPUT, "r") as f:
                            for line in f:
                                row_flow = json.loads(line)

                                print(f"{func}: Evaluating for flow: {row_flow['flow_id']}")

                                flow_alert_ids = "','".join(row_flow["ids"])
                                flow_alert_ids = f"('{flow_alert_ids}')"

                                # filter out involved events that are already added to UI

                                select_flow_output = f"""
                                SELECT
                                    fo.flow_id,
                                    fo.campaign_id,
                                    e.alert_id,
                                    e.src,
                                    e.dst,
                                    e.time,
                                    e.name,
                                    e.tech,
                                    e.tac,
                                    e.stage,
                                    e.other_attributes
                                FROM flow_output fo, event e
                                WHERE fo.flow_id = :flow_id
                                AND e.alert_id IN {flow_alert_ids}
                                AND e.flow_added_to_ui = 0
                                ORDER BY e.alert_id
                                ;
                                """
                                param = {"flow_id": row_flow['flow_id']}
                                cursor.execute(select_flow_output, param)

                                result = cursor.fetchone()
                                if result is None:
                                    continue

                                row = dict(result)
                                campaign_id_sql = row["campaign_id"]

                                involved_events = []
                                while result is not None:
                                    row = dict(result)

                                    involved_event_dict = {
                                        "id": row["alert_id"],
                                        "src": row["src"],
                                        "dst": row["dst"],
                                        "time": row["time"],
                                        "name": row["name"],
                                        "tech": json.loads(row["tech"]),
                                        "tac": json.loads(row["tac"]),
                                        "stage": json.loads(row["stage"]),
                                        "other_attributes": json.loads(row["other_attributes"]),
                                    }
                                    involved_events.append(involved_event_dict)

                                    result = cursor.fetchone()

                                involved_events_df = pd.DataFrame(involved_events)

                                involved_events_df['time_df'] = pd.to_datetime(involved_events_df['time'], unit="s")
                                involved_events_df['time_df'] = involved_events_df['time_df'].dt.strftime('%Y-%m-%d %H:%M:%S')

                                events = create_events_list_for_ui(involved_events_df)

                                if len(events) < EVENT_THRESHOLD:
                                    print(
                                        f"{func}: Length of list of events is less than threshold set. Skip creation of campaign. length: {len(events)}")
                                    continue

                                files, payload = create_payload_to_create_campaign_and_events_in_ui(events, row_flow['flow_id'])

                                if campaign_id_sql is None:
                                    print(f"{func}: Create campaign")
                                    try:
                                        response_from_ui = ui_session.post(f"{HOST}api/v2/flow/", data=payload, files=files, cookies=ui_cookies, headers=ui_headers)

                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                        print(f"{func}: Campaign creation successful")
                                        response_from_ui = json.loads(response_from_ui.content)
                                        campaign_id = response_from_ui["id"]
                                        print("Campaign id: ", campaign_id)

                                        update_cluster_output_campaign_id_param = {
                                            "flow_id": row_flow['flow_id'],
                                            "campaign_id": campaign_id
                                        }
                                        cursor.execute(update_flow_output_campaign_id, update_cluster_output_campaign_id_param)

                                    except Exception as e:
                                        print(f"{func}: Failed to create campaign")
                                        raise e
                                else:
                                    print(f"{func}: Update campaign: {campaign_id_sql}")
                                    try:
                                        response_from_ui = ui_session.put(f"{HOST}api/v2/flow/{campaign_id_sql}/", data=payload, files=files, cookies=ui_cookies, headers=ui_headers)
                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                    except Exception as e:
                                        print(f"{func}: Failed to update campaign")
                                        raise e

                                # update only those events that were added per flow. in case the flow has 1 event its not added. but it could get more events in next batch.
                                update_event_added_to_ui = f"""
                                UPDATE event
                                SET flow_added_to_ui = 1
                                WHERE alert_id IN {flow_alert_ids}
                                ;
                                """
                                cursor.execute(update_event_added_to_ui)

                        conn.commit()

            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e


        def create_payload_to_create_campaign_and_events_in_ui(events, campaign):
            '''
            Create payload for creating campaign and event in UI
            Args:
                events: list of events
                campaign: campaign name
            Return:
                files: The zip file that contains events data
                payload: JSON object to create campaign
            '''
            global CAMPAIGN_MAP

            req_gzip_filename = f"/tmp/req.gzip"

            req = json.dumps(events)
            req = gzip.compress(bytes(req, "utf-8"))
            open(req_gzip_filename, "wb").write(req)
            payload = {"name": f"{CAMPAIGN_MAP}-{campaign}", "description": "automatically created alerts", "auto_extract_iocs": "true"}
            files = [('details', ('req.gzip', open(req_gzip_filename, 'rb'), 'application/octet-stream'))]

            return files, payload


        def create_events_list_for_ui(involved_events_df: pd.DataFrame):
            '''
            Create list of events for a campaign
            Args:
                involved_events_df: pandas dataframe of involved events
            Return:
                events: List of events for UI
            '''
            global TAC_THRESHOLD
            events = []

            for _, row in involved_events_df.iterrows():
                row_dict = row.to_dict()
                raw_data = row_dict
                tac = row_dict["tac"]
                if len(tac) < TAC_THRESHOLD:
                    continue
                tech = row_dict["tech"]
                tech = [int(t[1:]) for t in tech]

                mapped_data = {
                    "TTP": {"TACTICS": tac, "TECHNIQUES": tech},
                    "suggested_ttp": tac,
                    "UID": row_dict["id"],
                    "SOURCE_IP": row_dict["src"],
                    "DESTINATION_IP": row_dict["dst"],
                    "HOSTNAME": row_dict["name"],
                    "MESSAGE": row_dict["name"],
                    "TIMESTAMP": row_dict["time_df"],
                }

                raw_data.pop("time_df")
                if "label" in raw_data:
                    raw_data.pop("label")
                if "other_attributes_dict" in raw_data and "empty" in raw_data["other_attributes_dict"]:
                    raw_data["other_attributes_dict"].pop("empty")
                for key, value in raw_data.items():
                    raw_data[key] = str(value)
                events.append({"raw_data": raw_data, "mapped_data": mapped_data})

            return events


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download intermediate files for processing.
            Args:
                bucket: Bucket name
                unique_id: unique id for current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global PATH_TO_SQLITE
            global TEMP_SQLITE_FILENAME
            global TEMP_GLOBAL_METRICS_FILENAME
            global TEMP_GLOBAL_FEATURE_FILENAME
            global TEMP_FLOW_GLOBAL_METRICS_FILENAME

            func = "download_intermediate_input_files"

            fetch_flow_ticket_output_if_present(bucket, unique_id)

            try:
                print(f"{func}: Download sqlite file from S3")
                S3_CLIENT.download_file(bucket, PATH_TO_SQLITE, TEMP_SQLITE_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{PATH_TO_SQLITE}")
                raise e

            global_metrics_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/global_metrics.json"
            try:
                print(f"{func}: Download the global metrics file")
                S3_CLIENT.download_file(bucket, global_metrics_filename, TEMP_GLOBAL_METRICS_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{global_metrics_filename}")
                raise e

            if FLOW_TICKET_OUTPUT_EXIST:
                flow_global_metrics_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_global_metrics.json"
                try:
                    print(f"{func}: Download the flow global metrics file")
                    S3_CLIENT.download_file(bucket, flow_global_metrics_filename, TEMP_FLOW_GLOBAL_METRICS_FILENAME)
                except Exception as e:
                    print(f"{func}: Failed to download file from S3: {bucket}/{flow_global_metrics_filename}")
                    raise e

            print(f"{func}: Download global feature file")
            global_feature_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/global_feature.json"
            try:
                print(f"{func}: Download the global feature file")
                S3_CLIENT.download_file(bucket, global_feature_filename, TEMP_GLOBAL_FEATURE_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{global_feature_filename}")
                raise e


        def fetch_flow_ticket_output_if_present(bucket, unique_id):
            '''
            Fetch flow ticket output if present
            Args:
                bucket: Bucket name
            '''
            global S3_CLIENT
            global FLOW_TICKET_OUTPUT_EXIST
            global TEMP_FLOW_TICKET_OUTPUT

            func = "fetch_flow_ticket_output_if_present"

            flow_ticket_output_file = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_ticket_output.json"

            try:
                print(
                    f"{func}: Checking if flow ticket output exists on S3 path {bucket}/{flow_ticket_output_file}")

                S3_CLIENT.head_object(Bucket=bucket, Key=flow_ticket_output_file)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e
                FLOW_TICKET_OUTPUT_EXIST = False
                print(f"{func}: Flow ticket output does not exist on S3")
            else:
                try:
                    print(f"{func}: Download available flow ticket output from S3")

                    S3_CLIENT.download_file(bucket, flow_ticket_output_file, TEMP_FLOW_TICKET_OUTPUT)
                    FLOW_TICKET_OUTPUT_EXIST = True
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download flow ticket output from S3: {bucket}/{flow_ticket_output_file}")
                    raise e


        def get_ui_session():
            '''
            Get UI session and cookies
            Returns:
                cookies: Cookies for the connection to UI
                headers: Headers for the connection to UI
                s: session for the connection to UI
            '''
            global HOST
            global UI_PASSWORD
            global UI_USERNAME

            try:
                s = requests.Session()
                response = s.get(HOST + "login")
                response.raise_for_status()

                cookies = s.cookies.get_dict()
                csrf = cookies.get("csrftoken")
                if not csrf:
                    raise ValueError("CSRF token not found in cookies")

                headers = {"X-CSRFToken": csrf, "Accept": "application/json"}

                response = s.post(f"{HOST}login/?next",
                                  data={"username": UI_USERNAME, "password": UI_PASSWORD, "csrfmiddlewaretoken": csrf})

                if response.status_code != 404 and not response.url.endswith("/accounts/profile/"):
                    response.raise_for_status()

                print("login successful")
                return cookies, headers, s

            except requests.exceptions.RequestException as e:
                print("Request error occurred.")
                raise e
            except Exception as e:
                print("An unexpected error occurred.")
                raise e


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          campaign_map: !Ref ClusterOrFlowMapToCampaign
          event_threshold: !Ref EventThreshold
          tac_threshold: !Ref TacticThreshold
          ui_lb_url: !GetAtt LoadBalancer.DNSName
          ui_password: !Ref SuperuserPassword
          ui_username: !Ref SuperuserUsername
  createCampaignLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${createCampaign}
  createCampaignLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref createCampaign
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  saveFeedback:
    Type: AWS::Serverless::Function
    DependsOn:
    - LoadBalancer
    - ECSLogGroup
    Properties:
      FunctionName: !Sub ${AWS::StackName}_save_feedback
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: saveFeedback
      InlineCode: |
        '''
        Save copy/cut action feedback to S3

        Input: log message with action id and campaign ids involved
        '''

        import ast
        import base64
        import gzip
        import json
        import os
        import time
        import uuid
        import glob
        from datetime import datetime, timezone
        import sqlite3
        from contextlib import closing

        import pandas as pd
        import boto3
        from botocore.exceptions import ClientError
        import requests
        from requests.exceptions import RequestException

        pd.options.mode.chained_assignment = None

        UI_LB_URL = os.getenv("ui_lb_url")

        BUCKET = os.getenv("bucket")

        UI_USERNAME = os.getenv("ui_username")

        UI_PASSWORD = os.getenv("ui_password")

        missing_variables = []

        if UI_LB_URL is None:
            missing_variables.append("ui_lb_url")
        if BUCKET is None:
            missing_variables.append("bucket")
        if UI_USERNAME is None:
            missing_variables.append("ui_username")
        if UI_PASSWORD is None:
            missing_variables.append("ui_password")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        SCRATCH_DIR = "scratch"

        S3_CLIENT = boto3.client("s3")

        TEMP_CLUSTER_OF_ALERTS_JSON_FILENAME = "/tmp/feedback_alerts_cluster.json"

        PATH_TO_SQLITE = f"{SCRATCH_DIR}/cypienta.db"
        TEMP_SQLITE_FILENAME = "/tmp/cypienta.db"

        TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME = "/tmp/attribute_weights.json"
        TEMP_GLOBAL_CLUSTER_OPERATIONS_FILENAME = "/tmp/global_cluster_operations.json"

        TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME = "/tmp/current_cluster_ticket_output.json"

        PATH_TO_FINAL_CLUSTER_OUTPUT = f"{SCRATCH_DIR}/cluster.json"
        TEMP_FINAL_CLUSTER_OUTPUT_FILENAME = "/tmp/final_cluster_output.json"

        PATH_TO_FINAL_CLUSTER_TICKET_OUTPUT = f"{SCRATCH_DIR}/cluster_ticket_output.json"
        TEMP_FINAL_CLUSTER_TICKET_OUTPUT_FILENAME = "/tmp/final_cluster_ticket_output.json"

        INITIAL_WEIGHTS_VALUE = 100
        MIN_WEIGHTS_VALUE = 0
        MAX_WEIGHTS_VALUE = 200

        SOURCE_EMPTY = False

        HOST = f"http://{UI_LB_URL}:8000/"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global SCRATCH_DIR
            global PATH_TO_SQLITE
            global TEMP_SQLITE_FILENAME
            global SOURCE_EMPTY

            func = "lambda_handler"

            SOURCE_EMPTY = False

            print(f"{func}: Received event: {json.dumps(event)}")

            aws_log = event['awslogs']['data']
            compressed_payload = base64.b64decode(aws_log)
            uncompressed_payload = gzip.decompress(compressed_payload)
            decoded_payload = json.loads(uncompressed_payload.decode('utf-8'))

            print("Decoded CloudWatch data: ", decoded_payload)

            message = decoded_payload["logEvents"][0]["message"]

            clear_temp_dir()

            try:
                message_json = json.loads(message)

                campaign_id_list = message_json["message"]["campaign_id"]
                action_id = message_json["message"]["action_id"]
            except json.JSONDecodeError as json_error:
                print(f"Error parsing JSON from message: {message}")
                raise json_error
            except KeyError as key_error:
                print("Missing key in JSON message")
                raise key_error

            print(f"{func}: Save feedback for campaign id(s): {campaign_id_list}")
            print(f"{func}: Save feedback for action id: {action_id}")

            try:
                print(f"{func}: Download sqlite file from S3")
                S3_CLIENT.download_file(BUCKET, PATH_TO_SQLITE, TEMP_SQLITE_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file: {BUCKET}/{PATH_TO_SQLITE}")
                raise e

            save_feedback(action_id)

            try:
                print(f"{func}: Upload sqlite to S3")

                S3_CLIENT.upload_file(TEMP_SQLITE_FILENAME, BUCKET, PATH_TO_SQLITE)

                print(f"{func}: Upload completed")
            except Exception as e:
                print(f"{func}: Failed to upload sqlite from S3: {BUCKET}/{PATH_TO_SQLITE}")
                raise e


        def save_feedback(action_id):
            '''
            Fetch action performed on campaigns and create operations list, update final cluster json
            create new cluster ticket output
            Args:
                action_id: action id in UI that has details for the action performed
            '''
            global BUCKET
            global S3_CLIENT

            func = "save_feedback"

            try:
                ui_cookies, ui_headers, ui_session = get_ui_session()
            except Exception as e:
                print(f"{func}: Failed to get session.")
                raise e

            try:
                action_attributes = get_action_attributes(ui_cookies, ui_headers, ui_session, action_id)
            except Exception as e:
                print(f"{func}: Failed to get action attributes.")
                raise e

            # get weights from the action and save to global weights
            update_global_attributes_weights(action_attributes)

            # create current action operations
            source_cluster_id, destination_cluster_id = create_current_operations(action_attributes, ui_cookies, ui_headers, ui_session)

            metrics = update_cluster_output_sql(source_cluster_id, destination_cluster_id)

            update_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id, metrics)

            create_current_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id)

            # upload feedback cluster ticket output to S3
            current_timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %z")
            path_to_feedback_cluster_ticket_output = f"{SCRATCH_DIR}/feedback/cluster_ticket_output_{current_timestamp}.json"
            try:
                S3_CLIENT.upload_file(TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME, BUCKET, path_to_feedback_cluster_ticket_output)
                print(f"{func}: Uploaded file to S3: {BUCKET}/{path_to_feedback_cluster_ticket_output}")
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {BUCKET}/{path_to_feedback_cluster_ticket_output}")
                raise e


        def create_current_operations(action_attributes, ui_cookies, ui_headers, ui_session):
            '''
            Create operations of create and delete to perform on clusters
            Args:
                action_attributes: Record of current action performed on which the function is triggered
            '''
            global TEMP_SQLITE_FILENAME

            func = "create_current_operations"

            print(f"{func}: Get involved events list")
            events_involved = action_attributes["events_involved"][str(action_attributes['dst_campaign'][0])]
            print(f"{func}: Involved events: {events_involved}")

            # get user alert ids
            print(f"{func}: Get involved user alert ids")
            involved_user_alert_ids = []
            for event_id in events_involved:
                raw_event = get_raw_event(ui_cookies, ui_headers, ui_session, action_attributes['dst_campaign'][0], event_id)
                involved_user_alert_ids.append(raw_event["raw_data"]["id"])

            print(f"{func}: Involved user alert ids: {involved_user_alert_ids}")

            insert_operation_query = """
            INSERT INTO operation_on_cluster (
                cluster_id,
                alert_ids,
                operation_type
            )
            VALUES (
                :cluster_id,
                :alert_ids,
                :action_type
            )
            ;
            """

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:

                        campaign_id_list = ",".join(map(str, [action_attributes['src_campaign'], action_attributes['dst_campaign'][0]]))
                        campaign_id_list = f"({campaign_id_list})"
                        select_cluster_output = f"""
                        SELECT cluster_id, campaign_id
                        FROM cluster_output
                        WHERE campaign_id IN {campaign_id_list}
                        ;
                        """

                        cursor.execute(select_cluster_output)
                        result = cursor.fetchall()

                        source_cluster_id = None
                        destination_cluster_id = None

                        for row in result:
                            row = dict(row)
                            if row["campaign_id"] == action_attributes['src_campaign']:
                                source_cluster_id = row["cluster_id"]
                            if row["campaign_id"] == action_attributes['dst_campaign'][0]:
                                destination_cluster_id = row["cluster_id"]

                        if not source_cluster_id:
                            raise ValueError(f"Unable to find cluster id for campaign id: {action_attributes['src_campaign']}")
                        if not destination_cluster_id:
                            raise ValueError(f"Unable to find cluster id for campaign id: {action_attributes['dst_campaign'][0]}")

                        print(f"{func}: Source cluster id: {source_cluster_id}")
                        print(f"{func}: Destination cluster id: {destination_cluster_id}")

                        insert_operation_params = [
                            {
                                "action_type": "DELETE",
                                "cluster_id": source_cluster_id,
                                "alert_ids": json.dumps([involved_user_alert_ids])
                            },
                            {
                                "action_type": "INSERT",
                                "cluster_id": destination_cluster_id,
                                "alert_ids": json.dumps([involved_user_alert_ids])
                            }
                        ]

                        cursor.executemany(insert_operation_query, insert_operation_params)

                        update_event_params = {
                            "cluster_id": destination_cluster_id
                        }
                        alert_id_list = "','".join(map(str, involved_user_alert_ids))
                        alert_id_list = f"('{alert_id_list}')"
                        update_event_query = f"""
                        UPDATE event
                        SET cluster_id = :cluster_id
                        WHERE alert_id IN {alert_id_list}
                        ;
                        """
                        print(update_event_query)
                        cursor.execute(update_event_query, update_event_params)
                    conn.commit()

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            return source_cluster_id, destination_cluster_id


        def update_cluster_output_sql(source_cluster_id, destination_cluster_id):
            '''
            Update cluster output table in sqlite
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
            Returns:
                metrics dictionary for cluster id (contains count of unique tech, tac, stage per cluster)
            '''
            global TEMP_SQLITE_FILENAME
            global SOURCE_EMPTY

            func = "update_cluster_output_sql"

            batch_cluster_id_list = [source_cluster_id, destination_cluster_id]

            update_cluster_query = """
            UPDATE cluster_output
            SET cluster_starttime = :cluster_starttime,
                cluster_endtime = :cluster_endtime,
                cluster_srcips = :cluster_srcips,
                cluster_dstips = :cluster_dstips,
                cluster_techs = :cluster_techs,
                cluster_tacs = :cluster_tacs,
                cluster_stages = :cluster_stages
            WHERE cluster_id = :cluster_id
            ;
            """

            metrics = {}

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:

                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"

                        select_events_filter_cluster_query = f"""
                        SELECT
                            cluster_id,
                            MIN(time) AS min_time,
                            MAX(time) AS max_time,
                            GROUP_CONCAT(DISTINCT src) AS unique_src,
                            GROUP_CONCAT(DISTINCT dst) AS unique_dst,
                            GROUP_CONCAT(tech,';') AS combined_tech,
                            GROUP_CONCAT(tac,';') AS combined_tac,
                            GROUP_CONCAT(stage,';') AS combined_stage
                        FROM
                            event
                        WHERE cluster_id IN {cluster_id_list}
                        GROUP BY cluster_id
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_events_filter_cluster_query)
                        result = cursor.fetchall()

                        update_cluster_output_params = []
                        for row in result:
                            row = dict(row)
                            src_ips = row['unique_src']
                            src_ips = src_ips.split(",")

                            dst_ips = row['unique_dst']
                            dst_ips = dst_ips.split(",")

                            combined_tech = row['combined_tech']
                            combined_tech = combined_tech.split(";")
                            tech_list = []
                            for techs in combined_tech:
                                tech_list += json.loads(techs)
                            tech_list = list(set(tech_list))

                            combined_tac = row['combined_tac']
                            combined_tac = combined_tac.split(";")
                            tac_list = []
                            for tacs in combined_tac:
                                tac_list += json.loads(tacs)
                            tac_list = list(set(tac_list))

                            combined_stage = row['combined_stage']
                            combined_stage = combined_stage.split(";")
                            stage_list = []
                            for stage in combined_stage:
                                stage_list += json.loads(stage)
                            stage_list = list(set(stage_list))

                            metrics[row['cluster_id']] = {
                                "tech": {"count": len(tech_list)},
                                "tac": {"count": len(tac_list)},
                                "stage": {"count": len(stage_list)}
                            }

                            # if cluster id is present in table. Update fields for existing row.
                            params = {
                                "cluster_starttime": row['min_time'],
                                "cluster_endtime": row['max_time'],
                                "cluster_srcips": json.dumps(src_ips),
                                "cluster_dstips": json.dumps(dst_ips),
                                "cluster_techs": json.dumps(tech_list),
                                "cluster_tacs": json.dumps(tac_list),
                                "cluster_stages": json.dumps(stage_list),
                                "cluster_id": row['cluster_id']
                            }

                            update_cluster_output_params.append(params)

                        if source_cluster_id not in metrics:
                            SOURCE_EMPTY = True
                            params = {
                                "cluster_starttime": 0.0,
                                "cluster_endtime": 0.0,
                                "cluster_srcips": json.dumps([]),
                                "cluster_dstips": json.dumps([]),
                                "cluster_techs": json.dumps([]),
                                "cluster_tacs": json.dumps([]),
                                "cluster_stages": json.dumps([]),
                                "cluster_id": source_cluster_id
                            }

                            update_cluster_output_params.append(params)

                        cursor.executemany(update_cluster_query, update_cluster_output_params)

                    conn.commit()
                return metrics
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id, metrics):
            '''
            Update cluster ticket output metrics
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
                metrics: list of metric for cluster id (contains count of unique tech, tac, stage per cluster)
            '''
            global TEMP_SQLITE_FILENAME

            func = "update_cluster_ticket_output_sql"

            batch_cluster_id_list = [source_cluster_id, destination_cluster_id]

            metrics_cols = ["tech", "tac", "stage"]

            update_cluster_ticket_output_query = """
            UPDATE cluster_ticket_output
            SET metrics = :metrics
            WHERE cluster_id = :cluster_id
            ;
            """

            try:
                with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                    conn.row_factory = sqlite3.Row
                    conn.execute("PRAGMA foreign_keys = ON;")
                    with closing(conn.cursor()) as cursor:
                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"
                        select_event_query = f"""
                        SELECT *
                        FROM event
                        WHERE cluster_id IN {cluster_id_list}
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_event_query)
                        results = cursor.fetchall()
                        results = [dict(row) for row in results]
                        events_df = pd.DataFrame(results)

                        update_cluster_ticket_output_params = []
                        for cluster_id in batch_cluster_id_list:
                            if cluster_id in metrics:
                                filtered_events = events_df[events_df["cluster_id"] == cluster_id]
                                cluster_metrics = per_cluster_metrics_events(filtered_events)

                                for metric_col in metrics_cols:
                                    cluster_metrics[metric_col]["count"] = float(metrics[cluster_id][metric_col]["count"])

                                params = {
                                    "cluster_id": cluster_id,
                                    "metrics": json.dumps(cluster_metrics)
                                }
                                update_cluster_ticket_output_params.append(params)
                            else:
                                params = {
                                    "cluster_id": cluster_id,
                                    "metrics": json.dumps({})
                                }
                                update_cluster_ticket_output_params.append(params)
                        cursor.executemany(update_cluster_ticket_output_query, update_cluster_ticket_output_params)

                    conn.commit()

            except Exception as e:
                print(f"{func}: Failed to update cluster ticket output table.")
                raise e


        def per_cluster_metrics_events(involved_events_df):
            '''
            Get metrics per cluster
            Args:
                row_cluster_ticket_output: one record of cluster ticket output
            Returns:
                metrics of one record of cluster ticket output
            '''

            metrics_cols = ["tech", "tac", "stage"]

            metrics = {}

            for metric_col in metrics_cols:
                involved_events_df[f'{metric_col}'] = involved_events_df[f'{metric_col}'].apply(json.loads)
                involved_events_df[f'{metric_col}_count'] = involved_events_df[f'{metric_col}'].apply(len)

                metrics[metric_col] = {
                    "avg": float(involved_events_df[f"{metric_col}_count"].mean()),
                    "min": float(involved_events_df[f"{metric_col}_count"].min()),
                    "max": float(involved_events_df[f"{metric_col}_count"].max()),
                    "median": float(involved_events_df[f"{metric_col}_count"].median())
                }

            return metrics


        def create_current_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id):
            '''
            Create cluster ticket output for current operation clusters
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_SQLITE_FILENAME
            global TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME
            global SOURCE_EMPTY

            func = "create_current_cluster_ticket_output_sql"

            input_cluster_id_list = [source_cluster_id, destination_cluster_id]

            try:
                with open(TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME, "w") as f:
                    with closing(sqlite3.connect(TEMP_SQLITE_FILENAME)) as conn:
                        conn.row_factory = sqlite3.Row
                        conn.execute("PRAGMA foreign_keys = ON;")
                        with closing(conn.cursor()) as cursor:
                            cluster_id_list = ",".join(map(str, input_cluster_id_list))
                            cluster_id_list = f"({cluster_id_list})"
                            select_query = f"""
                            SELECT 
                                co.cluster_id,
                                co.cluster_starttime,
                                co.cluster_endtime,
                                co.cluster_srcips,
                                co.cluster_dstips,
                                co.cluster_techs,
                                co.cluster_tacs,
                                co.cluster_stages,
                                cto.ticket_id,
                                cto.metrics,
                                e.alert_id,
                                e.src AS event_src,
                                e.dst AS event_dst,
                                e.time AS event_time,
                                e.name AS event_name,
                                e.tech AS event_tech,
                                e.tac AS event_tac,
                                e.stage AS event_stage,
                                e.other_attributes AS event_other_attributes
                            FROM cluster_ticket_output cto
                            INNER JOIN cluster_output co ON co.cluster_id = cto.cluster_id
                            INNER JOIN event e ON e.cluster_id = cto.cluster_id
                            WHERE cto.cluster_id IN {cluster_id_list}
                            ORDER BY cto.cluster_id
                            ;
                            """
                            cursor.execute(select_query)

                            row = cursor.fetchone()
                            prev_cluster = None

                            involved_events = []

                            while row:
                                row = dict(row)

                                curr_cluster = row
                                if prev_cluster is None:
                                    prev_cluster = curr_cluster

                                if curr_cluster["cluster_id"] != prev_cluster["cluster_id"]:
                                    row_cluster_ticket_output = {
                                        "ticket_id": prev_cluster["ticket_id"],
                                        "cluster_id": prev_cluster["cluster_id"],
                                        "involved_events": involved_events,
                                        "start_time": prev_cluster["cluster_starttime"],
                                        "end_time": prev_cluster["cluster_endtime"],
                                        "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                        "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                        "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                        "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                        "metrics": json.loads(prev_cluster["metrics"]),
                                    }
                                    f.write(json.dumps(row_cluster_ticket_output) + "\n")
                                    involved_events = []

                                events = {
                                    "id": row["alert_id"],
                                    "src": row["event_src"],
                                    "dst": row["event_dst"],
                                    "time": row["event_time"],
                                    "name": row["event_name"],
                                    "tech": json.loads(row["event_tech"]),
                                    "tac": json.loads(row["event_tac"]),
                                    "stage": json.loads(row["event_stage"]),
                                    "other_attributes_dict": json.loads(row["event_other_attributes"]),
                                }
                                involved_events.append(events)

                                prev_cluster = curr_cluster
                                row = cursor.fetchone()

                            row_cluster_ticket_output = {
                                "ticket_id": prev_cluster["ticket_id"],
                                "cluster_id": prev_cluster["cluster_id"],
                                "involved_events": involved_events,
                                "start_time": prev_cluster["cluster_starttime"],
                                "end_time": prev_cluster["cluster_endtime"],
                                "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                "metrics": json.loads(prev_cluster["metrics"]),
                            }
                            f.write(json.dumps(row_cluster_ticket_output) + "\n")

                            if SOURCE_EMPTY:
                                select_query = f"""
                                SELECT 
                                    co.cluster_id,
                                    co.cluster_starttime,
                                    co.cluster_endtime,
                                    co.cluster_srcips,
                                    co.cluster_dstips,
                                    co.cluster_techs,
                                    co.cluster_tacs,
                                    co.cluster_stages,
                                    cto.ticket_id,
                                    cto.metrics
                                FROM cluster_ticket_output cto
                                INNER JOIN cluster_output co ON co.cluster_id = cto.cluster_id
                                WHERE cto.cluster_id = {source_cluster_id}
                                ORDER BY cto.cluster_id
                                ;
                                """
                                cursor.execute(select_query)

                                row = cursor.fetchone()
                                row_cluster_ticket_output = {
                                    "ticket_id": row["ticket_id"],
                                    "cluster_id": row["cluster_id"],
                                    "involved_events": [],
                                    "start_time": row["cluster_starttime"],
                                    "end_time": row["cluster_endtime"],
                                    "involved_entities": list(set(json.loads(row["cluster_dstips"]) + json.loads(row["cluster_srcips"]))),
                                    "involved_techs": json.loads(row["cluster_techs"]),
                                    "involved_tacs": json.loads(row["cluster_tacs"]),
                                    "involved_stages": json.loads(row["cluster_stages"]),
                                    "metrics": json.loads(row["metrics"]),
                                }
                            f.write(json.dumps(row_cluster_ticket_output) + "\n")

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_global_attributes_weights(action_attributes):
            '''
            Update global attributes weights file
            Args:
                action_attributes: Record of current action performed on which the function is triggered
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME
            global INITIAL_WEIGHTS_VALUE

            func = "update_global_attributes_weights"

            path_to_global_attribute_weights = f"{SCRATCH_DIR}/attribute_weights.json"
            fetch_or_create_global_attribute_weights(path_to_global_attribute_weights)

            global_attribute_weights_json = json.load(open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME))

            print(f"{func}: Current weights: {global_attribute_weights_json}")

            attributes_list = ["node", "event"]

            map_attributes = {
                "tech": "technique"
            }

            for attr in attributes_list:
                for key, value in action_attributes["attributes"][attr].items():
                    if key in map_attributes:
                        key = map_attributes[key]
                    if key not in global_attribute_weights_json[attr]:
                        global_attribute_weights_json[attr][key] = INITIAL_WEIGHTS_VALUE

                    global_attribute_weights_json[attr][key] += value

                    if global_attribute_weights_json[attr][key] < MIN_WEIGHTS_VALUE:
                        global_attribute_weights_json[attr][key] = MIN_WEIGHTS_VALUE

                    if global_attribute_weights_json[attr][key] > MAX_WEIGHTS_VALUE:
                        global_attribute_weights_json[attr][key] = MAX_WEIGHTS_VALUE
            
            print(f"{func}: After update weights: {global_attribute_weights_json}")
            json.dump(global_attribute_weights_json, open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, "w"))

            try:
                print(f"{func}: Upload file to S3: {BUCKET}/{path_to_global_attribute_weights}")
                S3_CLIENT.upload_file(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, BUCKET, path_to_global_attribute_weights)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {BUCKET}/{path_to_global_attribute_weights}")
                raise e


        def fetch_or_create_global_attribute_weights(path_to_global_attribute_weights):
            '''
            Get or initialize the global attribute weights file
            Args:
                path_to_global_attribute_weights: global attribute weights object key
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global BUCKET
            global TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME
            global INITIAL_WEIGHTS_VALUE
            func = "fetch_or_create_global_attribute_weights"

            try:
                print(
                    f"{func}: Checking if file exists on S3 path {BUCKET}/{path_to_global_attribute_weights}")

                S3_CLIENT.head_object(Bucket=BUCKET, Key=path_to_global_attribute_weights)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: File does not exist on S3. Initialize file")

                global_attribute_weights_json = {
                    "event": {
                        "technique": INITIAL_WEIGHTS_VALUE,
                        "tactic": INITIAL_WEIGHTS_VALUE,
                        "stage": INITIAL_WEIGHTS_VALUE,
                        "count": INITIAL_WEIGHTS_VALUE,
                        "priority": INITIAL_WEIGHTS_VALUE,
                        "port": INITIAL_WEIGHTS_VALUE,
                        "url": INITIAL_WEIGHTS_VALUE,
                        "user_agent": INITIAL_WEIGHTS_VALUE,
                        "cert": INITIAL_WEIGHTS_VALUE
                    },
                    "node": {
                        "os": INITIAL_WEIGHTS_VALUE,
                        "risk": INITIAL_WEIGHTS_VALUE,
                        "user": INITIAL_WEIGHTS_VALUE,
                        "domain": INITIAL_WEIGHTS_VALUE,
                        "subnet": INITIAL_WEIGHTS_VALUE,
                        "usergroup": INITIAL_WEIGHTS_VALUE,
                        "geolocation": INITIAL_WEIGHTS_VALUE
                    }
                }
                json.dump(global_attribute_weights_json, open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, "w"))

                print(f"{func}: Initiated file")
            else:
                try:
                    print(f"{func}: Download available file from S3")

                    S3_CLIENT.download_file(BUCKET, path_to_global_attribute_weights, TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {BUCKET}/{path_to_global_attribute_weights}")
                    raise e


        def get_campaign(ui_cookies, ui_headers, ui_session, campaign_id):
            '''
            Get campaign details by sending a request to the UI
            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                campaign_id: ID of the campaign
            Returns:
                JSON response containing the campaign
            '''
            global HOST

            func = "get_campaign"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/campaign/{campaign_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get campaign details for campaign id: {campaign_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response for campaign_id id: {campaign_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_raw_event(ui_cookies, ui_headers, ui_session, campaign_id, event_id):
            '''
            Get raw event details by event ID within a campaign.

            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                campaign_id: ID of the campaign
                event_id: ID of the event
            Returns:
                JSON response containing the raw event details
            '''
            global HOST

            func = "get_raw_event"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/campaign/{campaign_id}/event/{event_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get event details for event id: {event_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response for event id: {event_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_action_attributes(ui_cookies, ui_headers, ui_session, action_id):
            '''
            Get action attributes by action ID.

            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                action_id: ID of the action

            Returns:
                JSON response containing the action attributes
            '''
            global HOST

            func = "get_action_attributes"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/action/{action_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get attributes from action id: {action_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response from action id: {action_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_ui_session():
            '''
            Get UI session and cookies
            Returns:
                cookies: Cookies for the connection to UI
                headers: Headers for the connection to UI
                s: session for the connection to UI
            '''
            global HOST
            global UI_PASSWORD
            global UI_USERNAME

            try:
                s = requests.Session()
                response = s.get(HOST + "login")
                response.raise_for_status()

                cookies = s.cookies.get_dict()
                csrf = cookies.get("csrftoken")
                if not csrf:
                    raise ValueError("CSRF token not found in cookies")

                headers = {"X-CSRFToken": csrf, "Accept": "application/json"}

                response = s.post(f"{HOST}login/?next",
                                  data={"username": UI_USERNAME, "password": UI_PASSWORD, "csrfmiddlewaretoken": csrf})

                if response.status_code != 404 and not response.url.endswith("/accounts/profile/"):
                    response.raise_for_status()

                print("login successful")
                return cookies, headers, s

            except requests.exceptions.RequestException as e:
                print("Request error occurred.")
                raise e
            except Exception as e:
                print("An unexpected error occurred.")
                raise e


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
          ui_lb_url: !GetAtt LoadBalancer.DNSName
          ui_password: !Ref SuperuserPassword
          ui_username: !Ref SuperuserUsername
      Events:
        MyLogGroupTrigger:
          Type: CloudWatchLogs
          Properties:
            LogGroupName: !Ref ECSLogGroup
            FilterPattern: Save campaign json
  saveFeedbackLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${saveFeedback}
  saveFeedbackLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref saveFeedback
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  stepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub ${AWS::StackName}_step_function
      RoleArn: !GetAtt StepFunctionRole.Arn
      LoggingConfiguration:
        Level: ALL
        IncludeExecutionData: true
        Destinations:
        - CloudWatchLogsLogGroup:
            LogGroupArn: !GetAtt StepFunctionLogGroup.Arn
      DefinitionString: !Sub |
        {
          "Comment": "A description of my state machine",
          "StartAt": "enrich_with_technique",
          "States": {
            "enrich_with_technique": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${enrichWithTechnique.Arn}",
                "Payload.$": "$"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "Next": "tech_or_cluster"
            },
            "wait technique job": {
              "Type": "Wait",
              "Next": "technique_transform_job",
              "Seconds": 300
            },
            "technique_transform_job": {
              "Type": "Task",
              "Parameters": {
                "TransformJobName.$": "$.TransformJobName"
              },
              "Resource": "arn:aws:states:::aws-sdk:sagemaker:describeTransformJob",
              "Next": "technique Job Complete?",
              "Retry": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "BackoffRate": 2,
                  "IntervalSeconds": 1,
                  "MaxAttempts": 2
                }
              ],
              "ResultSelector": {
                "TransformJobStatus.$": "$.TransformJobStatus",
                "bucket.$": "States.ArrayGetItem(States.StringSplit($.TransformOutput.S3OutputPath, '/'),1)",
                "prefix.$": "States.Format('scratch/response/classification_out/{}/', States.ArrayGetItem(States.StringSplit($.TransformOutput.S3OutputPath, '/'),5))"
              },
              "ResultPath": "$.response"
            },
            "technique Job Complete?": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.response.TransformJobStatus",
                  "StringEquals": "Failed",
                  "Next": "Fail"
                },
                {
                  "Variable": "$.response.TransformJobStatus",
                  "StringEquals": "Completed",
                  "Next": "map tech"
                },
                {
                  "Variable": "$.response.TransformJobStatus",
                  "StringEquals": "Stopped",
                  "Next": "Fail"
                }
              ],
              "Default": "wait technique job"
            },
            "map tech": {
              "Type": "Map",
              "ItemProcessor": {
                "ProcessorConfig": {
                  "Mode": "DISTRIBUTED",
                  "ExecutionType": "STANDARD"
                },
                "StartAt": "process_enriched_with_technique",
                "States": {
            "process_enriched_with_technique": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${processEnrichedWithTechnique.Arn}",
                "Payload.$": "$"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException",
                    "States.TaskFailed"
                  ],
                  "MaxAttempts": 3,
                  "BackoffRate": 2,
                  "IntervalSeconds": 5
                }
              ],
                    "End": true
                  }
                }
              },
              "Next": "create_embedding",
              "Label": "maptech",
              "ItemReader": {
                "Resource": "arn:aws:states:::s3:listObjectsV2",
                "Parameters": {
                  "Bucket.$": "$.bucket",
                  "Prefix.$": "$.prefix"
                }
              },
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "OutputPath": "$.[0]",
              "InputPath": "$.response"
            },
            "tech_or_cluster": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-tech*",
                  "Next": "wait technique job"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-cluster*",
                  "Next": "create_embedding"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringEquals": "skipped_input",
                  "Next": "Success"
                }
              ],
              "Default": "Fail"
            },
            "create_embedding": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$",
                "FunctionName": "${createEmbedding.Arn}"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Next": "wait embedding job",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "wait embedding job": {
              "Type": "Wait",
              "Next": "embedding_transform_job",
              "Seconds": 300
            },
            "embedding_transform_job": {
              "Type": "Task",
              "Parameters": {
                "TransformJobName.$": "$.TransformJobName"
              },
              "Resource": "arn:aws:states:::aws-sdk:sagemaker:describeTransformJob",
              "Next": "embedding Job Complete?",
              "Retry": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "BackoffRate": 2,
                  "IntervalSeconds": 1,
                  "MaxAttempts": 2
                }
              ],
              "ResultSelector": {
                "TransformJobStatus.$": "$.TransformJobStatus",
                "bucket.$": "States.ArrayGetItem(States.StringSplit($.TransformOutput.S3OutputPath, '/'),1)",
                "prefix.$": "States.Format('scratch/response/embedding_out/{}/', States.ArrayGetItem(States.StringSplit($.TransformOutput.S3OutputPath, '/'),5))"
              },
              "ResultPath": "$.response"
            },
            "embedding Job Complete?": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.response.TransformJobStatus",
                  "StringEquals": "Failed",
                  "Next": "Fail"
                },
                {
                  "Variable": "$.response.TransformJobStatus",
                  "StringEquals": "Completed",
                  "Next": "map embedding"
                },
                {
                  "Variable": "$.response.TransformJobStatus",
                  "StringEquals": "Stopped",
                  "Next": "Fail"
                }
              ],
              "Default": "wait embedding job"
            },
            "map embedding": {
              "Type": "Map",
              "ItemProcessor": {
                "ProcessorConfig": {
                  "Mode": "DISTRIBUTED",
                  "ExecutionType": "STANDARD"
                },
                "StartAt": "process_embedding",
                "States": {
                  "process_embedding": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "OutputPath": "$.Payload",
                    "Parameters": {
                      "FunctionName": "${processEmbedding.Arn}",
                      "Payload.$": "$"
                    },
                    "Retry": [
                      {
                        "ErrorEquals": [
                          "Lambda.ServiceException",
                          "Lambda.AWSLambdaException",
                          "Lambda.SdkClientException",
                          "Lambda.TooManyRequestsException",
                          "States.TaskFailed"
                        ],
                        "MaxAttempts": 3,
                        "BackoffRate": 2,
                        "IntervalSeconds": 5
                      }
                    ],
                    "End": true
                  }
                }
              },
              "Next": "create_cluster",
              "Label": "Map",
              "ItemReader": {
                "Resource": "arn:aws:states:::s3:listObjectsV2",
                "Parameters": {
                  "Bucket.$": "$.bucket",
                  "Prefix.$": "$.prefix"
                }
              },
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "OutputPath": "$.[0]",
              "InputPath": "$.response"
            },
            "create_cluster": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$",
                "FunctionName": "${createCluster.Arn}"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Next": "wait cluster job",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "wait cluster job": {
              "Type": "Wait",
              "Next": "cluster_transform_job",
              "Seconds": 300
            },
            "cluster_transform_job": {
              "Type": "Task",
              "Parameters": {
                "TransformJobName.$": "$.TransformJobName"
              },
              "Resource": "arn:aws:states:::aws-sdk:sagemaker:describeTransformJob",
              "Next": "cluster Job Complete?",
              "Retry": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "BackoffRate": 2,
                  "IntervalSeconds": 1,
                  "MaxAttempts": 2
                }
              ]
            },
            "cluster Job Complete?": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.TransformJobStatus",
                  "StringEquals": "Failed",
                  "Next": "Fail"
                },
                {
                  "Variable": "$.TransformJobStatus",
                  "StringEquals": "Completed",
                  "Next": "process_cluster"
                },
                {
                  "Variable": "$.TransformJobStatus",
                  "StringEquals": "Stopped",
                  "Next": "Fail"
                }
              ],
              "Default": "wait cluster job"
            },
            "process_cluster": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${processCluster.Arn}",
                "Payload": {
                  "inputPath.$": "$.TransformInput.DataSource.S3DataSource.S3Uri",
                  "outputPath.$": "$.TransformOutput.S3OutputPath"
                }
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException",
                    "States.TaskFailed"
                  ],
                  "IntervalSeconds": 5,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Next": "cluster_or_flow",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "cluster_or_flow": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-cluster*",
                  "Next": "wait cluster job"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-flow*",
                  "Next": "create_flow"
                }
              ],
              "Default": "Fail"
            },
            "create_flow": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$",
                "FunctionName": "${createFlow.Arn}"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Next": "Wait flow job",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "Wait flow job": {
              "Type": "Wait",
              "Next": "flow_transform_job",
              "Seconds": 300
            },
            "flow_transform_job": {
              "Type": "Task",
              "Parameters": {
                "TransformJobName.$": "$.TransformJobName"
              },
              "Resource": "arn:aws:states:::aws-sdk:sagemaker:describeTransformJob",
              "Next": "flow Job Complete?",
              "Retry": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "BackoffRate": 2,
                  "IntervalSeconds": 1,
                  "MaxAttempts": 2
                }
              ]
            },
            "flow Job Complete?": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.TransformJobStatus",
                  "StringEquals": "Failed",
                  "Next": "Fail"
                },
                {
                  "Variable": "$.TransformJobStatus",
                  "StringEquals": "Completed",
                  "Next": "process_flow"
                },
                {
                  "Variable": "$.TransformJobStatus",
                  "StringEquals": "Stopped",
                  "Next": "Fail"
                }
              ],
              "Default": "Wait flow job"
            },
            "process_flow": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${processFlow.Arn}",
                "Payload": {
                  "inputPath.$": "$.TransformInput.DataSource.S3DataSource.S3Uri",
                  "outputPath.$": "$.TransformOutput.S3OutputPath"
                }
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Next": "create_campaign",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "create_campaign": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$",
                "FunctionName": "${createCampaign.Arn}"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "Next": "Success"
            },
            "Success": {
              "Type": "Succeed"
            },
            "Fail": {
              "Type": "Fail"
            }
          }
        }

  StepFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/vendedlogs/states/${AWS::StackName}_step_function

  ExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ecs-tasks.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy
      - arn:aws:iam::aws:policy/AWSMarketplaceMeteringRegisterUsage

  EcsInstanceExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role

  EcsInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
      - !Ref EcsInstanceExecutionRole

  TaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - LoadBalancer
    Properties:
      Family: !Ref ServiceAndTaskDefinitionName
      Cpu: !Ref Cpu
      Memory: !Ref Memory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt ExecutionRole.Arn
      TaskRoleArn: !GetAtt ExecutionRole.Arn
      ContainerDefinitions:
      - Name: bastet_db
        Image: postgres:13-bullseye
        Cpu: 0
        PortMappings: []
        Essential: false
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_DB
          Value: bastet
        MountPoints:
        - SourceVolume: postgres_data
          ContainerPath: /var/lib/postgresql/data/
          ReadOnly: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${ECSClusterName}
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_redis
        Image: redis:6.2.6-alpine
        Cpu: 0
        PortMappings:
        - Name: bastet_redis-6379-tcp
          ContainerPort: 6379
          HostPort: 6379
          Protocol: tcp
        Essential: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${ECSClusterName}
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_rabbitmq
        Image: rabbitmq:alpine
        Cpu: 0
        PortMappings:
        - Name: bastet_rabbitmq-5672-tcp
          ContainerPort: 5672
          HostPort: 5672
          Protocol: tcp
        Essential: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${ECSClusterName}
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_web
        Image: !Ref WebContainerImage
        Cpu: 0
        PortMappings:
        - Name: bastet_web-8000-tcp
          ContainerPort: 8000
          HostPort: 8000
          Protocol: tcp
        Essential: false
        Command:
        - ./entry.sh
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: bastet.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_EMAIL
          Value: !Ref SuperuserEmail
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_USERNAME
          Value: !Ref SuperuserUsername
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_PASSWORD
          Value: !Ref SuperuserPassword
        - Name: POSTGRES_DB_NAME
          Value: bastet
        - Name: AWS_REGION
          Value: !Ref AWS::Region
        MountPoints:
        - SourceVolume: static_volume
          ContainerPath: /code/static
          ReadOnly: false
        - SourceVolume: media_volume
          ContainerPath: /code/media
          ReadOnly: false
        - SourceVolume: shared_temp
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_db
          Condition: START
        - ContainerName: bastet_redis
          Condition: START
        - ContainerName: bastet_rabbitmq
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${ECSClusterName}
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_celery
        Image: !Ref WebContainerImage
        Cpu: 0
        Essential: false
        Command:
        - celery
        - -A
        - bastet
        - worker
        - -l
        - info
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: bastet.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: POSTGRES_DB_NAME
          Value: bastet
        MountPoints:
        - SourceVolume: shared_temp
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_db
          Condition: START
        - ContainerName: bastet_redis
          Condition: START
        - ContainerName: bastet_rabbitmq
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${ECSClusterName}
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_nginx
        Image: !Ref NginxContainerImage
        Cpu: 0
        PortMappings:
        - Name: bastet_nginx-80-tcp
          ContainerPort: 80
          HostPort: 80
          Protocol: tcp
        Essential: true
        MountPoints:
        - SourceVolume: static_volume
          ContainerPath: /home/app/web/staticfiles
          ReadOnly: false
        - SourceVolume: media_volume
          ContainerPath: /home/app/web/mediafiles
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_web
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${ECSClusterName}
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      Volumes:
      - Name: postgres_data
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: static_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: media_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: shared_temp
        DockerVolumeConfiguration:
          Scope: task
          Driver: local

  EcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - TaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Ref ServiceAndTaskDefinitionName
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt TaskDefinition.TaskDefinitionArn
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: bastet_nginx
        ContainerPort: 80
        TargetGroupArn: !GetAtt TargetGroup.TargetGroupArn

  LoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - TargetGroup
    Properties:
      Name: !Ref LoadBalancerName
      Subnets:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      SecurityGroups:
      - !Ref SecurityGroup

  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: !Ref LoadBalancerName
      VpcId: !Ref VPC
      Port: 80
      Protocol: HTTP
      HealthCheckProtocol: HTTP
      HealthCheckPort: 80
      HealthCheckPath: /
      Matcher:
        HttpCode: 200-399
      TargetType: instance

  Listener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt LoadBalancer.LoadBalancerArn
      Port: 8000
      Protocol: HTTP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt TargetGroup.TargetGroupArn

  ECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref LatestECSOptimizedAMI
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        InstanceType: !Ref ECSClusterInstanceType
        IamInstanceProfile:
          Arn: !GetAtt EcsInstanceProfile.Arn
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Ref ECSClusterName

  ECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref ECSLaunchTemplate
        Version: !GetAtt ECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Ref ECSClusterName

  ECSCluster:
    Type: AWS::ECS::Cluster
    DependsOn:
    - EcsInstanceProfile
    Properties:
      ClusterName: !Ref ECSClusterName
      ClusterSettings:
      - Name: containerInsights
        Value: disabled
      Configuration:
        ExecuteCommandConfiguration:
          Logging: DEFAULT
      ServiceConnectDefaults:
        Namespace: !Ref ECSClusterName
      Tags: []
  ECSLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /ecs/${ECSClusterName}

  EC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref ECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  ClusterCPAssociation:
    Type: AWS::ECS::ClusterCapacityProviderAssociations
    DependsOn:
    - ECSCluster
    Properties:
      Cluster: !Ref ECSClusterName
      CapacityProviders:
      - !Ref EC2CapacityProvider
      DefaultCapacityProviderStrategy:
      - Base: 0
        Weight: 1
        CapacityProvider: !Ref EC2CapacityProvider

  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCidr
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
      - Key: Name
        Value: !Sub "${VPCNamePrefix}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
      - Key: Name
        Value: !Sub "${VPCNamePrefix}-InternetGateway"

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    DependsOn:
    - VPC
    - InternetGateway
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet1Cidr
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [0, !GetAZs ""]
      Tags:
      - Key: Name
        Value: !Sub "${VPCNamePrefix}-PublicSubnet1"

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet2Cidr
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [1, !GetAZs ""]
      Tags:
      - Key: Name
        Value: !Sub "${VPCNamePrefix}-PublicSubnet2"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: !Sub "${VPCNamePrefix}-PublicRouteTable"

  Route:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation1:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref RouteTable

  SubnetRouteTableAssociation2:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref RouteTable

  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Allow HTTP
      VpcId: !Ref VPC
      SecurityGroupIngress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      SecurityGroupEgress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      Tags:
      - Key: Name
        Value: !Sub "${VPCNamePrefix}-PublicSecurityGroup"

Outputs:
  LoadBalancerDNSName:
    Description: The DNS name of the load balancer
    Value: !Sub "http://${LoadBalancer.DNSName}:8000"
