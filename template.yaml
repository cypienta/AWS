Transform: AWS::Serverless-2016-10-31
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:

    - Label:
        default: Pipeline components
      Parameters:
      - DisableTech
      - DisableSequencer

    - Label:
        default: UI admin credentials
      Parameters:
      - SuperuserEmail
      - SuperuserUsername
      - SuperuserPassword

    - Label:
        default: Airflow admin credentials
      Parameters:
      - AirflowUsername
      - AirflowPassword

    - Label:
        default: Load Balancer Configuration
      Parameters:
      - UILoadBalancerName
      - DbLoadBalancerName
      - AirflowLoadBalancerName

    - Label:
        default: Input processing Parameters
      Parameters:
      - FlowInputMaxClusters
      - TechAPIURL

    - Label:
        default: ECS AMIs
      Parameters:
      - CpuECSOptimizedAMI
      - DbECSOptimizedAMI
      - GpuECSOptimizedAMI

    - Label:
        default: UI ECS Configuration
      Parameters:
      - UITaskMemory
      - UIECSClusterInstanceType

    - Label:
        default: Database ECS Configuration
      Parameters:
      - DBTaskMemory
      - DbECSClusterInstanceType

    - Label:
        default: Cluster model Part 1 ECS Configuration
      Parameters:
      - ClusterPart1ECSClusterInstanceType
      - ClusterPart1TaskMemory

    - Label:
        default: Flow model ECS Configuration
      Parameters:
      - FlowECSClusterInstanceType
      - FlowTaskMemory

    - Label:
        default: Lambda function ECS Configuration
      Parameters:
      - LambdaECSClusterInstanceType
      - LambdaTaskMemory

    - Label:
        default: Airflow ECS Configuration
      Parameters:
      - AirflowECSClusterInstanceType
      - AirflowTaskMemory

    - Label:
        default: Auto Scaling Configuration
      Parameters:
      - ClusterAutoScalingMinSize
      - ClusterAutoScalingMaxSize

    - Label:
        default: VPC Configuration
      Parameters:
      - VpcCidr
      - Subnet1Cidr
      - Subnet2Cidr

    - Label:
        default: Database credentials
      Parameters:
      - DbUsername
      - DbPassword

Parameters:

  DisableTech:
    Type: String
    Description: Disable technique classificaiton
    Default: 'false'
    AllowedValues:
    - 'true'
    - 'false'

  DisableSequencer:
    Type: String
    Description: Disable sequencer model
    Default: 'false'
    AllowedValues:
    - 'true'
    - 'false'

  TechAPIURL:
    Type: String
    Description: Technique classifier API URL
    Default: http://tech-api-e592-1773406980.us-west-1.elb.amazonaws.com:5000
    MinLength: 5
    AllowedPattern: ^(http:\/\/|https:\/\/)((\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\-]*[a-zA-Z0-9])\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\-]*[A-Za-z0-9]))(:(?:\d{1,5}))?$

  DbUsername:
    Type: String
    Description: Database Username for lambda functions
    Default: lambda
    MinLength: 5

  DbPassword:
    Type: String
    Description: Database Passwrod for lambda functions
    Default: lambda
    MinLength: 5

  FlowInputMaxClusters:
    Type: Number
    Description: Number of clusters as input to Flow detection model to be processed
      at a time for an input file
    Default: 5000

  UIECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for EC2
    Default: m5.xlarge
    AllowedValues:
    - t2.large
    - t3.large
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  DbECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Database EC2
    Default: m5.large
    AllowedValues:
    - t2.large
    - t3.large
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  ClusterPart1ECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Cluster model part 1 EC2. Choose a
      CPU instance type
    Default: c5.2xlarge
    AllowedValues:
    - c5.2xlarge
    - c5.4xlarge
    - c5.9xlarge
    - c5.12xlarge
    - c5.18xlarge
    - c5.24xlarge
    - c4.2xlarge
    - c4.4xlarge
    - c4.8xlarge

  FlowECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Technique model EC2. Choose a CPU instance
      type
    Default: c5.2xlarge
    AllowedValues:
    - c5.2xlarge
    - c5.4xlarge
    - c5.9xlarge
    - c5.12xlarge
    - c5.18xlarge
    - c5.24xlarge
    - c4.2xlarge
    - c4.4xlarge
    - c4.8xlarge

  LambdaECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for EC2
    Default: t3a.large
    AllowedValues:
    - t2.large
    - t3.large
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  AirflowECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Airflow.
    Default: t3.xlarge
    AllowedValues:
    - t2.xlarge
    - t2.2xlarge
    - t3.xlarge
    - t3.2xlarge
    - t3a.xlarge
    - t3a.2xlarge
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.medium
    - m6g.xlarge
    - m6g.2xlarge

  UITaskMemory:
    Description: Amount of memory (in MiB) used by the UI and Database task. 1 GB
      = 1024
    Type: String
    Default: 15360
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  DBTaskMemory:
    Description: Amount of memory (in MiB) used by the UI and Database task. 1 GB
      = 1024
    Type: String
    Default: 7168
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterPart1TaskMemory:
    Description: Amount of memory (in MiB) used by the cluster model part 1 task.
      1 GB = 1024
    Type: String
    Default: 15360
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  FlowTaskMemory:
    Description: Amount of memory (in MiB) used by the flow model task. 1 GB = 1024
    Type: String
    Default: 15360
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  LambdaTaskMemory:
    Description: Amount of memory (in MiB) used by the lambda function task. 1 GB
      = 1024
    Type: String
    Default: 7168
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  AirflowTaskMemory:
    Description: Amount of memory (in MiB) used by the airflow. 1 GB = 1024
    Type: String
    Default: 15360
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  SuperuserEmail:
    Description: Email of superuser
    Type: String
    Default: admin@admin.com
    AllowedPattern: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$
    ConstraintDescription: Must be a valid email address

  SuperuserUsername:
    Description: Superuser username. Minimum length of 3.
    Type: String
    Default: cypienta
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  SuperuserPassword:
    Description: Superuser password. Minimum length of 8
    Type: String
    Default: cypienta
    MinLength: 8
    ConstraintDescription: Must be minimum length of 8

  AirflowUsername:
    Description: Airflow username. Minimum length of 3.
    Type: String
    Default: cypienta
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  AirflowPassword:
    Description: Airflow password. Minimum length of 8
    Type: String
    Default: cypienta
    MinLength: 8
    ConstraintDescription: Must be minimum length of 8

  VpcCidr:
    Description: The CIDR block for the VPC
    Type: String
    Default: 10.0.0.0/16
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  Subnet1Cidr:
    Description: The CIDR block for the first public subnet
    Type: String
    Default: 10.0.0.0/20
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  Subnet2Cidr:
    Description: The CIDR block for the second public subnet
    Type: String
    Default: 10.0.16.0/20
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  UILoadBalancerName:
    Description: Name of UI Load balancer
    Type: String
    Default: cypienta-ui
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  DbLoadBalancerName:
    Description: Name of Database Load balancer
    Type: String
    Default: cypienta-db
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  AirflowLoadBalancerName:
    Description: Name of airflow Load balancer
    Type: String
    Default: cypienta-airflow
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  ClusterAutoScalingMinSize:
    Description: The minimum size of the auto scaling group for the ECS cluster
    Type: String
    Default: 0
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterAutoScalingMaxSize:
    Description: The maximum size of the auto scaling group for the ECS cluster
    Type: String
    Default: 5
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  CpuECSOptimizedAMI:
    Description: AMI ID for UI task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/recommended/image_id

  DbECSOptimizedAMI:
    Description: AMI ID for DB task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux/recommended/image_id

  GpuECSOptimizedAMI:
    Description: AMI ID for Database task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/gpu/recommended/image_id

Mappings:
  Images:
    LambdaContainerImage:
      ImageUri: >-
        709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-lambda-function:v0.10.0.3
    LambdaAggContainerImage:
      ImageUri: >-
        709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-lambda-function-agg:v0.10.0.1
    AirflowContainerImage:
      ImageUri: >-
        709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-airflow:v0.10.0.1
    ClusterModelPart1ContainerImage:
      ImageUri: >-
        709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-cluster-part-1:v0.10.1.2
    FlowModelContainerImage:
      ImageUri: >-
        709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-flow-detector:v0.10.1.2
    WebContainerImage:
      ImageUri: >-
        709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-ui:v0.10.0.1
    NginxContainerImage:
      ImageUri: >-
        709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/pipeline-ui-nginx:v0.9.1.1

  UI:
    ServiceWorkerUsername:
      Value: >-
        service_worker
    ServiceWorkerPassword:
      Value: >-
        service_worker_admin

    OTEL-EXPORTER-OTLP-ENDPOINT:
      Value: >-
        http://localhost:4317

  Lambda:
    UI-ENTITIES-TABLE-FILE-PATH:
      Value: >-
        scratch/user_node_feature.json

    NODE-FEATURE-FILE:
      Value: >-
        node_feature/node_feature.json

    CLUSTER-CONFIG-PREFIX:
      Value: >-
        clustering_agent/

    VOLUME-SERVICE-PLATFORM:
      Value: >-
        AWS

    ATTRIBUTE-WEIGHTS-FILE:
      Value: >-
        scratch/attribute_weights.json

  RegionMap:
    af-south-1:
      lambdaLayer: arn:aws:lambda:af-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-3:336392948345:layer:AWSSDKPandas-Python311:12
    ap-south-1:
      lambdaLayer: arn:aws:lambda:ap-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ca-central-1:
      lambdaLayer: arn:aws:lambda:ca-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-central-1:
      lambdaLayer: arn:aws:lambda:eu-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-north-1:
      lambdaLayer: arn:aws:lambda:eu-north-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-1:
      lambdaLayer: arn:aws:lambda:eu-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-2:
      lambdaLayer: arn:aws:lambda:eu-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-3:
      lambdaLayer: arn:aws:lambda:eu-west-3:336392948345:layer:AWSSDKPandas-Python311:12
    sa-east-1:
      lambdaLayer: arn:aws:lambda:sa-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-1:
      lambdaLayer: arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-2:
      lambdaLayer: arn:aws:lambda:us-east-2:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-1:
      lambdaLayer: arn:aws:lambda:us-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-2:
      lambdaLayer: arn:aws:lambda:us-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-east-1:
      lambdaLayer: arn:aws:lambda:ap-east-1:839552336658:layer:AWSSDKPandas-Python311:14
    ap-south-2:
      lambdaLayer: arn:aws:lambda:ap-south-2:246107603503:layer:AWSSDKPandas-Python311:13
    ap-southeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-3:258944054355:layer:AWSSDKPandas-Python311:14
    ap-southeast-4:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-4:945386623051:layer:AWSSDKPandas-Python311:13
    eu-central-2:
      lambdaLayer: arn:aws:lambda:eu-central-2:956415814219:layer:AWSSDKPandas-Python311:13
    eu-south-1:
      lambdaLayer: arn:aws:lambda:eu-south-1:774444163449:layer:AWSSDKPandas-Python311:14
    eu-south-2:
      lambdaLayer: arn:aws:lambda:eu-south-2:982086096842:layer:AWSSDKPandas-Python311:13
    il-central-1:
      lambdaLayer: arn:aws:lambda:il-central-1:263840725265:layer:AWSSDKPandas-Python311:12
    me-central-1:
      lambdaLayer: arn:aws:lambda:me-central-1:593833071574:layer:AWSSDKPandas-Python311:12
    me-south-1:
      lambdaLayer: arn:aws:lambda:me-south-1:938046470361:layer:AWSSDKPandas-Python311:14
    cn-north-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-north-1:406640652441:layer:AWSSDKPandas-Python311:10
    cn-northwest-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-northwest-1:406640652441:layer:AWSSDKPandas-Python311:10

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DependsOn:
    - EcsInstanceExecutionRole
    - EcsInstanceProfile
    Properties:
      BucketName: !Sub
      - cypienta-${StackSuffix}
      - StackSuffix:
          Fn::Select:
          - 2
          - Fn::Split:
            - /
            - !Ref AWS::StackId

  CloudFormationRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: cloudformation.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-AllowECS"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ecs:*
            - ec2:*
            - ecr:*
            - autoscaling:*
            - elasticloadbalancing:*
            - cloudformation:*
            - iam:*
            - lambda:*
            - logs:*
            - route53:*
            - s3:*
            - servicediscovery:*
            - ssm:GetParameters
            - cloudwatch:*
            - events:*
            Resource: '*'

  LambdaRole:
    Type: AWS::IAM::Role
    DependsOn: CloudFormationRole
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-AllowECS"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ecs:*
            - cloudformation:*
            Resource: '*'
      - PolicyName: !Sub "${AWS::StackName}-IAM-passrole"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - iam:PassRole
            Resource: !GetAtt CloudFormationRole.Arn
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonS3FullAccess
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  autoUpdateStack:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_auto_update_stack
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: autoUpdateStack
      InlineCode: |
        import subprocess
        import sys
        import os
        import json
        import time

        import boto3
        from botocore.exceptions import ClientError
        import requests
        import requests.adapters
        from requests.auth import HTTPBasicAuth

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        os.makedirs("/tmp/nltk_download", exist_ok=True)
        subprocess.call('pip install ruamel.yaml -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        from ruamel.yaml import YAML


        class S3_service():
            '''
            S3 service class
            '''
            def __init__(self):
                '''
                Initialize the S3 service
                '''
                self.s3 = boto3.client('s3')
                self.retry_limit = 3
                self.retry_delay = 10

            def check_file_exists(self, bucket, key):
                '''
                Check if the file exists in the S3 bucket
                Parameters:
                    bucket (str): The name of the S3 bucket
                    key (str): The key of the file in the S3 bucket
                Returns:
                    bool: True if the file exists, False otherwise
                '''
                # check if the function parameters are valid
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(key, str):
                    raise ValueError("key must be a string")

                failure = False
                exception = None
                file_exists = False

                for i in range(self.retry_limit):
                    try:
                        try:
                            self.s3.head_object(Bucket=bucket, Key=key)
                            file_exists = True
                        except ClientError as e:
                            if e.response['Error']['Code'] != '404':
                                raise e

                            print(f"File does not exist: {key}")
                            file_exists = False

                        failure = False
                        break
                    except Exception as e:
                        print(f"Error downloading file from S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

                if file_exists:
                    return True

            def upload_file(self, file_path, bucket, key):
                '''
                Upload a file to the S3 bucket
                Parameters:
                    file_path (str): The path to the file to upload
                    bucket (str): The name of the S3 bucket
                    key (str): The key of the file in the S3 bucket
                '''
                # check if the function parameters are valid
                if not isinstance(file_path, str):
                    raise ValueError("file_path must be a string")
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(key, str):
                    raise ValueError("key must be a string")

                failure = False
                exception = None

                print(f"Uploading file to S3: {file_path} to {bucket}/{key}")

                for i in range(self.retry_limit):
                    try:
                        self.s3.upload_file(file_path, bucket, key)
                        failure = False
                        break
                    except Exception as e:
                        print(f"Error uploading file to S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

            def download_file(self, bucket, key, file_path):
                '''
                Download a file from the S3 bucket
                Parameters:
                    bucket (str): The name of the S3 bucket
                    key (str): The key of the file in the S3 bucket
                    file_path (str): The path to the file to download
                '''
                # check if the function parameters are valid
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(key, str):
                    raise ValueError("key must be a string")
                if not isinstance(file_path, str):
                    raise ValueError("file_path must be a string")

                failure = False
                exception = None

                print(f"Downloading file from S3: {bucket}/{key} to {file_path}")

                for i in range(self.retry_limit):
                    try:
                        self.s3.download_file(bucket, key, file_path)
                        failure = False
                        break
                    except Exception as e:
                        print(f"Error downloading file from S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

            def delete_file(self, Bucket, Key):
                '''
                Delete a file from the S3 bucket
                Parameters:
                    Bucket (str): The name of the S3 bucket
                    Key (str): The key of the file in the S3 bucket
                '''
                # check if the function parameters are valid
                if not isinstance(Bucket, str):
                    raise ValueError("Bucket must be a string")
                if not isinstance(Key, str):
                    raise ValueError("Key must be a string")

                failure = False
                exception = None

                for i in range(self.retry_limit):
                    try:
                        self.s3.delete_object(Bucket=Bucket, Key=Key)
                        failure = False
                        break
                    except Exception as e:
                        print(f"Error downloading file from S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

            def list_files(self, bucket, prefix):
                '''
                List files in the S3 bucket
                Parameters:
                    bucket (str): The name of the S3 bucket
                    prefix (str): The prefix of the files in the S3 bucket
                '''
                # check if the function parameters are valid
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(prefix, str):
                    raise ValueError("prefix must be a string")

                failure = False
                exception = None

                for i in range(self.retry_limit):
                    try:
                        response = self.s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
                        failure = False
                        break
                    except Exception as e:
                        print(f"Error downloading file from S3: {e}")
                        print(f"Retry in {self.retry_delay} seconds")
                        exception = e
                        failure = True
                        time.sleep(self.retry_delay)

                if failure:
                    raise exception

                return response

            def clean_objects_with_prefix(self, bucket, prefix):
                '''
                Clean up the S3 bucket
                Parameters:
                    bucket (str): The name of the S3 bucket
                    prefix (str): The prefix of the files in the S3 bucket
                '''
                # check if the function parameters are valid
                if not isinstance(bucket, str):
                    raise ValueError("bucket must be a string")
                if not isinstance(prefix, str):
                    raise ValueError("prefix must be a string")

                func = "clean_s3_prefix"

                print(f"{func}: Clean up folder: {prefix}")

                try:

                    response = self.list_files(bucket, prefix)

                    if 'Contents' in response:
                        for item in response['Contents']:
                            file_key = item['Key']

                            print(f"{func}: Deleting file: {file_key}")

                            if file_key[-1] == '/':
                                continue

                            self.delete_file(bucket, file_key)
                except Exception as e:
                    print(f"{func}: Failed to clean up scratch folder: {e}")
                    return


        class CloudFormation_service():
            '''
            CloudFormation service class
            '''
            def __init__(self):
                '''
                Initialize the CloudFormation service
                '''
                self.cf_client = boto3.client('cloudformation')
                self.retry_limit = 3
                self.retry_delay = 10

            def update_stack(self, stack_name, template_url, role_arn):
                '''
                Update the stack
                Args:
                    template_url (str): The URL of the template to update the stack with
                '''
                if not isinstance(template_url, str):
                    raise ValueError("template_url must be a string")

                parameters = []
                capabilities = [
                    'CAPABILITY_IAM',
                    'CAPABILITY_NAMED_IAM',
                    'CAPABILITY_AUTO_EXPAND'
                ]

                # Update the stack
                try:
                    response = self.cf_client.update_stack(
                        StackName=stack_name,
                        TemplateURL=template_url,
                        RoleARN=role_arn,
                        Capabilities=capabilities,
                        Parameters=parameters,
                    )
                    print(f"Stack update initiated successfully. Response: {json.dumps(response, indent=4)}")
                except ClientError as e:
                    if "No updates are to be performed" in str(e):
                        print("No changes detected in the stack.")
                    else:
                        print(f"Error updating stack: {e}")
                        raise e

            def get_stack_status(self, stack_name):
                '''
                Get the stack status
                Args:
                    stack_name (str): The name of the stack to get the status of
                Returns:
                    str: The status of the stack
                '''
                try:
                    response = self.cf_client.describe_stacks(StackName=stack_name)
                    return response['Stacks'][0]['StackStatus']
                except Exception as e:
                    print(f"Error getting stack status: {e}")
                    raise e

            def get_stack_resources(self, stack_name):
                '''
                Get the stack resources
                '''
                try:
                    response = self.cf_client.describe_stack_resources(StackName=stack_name)

                    return response['StackResources']
                except Exception as e:
                    print(f"Error getting stack resources: {e}")
                    raise e


        class Ecs_service():
            '''
            ECS service class
            '''
            def __init__(self):
                '''
                Initialize the ECS service
                '''
                self.ecs_client = boto3.client('ecs')
                self.retry_limit = 3
                self.retry_delay = 10

            def describe_service(self, cluster, services):
                '''
                Describe the service
                '''
                try:
                    response = self.ecs_client.describe_services(cluster=cluster, services=services)
                    return response['services']
                except Exception as e:
                    print(f"Error describing service: {e}")
                    raise e

            def describe_task_definition(self, task_definition):
                '''
                Describe the task definition
                '''
                try:
                    response = self.ecs_client.describe_task_definition(taskDefinition=task_definition)
                    return response
                except Exception as e:
                    print(f"Error describing task definition: {e}")
                    raise e

            def service_list_tasks(self, cluster, service):
                '''
                List the tasks for a service
                '''
                try:
                    response = self.ecs_client.list_tasks(cluster=cluster, serviceName=service, desiredStatus="RUNNING")
                    return response
                except Exception as e:
                    print(f"Error listing tasks: {e}")
                    raise e

            def describe_tasks(self, cluster, task_arns):
                '''
                Describe the tasks
                '''
                try:
                    response = self.ecs_client.describe_tasks(cluster=cluster, tasks=task_arns)
                    return response
                except Exception as e:
                    print(f"Error describing tasks: {e}")
                    raise e

            def update_service(self, cluster, service, task_definition, force_update=True):
                '''
                Update the service
                '''
                try:
                    response = self.ecs_client.update_service(
                        cluster=cluster,
                        serviceName=service,
                        taskDefinition=task_definition,
                        forceNewDeployment=force_update,
                    )
                    return response
                except Exception as e:
                    print(f"Error updating service: {e}")
                    raise e


        # Replace with your target repository details
        OWNER = os.getenv("OWNER")
        REPO = os.getenv("REPO")
        BRANCH = os.getenv("BRANCH")
        BUCKET = os.getenv("BUCKET")
        STACK_NAME = os.getenv("STACK_NAME")
        TEMPLATE_REPO_FILEPATH = os.getenv("TEMPLATE_REPO_FILEPATH")
        UI_LB_URL = os.getenv('UI_LB_URL')
        UI_USERNAME = os.getenv("UI_USERNAME")
        UI_PASSWORD = os.getenv("UI_PASSWORD")
        AIRFLOW_BASE_URL = os.getenv('AIRFLOW_BASE_URL')
        AIRFLOW_USERNAME = os.getenv('AIRFLOW_USERNAME')
        AIRFLOW_PASSWORD = os.getenv('AIRFLOW_PASSWORD')
        ROLE_ARN = os.getenv('ROLE_ARN')

        missing_variables = []

        if not OWNER:
            missing_variables.append("OWNER")
        if not REPO:
            missing_variables.append("REPO")
        if not BRANCH:
            missing_variables.append("BRANCH")
        if not BUCKET:
            missing_variables.append("BUCKET")
        if not STACK_NAME:
            missing_variables.append("STACK_NAME")
        if not TEMPLATE_REPO_FILEPATH:
            missing_variables.append("TEMPLATE_REPO_FILEPATH")
        if not UI_LB_URL:
            missing_variables.append("UI_LB_URL")
        if not UI_USERNAME:
            missing_variables.append("UI_USERNAME")
        if not UI_PASSWORD:
            missing_variables.append("UI_PASSWORD")
        if not AIRFLOW_BASE_URL:
            missing_variables.append("AIRFLOW_BASE_URL")
        if not ROLE_ARN:
            missing_variables.append("ROLE_ARN")

        if missing_variables:
            print(f"Missing variables: {missing_variables}")
            raise ValueError(f"Missing variables: {missing_variables}")

        S3_SERVICE = S3_service()
        CLOUDFORMATION_SERVICE = CloudFormation_service()
        ECS_SERVICE = Ecs_service()

        SCRATCH_DIR = "scratch"

        TEMP_FILE_PREFIX = "/tmp/lambda"
        os.makedirs(TEMP_FILE_PREFIX, exist_ok=True)

        TEMPLATE_REPO_COMMIT_FILEPATH = "repo/repo_commit.json"
        TEMP_TEMPLATE_REPO_COMMIT_FILEPATH = f"{TEMP_FILE_PREFIX}/repo_commit.json"

        TEMPLATE_FILEPATH = "repo/template.yaml"
        TEMP_TEMPLATE_FILEPATH = f"{TEMP_FILE_PREFIX}/template.yaml"


        def lambda_handler(event, context):

            latest_commit = get_latest_commit()
            if latest_commit is None:
                print("Failed to get the latest commit from the target branch.")
                return

            print(latest_commit)

            sha = latest_commit["sha"]
            message = latest_commit["commit"]["message"]
            author = latest_commit["commit"]["author"]["name"]
            date = latest_commit["commit"]["author"]["date"]

            file_exists = fetch_file_if_present(BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH, TEMP_TEMPLATE_REPO_COMMIT_FILEPATH)

            is_update_stack = True

            if file_exists:
                with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'r') as f:
                    template_repo_commit_json = json.load(f)

                # if the stack statuc on current sha is success, then no need to update the stack
                # if the stack status on current sha is failure, then it had failed to update the stack,
                # keep it in stable state as rolledback. Mark the current commit as failure.
                # if the stack status on current sha is anything else, then check for the current status of the stack and do the updates accordingly.
                if template_repo_commit_json["sha"] == sha:
                    print("No new commits. No need to update the stack.")
                    if template_repo_commit_json["stack_status"] == "success":
                        print("Stack is in success state. No need to update the stack.")
                        return
                    elif template_repo_commit_json["stack_status"] == "failed":
                        print("Stack is in failed state. Rolled back to the previous workingstate.")
                        return
                    else:
                        print("Stack status is unknown. Please check the stack status.")
                        is_update_stack = False

            else:
                print("No repo commit file found. Creating a new one.")
                print(f"sha: {sha}")

                # As this is a first check on the repo, we assume that the current commit is the latest used for stack
                # adding init status as success that the current commit id is in success state with stack.

                template_repo_commit_json = {
                    "sha": sha,
                    "message": message,
                    "author": author,
                    "date": date,
                    "stack_status": "success",
                }

                with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'w') as f:
                    json.dump(template_repo_commit_json, f)

                S3_SERVICE.upload_file(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH)

                return

            if is_update_stack:
                print("New commit found. Update stack.")
                update_stack(latest_commit)
            else:
                print("No new commit found. Check current stack status.")
                check_stack_update_status()


        def update_stack(latest_commit):
            '''
            Update the stack
            Args:
                latest_commit (dict): The latest commit from the target branch
            '''
            func = "update_stack"

            print(f"{func}: Update current stack: {STACK_NAME}")

            url = f"https://raw.githubusercontent.com/{OWNER}/{REPO}/{BRANCH}/{TEMPLATE_REPO_FILEPATH}"
            headers = {}

            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()

                file_content = response.text

                with open(TEMP_TEMPLATE_FILEPATH, "w", encoding="utf-8") as file:
                    file.write(file_content)
                print(f"Template file saved at: {TEMP_TEMPLATE_FILEPATH}")

            except Exception as e:
                print(f"{func}: Failed to get the template from the repo: {e}")
                raise e

            # update the template file on s3
            S3_SERVICE.upload_file(TEMP_TEMPLATE_FILEPATH, BUCKET, TEMPLATE_FILEPATH)

            template_url = f"https://{BUCKET}.s3.amazonaws.com/{TEMPLATE_FILEPATH}"

            print(f"Template URL: {template_url}")

            # pause the pipeline
            print(f"{func}: Pause the pipeline on Airflow")
            payload = {
                "is_paused": True
            }
            api_airflow_patch_dag(AIRFLOW_BASE_URL, AIRFLOW_USERNAME, AIRFLOW_PASSWORD, "file_polling", payload)

            # check statuses of dags on Airflow
            print(f"{func}: Check statuses of dags on Airflow")
            can_update_stack = check_pipelines_completion_status(AIRFLOW_BASE_URL, AIRFLOW_USERNAME, AIRFLOW_PASSWORD)

            if can_update_stack:
                print(f"{func}: Update stack")
                CLOUDFORMATION_SERVICE.update_stack(STACK_NAME, template_url, ROLE_ARN)
            else:
                print(f"{func}: Cannot update stack yet")

            print("Update the repo commit file")
            template_repo_commit_json = {
                "sha": latest_commit["sha"],
                "message": latest_commit["commit"]["message"],
                "author": latest_commit["commit"]["author"]["name"],
                "date": latest_commit["commit"]["author"]["date"],
                "stack_status": "update_in_progress",
            }

            with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'w') as f:
                json.dump(template_repo_commit_json, f)

            S3_SERVICE.upload_file(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH)


        def check_stack_update_status():
            '''
            Check the stack update status
            '''
            stack_operation_status = check_stack_operation(STACK_NAME, "update")

            template_repo_commit_json = json.load(open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'r'))

            if stack_operation_status is None:
                print("Stack update is still in progress")
                return

            if stack_operation_status:
                print("Stack update completed successfully")
                template_repo_commit_json["stack_status"] = "success"
                check_current_task_running()
            else:
                print("Stack update failed")
                template_repo_commit_json["stack_status"] = "failed"

            print("Resume the pipeline on Airflow")
            payload = {
                "is_paused": False
            }
            api_airflow_patch_dag(AIRFLOW_BASE_URL, AIRFLOW_USERNAME, AIRFLOW_PASSWORD, "file_polling", payload)

            with open(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, 'w') as f:
                json.dump(template_repo_commit_json, f)

            S3_SERVICE.upload_file(TEMP_TEMPLATE_REPO_COMMIT_FILEPATH, BUCKET, TEMPLATE_REPO_COMMIT_FILEPATH)


        def check_stack_operation(stack_name, operation):
            """
            Checks cloud formation stack to exit with either complete for failed state for a given operation

            Args:
                stack_name: Name of cloud formation stack
                operation: Type of stack operation (create/update)

            Returns: None - if the stack is still in progress
                    True - if the stack is in success state
                    False - if the stack is in failed state

            """
            print(f"Waiting for stack {operation} to complete...")

            status = CLOUDFORMATION_SERVICE.get_stack_status(stack_name)

            if status.endswith('_ROLLBACK_COMPLETE'):
                print(f"Stack {operation} failed with status: {status}")
                return False
            if status.endswith('_COMPLETE'):
                print(f"Stack {operation} completed with status: {status}")
                return status == f'{operation.upper()}_COMPLETE'

            print(f"Current stack status: {status}")
            return None


        def check_pipelines_completion_status(airflow_url, username, password):
            """
            Check the status of all pipelines

            Args:
                airflow_url (str): The DNS name of the Airflow instance
                username (str): The username for the Airflow instance
                password (str): The password for the Airflow instance

            Returns:
                bool: True if all dag runs are either in failed or success state, False otherwise
            """
            dags_to_monitor = [
                'aggregator',
                'clustering',
                'file_polling',
                'schema_matcher_and_classifier',
                'sequencer',
                'save_feedback',
            ]

            dag_runs = {}  # Track runs for each DAG

            print("Checking DAG statuses...")

            # Check for new runs and update existing ones
            for dag_id in dags_to_monitor:
                active_runs = api_airflow_check_dag_status(airflow_url, username, password, dag_id)

                if active_runs:
                    if dag_id not in dag_runs:
                        dag_runs[dag_id] = {}

                    for run_status in active_runs:
                        run_id = run_status['run_id']

                        # If this is a new run we haven't seen before
                        if run_id not in dag_runs[dag_id]:
                            dag_runs[dag_id][run_id] = run_status
                            print(f"New run detected for {dag_id} - Run ID: {run_id}")

                        # Update the status for this run
                        dag_runs[dag_id][run_id] = run_status

                        # log current status
                        print(f"{dag_id} (Run: {run_id}): {run_status['status']}")

            # Check if all runs are complete
            all_complete = True
            active_runs_exist = False
            has_failures = False

            for dag_id in dags_to_monitor:
                if dag_id in dag_runs:
                    for run_id, run_status in dag_runs[dag_id].items():
                        active_runs_exist = True
                        current_status = api_airflow_dag_run_status(airflow_url, username, password, dag_id, run_id)

                        if current_status:
                            dag_runs[dag_id][run_id] = current_status

                            # if the dag id is file_polling, then paused dag may have queued statuses which is valid status for all completed
                            if dag_id == "file_polling" and current_status['status'] not in ['success', 'failed', 'queued']:
                                all_complete = False

                            elif current_status['status'] not in ['success', 'failed']:
                                all_complete = False

                            if current_status['status'] == 'failed':
                                has_failures = True

            # log summary of all runs
            print("\nCurrent DAG Run Summary:")
            for dag_id in dags_to_monitor:
                if dag_id in dag_runs:
                    for run_id, run_status in dag_runs[dag_id].items():
                        print(f"{dag_id} - Run {run_id}: {run_status['status']}")

            # If we have active runs and they're all complete, we're done
            if active_runs_exist and all_complete:
                if has_failures:
                    print("\nAll DAG runs completed, but some failed!")
                else:
                    print("\nAll DAG runs completed successfully!")
                return True

            elif not active_runs_exist:
                print("\nNo active runs found. Exiting...")
                return True

            return False


        def check_current_task_running():
            '''
            Check that current task are running with new image
            '''
            # check current template file on s3
            S3_SERVICE.download_file(BUCKET, TEMPLATE_FILEPATH, TEMP_TEMPLATE_FILEPATH)

            # Initialize ruamel.yaml YAML handler
            yaml = YAML()

            # Read the YAML file
            with open(TEMP_TEMPLATE_FILEPATH, 'r') as f:
                yaml_data = yaml.load(f)

            new_images_yaml = yaml_data["Mappings"]["Images"]

            new_images_keys = {
                "airflow": new_images_yaml["AirflowContainerImage"]["ImageUri"],
                "ui": new_images_yaml["WebContainerImage"]["ImageUri"],
            }

            print("Get tasks currently running on the stack")

            service_task_arns = get_service_task_arns()

            task_updated, task_to_update = check_task_image_update(service_task_arns, new_images_keys)

            if task_updated:
                print("Tasks are updated")
            else:
                print("Tasks are not updated. Force update the tasks")
                force_update_tasks(service_task_arns, task_to_update)


        def get_service_task_arns():
            '''
            Get the service, task arns from the stack
            Returns:
                dict: A dictionary containing the service and task arns
            '''

            service_task_arns = {
                "ui": {},
                "airflow": {},
            }

            try:
                # Describe stack resources
                resources = CLOUDFORMATION_SERVICE.get_stack_resources(STACK_NAME)

                # Loop through resources to find ECS Cluster
                for resource in resources:

                    if resource["LogicalResourceId"] == "UIEcsService":
                        service_task_arns["ui"]["service"] = resource["PhysicalResourceId"]

                    elif resource["LogicalResourceId"] == "AirflowEcsService":
                        service_task_arns["airflow"]["service"] = resource["PhysicalResourceId"]

                    elif resource["LogicalResourceId"] == "UITaskDefinition":
                        service_task_arns["ui"]["task_definition"] = resource["PhysicalResourceId"]

                    elif resource["LogicalResourceId"] == "AirflowTaskDefinition":
                        service_task_arns["airflow"]["task_definition"] = resource["PhysicalResourceId"]

                    elif resource["LogicalResourceId"] == "ECSCluster":
                        service_task_arns["cluster"] = resource["PhysicalResourceId"]

                return service_task_arns

            except Exception as e:
                raise e


        def check_task_image_update(service_task_arns, new_images_keys):
            '''
            Check if the task image is updated
            '''

            # check for UI service
            print("Check if UI service is updated")

            ui_service = ECS_SERVICE.describe_service(service_task_arns["cluster"], [service_task_arns["ui"]["service"]])
            service_task_def = ui_service[0]["taskDefinition"]

            if service_task_def == service_task_arns["ui"]["task_definition"]:
                print("UI service is updated")
            else:
                print("UI service is not updated")
                return False, "ui"

            print("Check UI task definition images")
            ui_task_definition = ECS_SERVICE.describe_task_definition(service_task_arns["ui"]["task_definition"])

            for container in ui_task_definition["taskDefinition"]["containerDefinitions"]:
                if container["name"] == "bastet_web":
                    if container["image"] == new_images_keys["ui"]:
                        print("UI task definition image is updated")
                    else:
                        print("UI task definition image is not updated")
                        return False, "ui"

            print("List service tasks")
            service_list_tasks = ECS_SERVICE.service_list_tasks(service_task_arns["cluster"], service_task_arns["ui"]["service"])

            if len(service_list_tasks["taskArns"]) <= 0:
                print("No tasks are running for the UI service")
                return False, "ui"

            tasks_response = ECS_SERVICE.describe_tasks(service_task_arns["cluster"], service_list_tasks["taskArns"])

            ui_task_updated = False
            for task in tasks_response["tasks"]:
                if task["taskDefinitionArn"] == service_task_arns["ui"]["task_definition"]:
                    print("UI task is updated")
                    ui_task_updated = True
                    break

            if not ui_task_updated:
                print("UI task is not updated")
                return False, "ui"

            # check for Airflow service
            print("Check if the Airflow service is updated")

            airflow_service = ECS_SERVICE.describe_service(service_task_arns["cluster"], [service_task_arns["airflow"]["service"]])
            service_task_def = airflow_service[0]["taskDefinition"]

            if service_task_def == service_task_arns["airflow"]["task_definition"]:
                print("Airflow service is updated")
            else:
                print("Airflow service is not updated")
                return False, "airflow"

            print("Check Airflow task definition images")
            airflow_task_definition = ECS_SERVICE.describe_task_definition(service_task_arns["airflow"]["task_definition"])

            for container in airflow_task_definition["taskDefinition"]["containerDefinitions"]:
                if container["name"] == "airflow-webserver":
                    if container["image"] == new_images_keys["airflow"]:
                        print("Airflow task definition image is updated")
                    else:
                        print("Airflow task definition image is not updated")
                        return False, "airflow"

            print("List service tasks")
            service_list_tasks = ECS_SERVICE.service_list_tasks(service_task_arns["cluster"], service_task_arns["airflow"]["service"])

            if len(service_list_tasks["taskArns"]) <= 0:
                print("No tasks are running for the Airflow service")
                return False, "airflow"

            tasks_response = ECS_SERVICE.describe_tasks(service_task_arns["cluster"], service_list_tasks["taskArns"])

            airflow_task_updated = False
            for task in tasks_response["tasks"]:
                if task["taskDefinitionArn"] == service_task_arns["airflow"]["task_definition"]:
                    print("Airflow task is updated")
                    airflow_task_updated = True
                    break

            if not airflow_task_updated:
                print("Airflow task is not updated")
                return False, "airflow"

            return True, None


        def force_update_tasks(service_task_arns, task_to_update):
            '''
            Force update the tasks
            '''
            print(f"Force update the {task_to_update} task")

            if task_to_update == "ui":
                ECS_SERVICE.update_service(service_task_arns["cluster"],
                                           service_task_arns["ui"]["service"],
                                           service_task_arns["ui"]["task_definition"])

            elif task_to_update == "airflow":
                ECS_SERVICE.update_service(service_task_arns["cluster"],
                                           service_task_arns["airflow"]["service"],
                                           service_task_arns["airflow"]["task_definition"])


        def get_latest_commit():
            '''
            Get the latest commit from the target branch
            Returns:
                commit: dict
            '''
            url = f"https://api.github.com/repos/{OWNER}/{REPO}/commits?sha={BRANCH}"
            headers = {}

            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()

                commits = response.json()
                commit = commits[0]

                return commit
            except Exception as e:
                print(f"Error fetching commits: {e}")

            return None


        def fetch_file_if_present(bucket, volume_file_key, temp_file_path):
            '''
            Check if the file exists on S3. Download if its present
            Args:
                bucket (string): bucket name
                volume_file_key (string): File key path to the cluster ticket output in volume
                temp_file_path (string): local file path to download the cluster ticket output to

            Returns:
                bool: Boolean value false, if there is no such file, true if there is a file
            '''

            if not isinstance(bucket, str):
                raise ValueError("bucket must be a string")
            if not isinstance(volume_file_key, str):
                raise ValueError("volume_file_key must be a string")
            if not isinstance(temp_file_path, str):
                raise ValueError("temp_file_path must be a string")

            func = "fetch_file_if_present"

            print(
                f"{func}: Checking if file exists on S3 path {bucket}/{volume_file_key}")

            file_exists = S3_SERVICE.check_file_exists(bucket, volume_file_key)

            if not file_exists:
                print(f"{func}: File does not exist on S3. Return False")
                return False

            else:
                print(f"{func}: File exists on S3. Downloading file")
                S3_SERVICE.download_file(bucket, volume_file_key, temp_file_path)
                print(f"{func}: File downloaded")
                return True


        def api_airflow_patch_dag(airflow_url, username, password, dag_id, payload):
            '''
            Pause the dag
            Args:
                airflow_url (string): Airflow URL
                username (string): Airflow username
                password (string): Airflow password
            '''
            func = "api_airflow_pause_dag"

            headers = {"Content-Type": "application/json"}

            try:
                url = f"{airflow_url}/api/v1/dags/{dag_id}"
                response = requests.patch(
                    url,
                    auth=HTTPBasicAuth(username, password),
                    headers=headers,
                    json=payload
                )
                response.raise_for_status()
                print(f"{func}: Pipeline patched")
            except Exception as e:
                print(f"{func}: Failed to patch the pipeline: {e}")
                raise e


        def api_airflow_check_dag_status(airflow_url, username, password, dag_id):
            """
            Check the status of a DAG in Airflow

            Args:
                airflow_url (str): The URL of the Airflow instance
                username (str): The username for the Airflow instance
                password (str): The password for the Airflow instance
                dag_id (str): The ID of the DAG to check

            Returns:
                list: A list of dictionaries containing the status of each DAG run
            """
            base_url = f"{airflow_url}/api/v1"

            headers = {"Content-Type": "application/json"}

            try:
                response = requests.get(
                    f"{base_url}/dags/{dag_id}/dagRuns",
                    auth=HTTPBasicAuth(username, password),
                    headers=headers,
                )

                response.raise_for_status()

                dag_runs = response.json()['dag_runs']
                # Return all runs, not just active ones
                runs = []
                for run in dag_runs:
                    runs.append({
                        'dag_id': dag_id,
                        'status': run['state'],
                        'start_date': run['start_date'],
                        'end_date': run['end_date'],
                        'run_id': run['dag_run_id']
                    })
                return runs
            except Exception as e:
                print(f"Error connecting to Airflow API for DAG {dag_id}: {e}")
                raise e


        def api_airflow_dag_run_status(airflow_url, username, password, dag_id, run_id):
            """
            Check the current status of a specific DAG run

            Args:
                airflow_url (str): The DNS name of the Airflow instance
                username (str): The username for the Airflow instance
                password (str): The password for the Airflow instance
                dag_id (str): The ID of the DAG to check
                run_id (str): The ID of the DAG run to check

            Returns:
                dict: A dictionary containing the status of the DAG run
            """
            base_url = f"{airflow_url}/api/v1"

            headers = {'Content-Type': 'application/json'}

            try:
                response = requests.get(
                    f"{base_url}/dags/{dag_id}/dagRuns/{run_id}",
                    auth=HTTPBasicAuth(username, password),
                    headers=headers,
                )

                response.raise_for_status()

                run = response.json()
                return {
                    'dag_id': dag_id,
                    'status': run['state'],
                    'start_date': run['start_date'],
                    'end_date': run['end_date'],
                    'run_id': run['dag_run_id']
                }
            except Exception as e:
                print(f"Error checking status for DAG {dag_id} run {run_id}: {e}")
                raise e

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          OWNER: Nidheesh-Panchal
          REPO: temp
          BRANCH: v0.10.1
          BUCKET: !Ref Bucket
          STACK_NAME: !Ref AWS::StackName
          TEMPLATE_REPO_FILEPATH: template-v0.10.1-local.yaml
          UI_LB_URL: !Sub "http://${UILoadBalancer.DNSName}:8000"
          UI_USERNAME: !FindInMap [UI, ServiceWorkerPassword, Value]
          UI_PASSWORD: !FindInMap [UI, ServiceWorkerUsername, Value]
          AIRFLOW_BASE_URL: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
          AIRFLOW_USERNAME: !Ref AirflowUsername
          AIRFLOW_PASSWORD: !Ref AirflowPassword
          ROLE_ARN: !GetAtt CloudFormationRole.Arn
      Events:
        Schedule1:
          Type: Schedule
          Properties:
            Schedule: rate(10 minutes)

  autoUpdateStackLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${autoUpdateStack}

  autoUpdateStackLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref autoUpdateStack
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  EcsModelExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ecs-tasks.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-ecsModelPolicy"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:*
            - states:*
            - logs:*
            - iam:PassRole
            - ecs:*
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy
      - arn:aws:iam::aws:policy/AWSMarketplaceMeteringRegisterUsage

  EcsInstanceExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-RexrayPolicy"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ec2:AttachVolume
            - ec2:CreateVolume
            - ec2:CreateSnapshot
            - ec2:CreateTags
            - ec2:DeleteVolume
            - ec2:DeleteSnapshot
            - ec2:DescribeAvailabilityZones
            - ec2:DescribeInstances
            - ec2:DescribeVolumes
            - ec2:DescribeVolumeAttribute
            - ec2:DescribeVolumeStatus
            - ec2:DescribeSnapshots
            - ec2:CopySnapshot
            - ec2:DescribeSnapshotAttribute
            - ec2:DetachVolume
            - ec2:ModifySnapshotAttribute
            - ec2:ModifyVolumeAttribute
            - ec2:DescribeTag
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role

  EcsInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
      - !Ref EcsInstanceExecutionRole

  # UI ECS
  UITaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - UILoadBalancer
    Properties:
      Family: !Sub ui-task-${AWS::StackName}
      Memory: !Ref UITaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      PlacementConstraints:
      - Type: memberOf
        Expression: !Sub
        - attribute:ecs.availability-zone==${AvailabilityZone}
        - AvailabilityZone: !Select
          - '0'
          - Fn::GetAZs: !Ref AWS::Region
      ContainerDefinitions:
      - Name: bastet_db
        Image: postgres:13-bullseye
        Cpu: 0
        PortMappings: []
        Essential: true
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_DB
          Value: bastet
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_ui_postgres_data"
          ContainerPath: /var/lib/postgresql/data/
          ReadOnly: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_redis
        Image: redis:6.2.6-alpine
        Cpu: 0
        PortMappings: []
        Essential: true
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_rabbitmq
        Image: rabbitmq:alpine
        Cpu: 0
        PortMappings:
        - Name: bastet_rabbitmq-5672-tcp
          ContainerPort: 5672
          HostPort: 5672
          Protocol: tcp
        Essential: true
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_web
        Image: !FindInMap [Images, WebContainerImage, ImageUri]
        Cpu: 0
        PortMappings:
        - Name: bastet_web-8000-tcp
          ContainerPort: 8000
          HostPort: 8000
          Protocol: tcp
        Essential: true
        Command:
        - ./scripts/entry.sh
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: config.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_EMAIL
          Value: !Ref SuperuserEmail
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_USERNAME
          Value: !Ref SuperuserUsername
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_PASSWORD
          Value: !Ref SuperuserPassword
        - Name: POSTGRES_DB_NAME
          Value: bastet
        - Name: AWS_REGION
          Value: !Ref AWS::Region
        - Name: MAX_USER_FIELD_MAPPING
          Value: 20
        - Name: AWS_STORAGE_BUCKET_NAME
          Value: !Ref Bucket
        - Name: GEN_AI_MODEL
          Value: gpt-3.5-turbo
        - Name: AIRFLOW_URL
          Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
        - Name: AIRFLOW_DAG_ID
          Value: file_polling
        - Name: AIRFLOW_USERNAME
          Value: !Ref AirflowUsername
        - Name: AIRFLOW_PASSWORD
          Value: !Ref AirflowPassword
        - Name: OTEL_EXPORTER_OTLP_ENDPOINT
          Value: !FindInMap [UI, OTEL-EXPORTER-OTLP-ENDPOINT, Value]
        MountPoints:
        - SourceVolume: static_volume
          ContainerPath: /code/static
          ReadOnly: false
        - SourceVolume: media_volume
          ContainerPath: /code/media
          ReadOnly: false
        - SourceVolume: config_volume
          ContainerPath: /code/otel_config
          ReadOnly: false
        - SourceVolume: !Sub "${AWS::StackName}_shared_temp"
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_db
          Condition: START
        - ContainerName: bastet_redis
          Condition: START
        - ContainerName: bastet_rabbitmq
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_celery
        Image: !FindInMap [Images, WebContainerImage, ImageUri]
        Cpu: 0
        Essential: true
        Command:
        - celery
        - -A
        - config
        - worker
        - -l
        - info
        - --concurrency=8
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: config.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: POSTGRES_DB_NAME
          Value: bastet
        - Name: AIRFLOW_URL
          Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
        - Name: AIRFLOW_DAG_ID
          Value: file_polling
        - Name: AIRFLOW_USERNAME
          Value: !Ref AirflowUsername
        - Name: AIRFLOW_PASSWORD
          Value: !Ref AirflowPassword
        - Name: OTEL_EXPORTER_OTLP_ENDPOINT
          Value: !FindInMap [UI, OTEL-EXPORTER-OTLP-ENDPOINT, Value]
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_shared_temp"
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_web
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_celery_beat
        Image: !FindInMap [Images, WebContainerImage, ImageUri]
        Cpu: 0
        Essential: true
        Command:
        - celery
        - -A
        - config
        - beat
        - -l
        - info
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: config.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: POSTGRES_DB_NAME
          Value: bastet
        - Name: AIRFLOW_URL
          Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
        - Name: AIRFLOW_DAG_ID
          Value: file_polling
        - Name: AIRFLOW_USERNAME
          Value: !Ref AirflowUsername
        - Name: AIRFLOW_PASSWORD
          Value: !Ref AirflowPassword
        - Name: OTEL_EXPORTER_OTLP_ENDPOINT
          Value: !FindInMap [UI, OTEL-EXPORTER-OTLP-ENDPOINT, Value]
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_shared_temp"
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_celery
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_nginx
        Image: !FindInMap [Images, NginxContainerImage, ImageUri]
        Cpu: 0
        PortMappings:
        - Name: bastet_nginx-80-tcp
          ContainerPort: 80
          HostPort: 80
          Protocol: tcp
        Essential: true
        MountPoints:
        - SourceVolume: static_volume
          ContainerPath: /home/app/web/staticfiles
          ReadOnly: false
        - SourceVolume: media_volume
          ContainerPath: /home/app/web/mediafiles
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_celery
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet-opentelemetry
        Image: otel/opentelemetry-collector-contrib:0.122.0
        Cpu: 0
        PortMappings:
        - Name: otel-4317-grpc
          ContainerPort: 4317
          HostPort: 4317
          Protocol: tcp
        - Name: otel-4318-tcp
          ContainerPort: 4318
          HostPort: 4318
          Protocol: tcp
        Essential: false
        Command:
        - --config
        - /code/otel_config/otel-collector-config.yaml
        Environment:
        - Name: STACK_NAME
          Value: !Sub ${AWS::StackName}
        MountPoints:
        - SourceVolume: config_volume
          ContainerPath: /code/otel_config
        DependsOn:
        - ContainerName: bastet_web
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        SystemControls: []
        VolumesFrom: []
      Volumes:
      - Name: !Sub "${AWS::StackName}_ui_postgres_data"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '30'
            volumetype: gp2
      - Name: static_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: media_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: config_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: !Sub "${AWS::StackName}_shared_temp"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2

  UIEcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - UITaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Sub ui-service-${ECSCluster}
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt UITaskDefinition.TaskDefinitionArn
      CapacityProviderStrategy:
      - CapacityProvider: !Ref UIEC2CapacityProvider
        Base: 0
        Weight: 1
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: bastet_nginx
        ContainerPort: 80
        TargetGroupArn: !GetAtt UITargetGroup.TargetGroupArn

  UILoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - UITargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${UILoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      Subnets:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      SecurityGroups:
      - !Ref SecurityGroup

  UITargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${UILoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      VpcId: !Ref VPC
      Port: 80
      Protocol: HTTP
      HealthCheckProtocol: HTTP
      HealthCheckPort: 80
      HealthCheckPath: /
      Matcher:
        HttpCode: 200-399
      TargetType: instance

  UIListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt UILoadBalancer.LoadBalancerArn
      Port: 8000
      Protocol: HTTP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt UITargetGroup.TargetGroupArn

  UIECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref DbECSOptimizedAMI
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        InstanceType: !Ref UIECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        UserData:
          Fn::Base64:
            Fn::Sub: |
              #!/bin/bash
              yum install -y aws-cfn-bootstrap
              /opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource UIECSLaunchTemplate
              /opt/aws/bin/cfn-signal -e $? --region ${AWS::Region} --stack ${AWS::StackName} --resource UIECSAutoScalingGroup
              exec 2>>/var/log/ecs/ecs-agent-install.log
              set -x
              until curl -s http://localhost:51678/v1/metadata
              do
                sleep 1
              done
              docker plugin install rexray/ebs REXRAY_PREEMPT=true EBS_REGION=${AWS::Region} --grant-all-permissions
              stop ecs
              start ecs

    Metadata:
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              aws-cli: []
              jq: []
              ecs-init: []
          commands:
            01_add_instance_to_cluster:
              command: !Sub echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config
            02_start_ecs_agent:
              command: start ecs
          files:
            /etc/cfn/cfn-hup.conf:
              mode: 256
              owner: root
              group: root
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.UIECSLaunchTemplate.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource UIECSLaunchTemplate
          services:
            sysvinit:
              cfn-hup:
                enabled: 'true'
                ensureRunning: 'true'
                files:
                - /etc/cfn/cfn-hup.conf
                - /etc/cfn/hooks.d/cfn-auto-reloader.conf

  UIECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 1
      LaunchTemplate:
        LaunchTemplateId: !Ref UIECSLaunchTemplate
        Version: !GetAtt UIECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      AvailabilityZones:
      - !Select
        - '0'
        - Fn::GetAZs: !Ref AWS::Region
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-ui-instance

  UIEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn:
    - UIECSAutoScalingGroup
    - MyWaitCondition
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref UIECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # DB ECS
  DbTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub db-task-${AWS::StackName}
      Memory: !Ref DBTaskMemory
      NetworkMode: host
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      RequiresCompatibilities:
      - EC2
      PlacementConstraints:
      - Type: memberOf
        Expression: !Sub
        - attribute:ecs.availability-zone==${AvailabilityZone}
        - AvailabilityZone: !Select
          - '0'
          - Fn::GetAZs: !Ref AWS::Region
      ContainerDefinitions:
      - Name: lambda_db
        Image: postgres:13-bullseye
        Cpu: 0
        PortMappings:
        - Name: lambda_db-5432-tcp
          ContainerPort: 5432
          HostPort: 5432
          Protocol: tcp
        Essential: true
        Environment:
        - Name: POSTGRES_USER
          Value: !Ref DbUsername
        - Name: POSTGRES_PASSWORD
          Value: !Ref DbPassword
        - Name: POSTGRES_DB
          Value: lambda
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_postgres_data"
          ContainerPath: /var/lib/postgresql/data/
          ReadOnly: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      Volumes:
      - Name: !Sub "${AWS::StackName}_postgres_data"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2

  DbEcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - DbTaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Sub db-service-${ECSCluster}
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt DbTaskDefinition.TaskDefinitionArn
      CapacityProviderStrategy:
      - CapacityProvider: !Ref DbEC2CapacityProvider
        Base: 0
        Weight: 1
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: lambda_db
        ContainerPort: 5432
        TargetGroupArn: !GetAtt DbTargetGroup.TargetGroupArn

  DbLoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - DbTargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${DbLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      Type: network
      Scheme: internet-facing
      Subnets:
      - !Ref PublicSubnet1
      SecurityGroups:
      - !Ref SecurityGroup

  DbTargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${DbLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      VpcId: !Ref VPC
      Port: 5432
      Protocol: TCP
      HealthCheckProtocol: TCP
      HealthCheckPort: 5432
      TargetType: instance

  DbListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt DbLoadBalancer.LoadBalancerArn
      Port: 5432
      Protocol: TCP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt DbTargetGroup.TargetGroupArn

  DbECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref DbECSOptimizedAMI
        InstanceType: !Ref DbECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        UserData:
          Fn::Base64:
            Fn::Sub: |
              #!/bin/bash
              yum install -y aws-cfn-bootstrap
              /opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSLaunchTemplate
              /opt/aws/bin/cfn-signal -e $? --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSAutoScalingGroup
              exec 2>>/var/log/ecs/ecs-agent-install.log
              set -x
              until curl -s http://localhost:51678/v1/metadata
              do
                sleep 1
              done
              docker plugin install rexray/ebs REXRAY_PREEMPT=true EBS_REGION=${AWS::Region} --grant-all-permissions
              stop ecs
              start ecs
      # Optionally, you can specify a LaunchTemplateName or other properties as needed

    Metadata:
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              aws-cli: []
              jq: []
              ecs-init: []
          commands:
            01_add_instance_to_cluster:
              command: !Sub echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config
            02_start_ecs_agent:
              command: start ecs
          files:
            /etc/cfn/cfn-hup.conf:
              mode: 256
              owner: root
              group: root
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.DbECSLaunchTemplate.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSLaunchTemplate
          services:
            sysvinit:
              cfn-hup:
                enabled: 'true'
                ensureRunning: 'true'
                files:
                - /etc/cfn/cfn-hup.conf
                - /etc/cfn/hooks.d/cfn-auto-reloader.conf

  DbECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 1
      LaunchTemplate:
        LaunchTemplateId: !Ref DbECSLaunchTemplate
        Version: !GetAtt DbECSLaunchTemplate.LatestVersionNumber
      # LaunchConfigurationName: !Ref DbECSLaunchConfiguration
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      AvailabilityZones:
      - !Select
        - '0'
        - Fn::GetAZs: !Ref AWS::Region
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-db-instance

  DbEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn:
    - DbECSAutoScalingGroup
    - MyWaitCondition
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref DbECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Cluster part 1 Model ECS
  ClusterPart1TaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub cluster-model-part-1-task-${AWS::StackName}
      Memory: !Ref ClusterPart1TaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !FindInMap [Images, ClusterModelPart1ContainerImage, ImageUri]
        Cpu: 0
        Essential: true
        EntryPoint:
        - ./main
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: NVIDIA_DRIVER_CAPABILITIES
          Value: all
        - Name: VOLUME_SERVICE_PLATFORM
          Value: !FindInMap [Lambda, VOLUME-SERVICE-PLATFORM, Value]
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  ClusterPart1ECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        InstanceType: !Ref ClusterPart1ECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '60'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
            echo ECS_ENABLE_GPU_SUPPORT=true >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  ClusterPart1ECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref ClusterPart1ECSLaunchTemplate
        Version: !GetAtt ClusterPart1ECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-cluster-1-instance

  ClusterPart1EC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn:
    - ClusterPart1ECSAutoScalingGroup
    - MyWaitCondition1
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref ClusterPart1ECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Flow Model ECS
  FlowTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub flow-model-task-${AWS::StackName}
      Memory: !Ref FlowTaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !FindInMap [Images, FlowModelContainerImage, ImageUri]
        Cpu: 0
        Essential: true
        EntryPoint:
        - ./main
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: disable_step_function
          Value: 'true'
        - Name: VOLUME_SERVICE_PLATFORM
          Value: !FindInMap [Lambda, VOLUME-SERVICE-PLATFORM, Value]
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  FlowECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        InstanceType: !Ref FlowECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  FlowECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref FlowECSLaunchTemplate
        Version: !GetAtt FlowECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-flow-instance

  FlowEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn:
    - FlowECSAutoScalingGroup
    - MyWaitCondition1
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref FlowECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Lambda ECS
  LambdaAggTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    Properties:
      Family: !Sub lambda-agg-task-${AWS::StackName}
      Memory: !Ref LambdaTaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: lambda
        Image: !FindInMap [Images, LambdaAggContainerImage, ImageUri]
        Cpu: 0
        Essential: true
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: BUCKET
          Value: !Ref Bucket
        - Name: DB_CONNECTION_STRING
          Value: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/lambda
        - Name: CLUSTER_CONFIG_PREFIX
          Value: !FindInMap [Lambda, CLUSTER-CONFIG-PREFIX, Value]
        - Name: SEQUENCER_CONTEXT_WINDOW_SIZE
          Value: '5000'
        - Name: TECH_API_URL
          Value: !Ref TechAPIURL
        - Name: UI_PASSWORD
          Value: !FindInMap [UI, ServiceWorkerPassword, Value]
        - Name: UI_USERNAME
          Value: !FindInMap [UI, ServiceWorkerUsername, Value]
        - Name: UI_LB_URL
          Value: !Sub "http://${UILoadBalancer.DNSName}:8000"
        - Name: DISABLE_TECH
          Value: !Ref DisableTech
        - Name: UI_ENTITIES_TABLE_FILE_PATH
          Value: !FindInMap [Lambda, UI-ENTITIES-TABLE-FILE-PATH, Value]
        - Name: NODE_FEATURE_FILE
          Value: !FindInMap [Lambda, NODE-FEATURE-FILE, Value]
        - Name: ATTRIBUTE_WEIGHTS_FILE
          Value: !FindInMap [Lambda, ATTRIBUTE-WEIGHTS-FILE, Value]
        - Name: VOLUME_SERVICE_PLATFORM
          Value: !FindInMap [Lambda, VOLUME-SERVICE-PLATFORM, Value]
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  LambdaTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    Properties:
      Family: !Sub lambda-task-${AWS::StackName}
      Memory: !Ref LambdaTaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: lambda
        Image: !FindInMap [Images, LambdaContainerImage, ImageUri]
        Cpu: 0
        Essential: true
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: BUCKET
          Value: !Ref Bucket
        - Name: DB_CONNECTION_STRING
          Value: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/lambda
        - Name: CLUSTER_CONFIG_PREFIX
          Value: !FindInMap [Lambda, CLUSTER-CONFIG-PREFIX, Value]
        - Name: SEQUENCER_CONTEXT_WINDOW_SIZE
          Value: !Ref FlowInputMaxClusters
        - Name: TECH_API_URL
          Value: !Ref TechAPIURL
        - Name: UI_PASSWORD
          Value: !FindInMap [UI, ServiceWorkerPassword, Value]
        - Name: UI_USERNAME
          Value: !FindInMap [UI, ServiceWorkerUsername, Value]
        - Name: UI_LB_URL
          Value: !Sub "http://${UILoadBalancer.DNSName}:8000"
        - Name: DISABLE_TECH
          Value: !Ref DisableTech
        - Name: UI_ENTITIES_TABLE_FILE_PATH
          Value: !FindInMap [Lambda, UI-ENTITIES-TABLE-FILE-PATH, Value]
        - Name: NODE_FEATURE_FILE
          Value: !FindInMap [Lambda, NODE-FEATURE-FILE, Value]
        - Name: ATTRIBUTE_WEIGHTS_FILE
          Value: !FindInMap [Lambda, ATTRIBUTE-WEIGHTS-FILE, Value]
        - Name: VOLUME_SERVICE_PLATFORM
          Value: !FindInMap [Lambda, VOLUME-SERVICE-PLATFORM, Value]
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  LambdaECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        InstanceType: !Ref LambdaECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  LambdaECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref LambdaECSLaunchTemplate
        Version: !GetAtt LambdaECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-lambda-instance

  LambdaEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn:
    - LambdaECSAutoScalingGroup
    - MyWaitCondition2
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref LambdaECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Airflow ECS
  AirflowTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - AirflowLoadBalancer
    Properties:
      Family: !Sub airflow-task-${AWS::StackName}
      TaskRoleArn: !Ref EcsModelExecutionRole
      ExecutionRoleArn: !Ref EcsModelExecutionRole
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      Memory: !Ref AirflowTaskMemory
      PlacementConstraints:
      - Type: memberOf
        Expression: !Sub
        - attribute:ecs.availability-zone==${AvailabilityZone}
        - AvailabilityZone: !Select
          - '0'
          - Fn::GetAZs: !Ref AWS::Region
      Volumes:
      - Name: !Sub "${AWS::StackName}_airflow_postgres-volume"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '30'
            volumetype: gp2
      - Name: !Sub "${AWS::StackName}_airflow_redis-volume"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2
      - Name: !Sub "${AWS::StackName}_logs_volume"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '30'
            volumetype: gp2
            mode: "0644"
      - Name: dags_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: config_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: plugins_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      ContainerDefinitions:
      - Name: postgres
        Image: postgres:13
        Cpu: 0
        PortMappings:
        - Name: postgres-5432-tcp
          ContainerPort: 5432
          HostPort: 5432
          Protocol: tcp
        Essential: true
        Environment:
        - Name: POSTGRES_USER
          Value: airflow
        - Name: POSTGRES_PASSWORD
          Value: airflow
        - Name: POSTGRES_DB
          Value: airflow
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_airflow_postgres-volume"
          ContainerPath: /var/lib/postgresql/data
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD
          - pg_isready
          - -U
          - airflow
          Interval: 10
          Timeout: 5
          Retries: 5
          StartPeriod: 5
        SystemControls: []
        VolumesFrom: []
      - Name: redis
        Image: redis:latest
        Cpu: 0
        PortMappings:
        - Name: redis-6379-tcp
          ContainerPort: 6379
          HostPort: 6379
          Protocol: tcp
        Essential: true
        Environment: []
        VolumesFrom: []
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_airflow_redis-volume"
          ContainerPath: /data
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD
          - redis-cli
          - ping
          Interval: 10
          Timeout: 5
          Retries: 5
          StartPeriod: 10
        SystemControls: []
      - Name: airflow-opentelemetry
        Image: otel/opentelemetry-collector-contrib:0.122.0
        Cpu: 0
        PortMappings:
        - Name: otel-4317-grpc
          ContainerPort: 4317
          HostPort: 4317
          Protocol: tcp
        - Name: otel-4318-tcp
          ContainerPort: 4318
          HostPort: 4318
          Protocol: tcp
        Essential: false
        Command:
        - --config
        - /opt/airflow/config/otel-collector-config.yaml
        Environment:
        - Name: STACK_NAME
          Value: !Sub ${AWS::StackName}
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /var/log/app
        - SourceVolume: config_volume
          ContainerPath: /opt/airflow/config
        DependsOn:
        - ContainerName: airflow-init
          Condition: SUCCESS
        - ContainerName: airflow-webserver
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        SystemControls: []
        VolumesFrom: []
      - Name: airflow-webserver
        Image: !FindInMap [Images, AirflowContainerImage, ImageUri]
        Cpu: 0
        PortMappings:
        - Name: airflow-webserver-8080-tcp
          ContainerPort: 8080
          HostPort: 8080
          Protocol: tcp
        Essential: true
        Command:
        - webserver
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS
          Value: airflow_local_settings.LOGGING_CONFIG
        - Name: VARIABLE_JSON_FILE
          Value: /opt/airflow/logs/run_vars.json
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: VOLUME_SERVICE_PLATFORM
          Value: !FindInMap [Lambda, VOLUME-SERVICE-PLATFORM, Value]
        - Name: BUCKET_NAME
          Value: !Ref Bucket
        - Name: INPUT_PREFIX
          Value: input/
        - Name: CLUSTER_CONFIG_PREFIX
          Value: !FindInMap [Lambda, CLUSTER-CONFIG-PREFIX, Value]
        - Name: NODE_FEATURE_FILE
          Value: !FindInMap [Lambda, NODE-FEATURE-FILE, Value]
        - Name: ATTRIBUTE_WEIGHTS_FILE
          Value: !FindInMap [Lambda, ATTRIBUTE-WEIGHTS-FILE, Value]
        - Name: ECS_CLUSTER
          Value: !Ref ECSCluster
        - Name: LAMBDA_AGG_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaAggTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_CONTAINER_NAME
          Value: lambda
        - Name: LAMBDA_CP
          Value: !Ref LambdaEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_CP
          Value: !Ref ClusterPart1EC2CapacityProvider
        - Name: FLOW_MODEL_CP
          Value: !Ref FlowEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: FLOW_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: MODEL_ECS_CONTAINER_NAME
          Value: model
        - Name: DISABLE_SEQUENCER
          Value: !Ref DisableSequencer
        - Name: UI_HOST
          Value: !Sub "http://${UILoadBalancer.DNSName}:8000"
        - Name: UI_USERNAME
          Value: !FindInMap [UI, ServiceWorkerUsername, Value]
        - Name: UI_PASSWORD
          Value: !FindInMap [UI, ServiceWorkerPassword, Value]
        - Name: AIRFLOW_BASE_URL
          Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
        MountPoints:
        - SourceVolume: dags_volume
          ContainerPath: /opt/airflow/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /opt/airflow/logs
        - SourceVolume: config_volume
          ContainerPath: /opt/airflow/config
        - SourceVolume: plugins_volume
          ContainerPath: /opt/airflow/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        - ContainerName: airflow-init
          Condition: SUCCESS
        - ContainerName: airflow-scheduler
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD
          - curl
          - --fail
          - http://localhost:8080/health
          Interval: 30
          Timeout: 10
          Retries: 5
          StartPeriod: 30
        SystemControls: []
      - Name: airflow-scheduler
        Image: !FindInMap [Images, AirflowContainerImage, ImageUri]
        Cpu: 0
        PortMappings: []
        Essential: true
        Command:
        - scheduler
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS
          Value: airflow_local_settings.LOGGING_CONFIG
        - Name: VARIABLE_JSON_FILE
          Value: /opt/airflow/logs/run_vars.json
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: VOLUME_SERVICE_PLATFORM
          Value: !FindInMap [Lambda, VOLUME-SERVICE-PLATFORM, Value]
        - Name: BUCKET_NAME
          Value: !Ref Bucket
        - Name: INPUT_PREFIX
          Value: input/
        - Name: CLUSTER_CONFIG_PREFIX
          Value: !FindInMap [Lambda, CLUSTER-CONFIG-PREFIX, Value]
        - Name: NODE_FEATURE_FILE
          Value: !FindInMap [Lambda, NODE-FEATURE-FILE, Value]
        - Name: ATTRIBUTE_WEIGHTS_FILE
          Value: !FindInMap [Lambda, ATTRIBUTE-WEIGHTS-FILE, Value]
        - Name: ECS_CLUSTER
          Value: !Ref ECSCluster
        - Name: LAMBDA_AGG_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaAggTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_CONTAINER_NAME
          Value: lambda
        - Name: LAMBDA_CP
          Value: !Ref LambdaEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_CP
          Value: !Ref ClusterPart1EC2CapacityProvider
        - Name: FLOW_MODEL_CP
          Value: !Ref FlowEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: FLOW_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: MODEL_ECS_CONTAINER_NAME
          Value: model
        - Name: DISABLE_SEQUENCER
          Value: !Ref DisableSequencer
        - Name: UI_HOST
          Value: !Sub "http://${UILoadBalancer.DNSName}:8000"
        - Name: UI_USERNAME
          Value: !FindInMap [UI, ServiceWorkerUsername, Value]
        - Name: UI_PASSWORD
          Value: !FindInMap [UI, ServiceWorkerPassword, Value]
        - Name: AIRFLOW_BASE_URL
          Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
        MountPoints:
        - SourceVolume: dags_volume
          ContainerPath: /opt/airflow/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /opt/airflow/logs
        - SourceVolume: config_volume
          ContainerPath: /opt/airflow/config
        - SourceVolume: plugins_volume
          ContainerPath: /opt/airflow/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        - ContainerName: airflow-init
          Condition: SUCCESS
        - ContainerName: airflow-worker
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD
          - curl
          - --fail
          - http://localhost:8974/health
          Interval: 30
          Timeout: 10
          Retries: 5
          StartPeriod: 30
        SystemControls: []
      - Name: airflow-worker
        Image: !FindInMap [Images, AirflowContainerImage, ImageUri]
        Cpu: 0
        PortMappings: []
        Essential: true
        Command:
        - celery
        - worker
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: DUMB_INIT_SETSID
          Value: '0'
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS
          Value: airflow_local_settings.LOGGING_CONFIG
        - Name: VARIABLE_JSON_FILE
          Value: /opt/airflow/logs/run_vars.json
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: VOLUME_SERVICE_PLATFORM
          Value: !FindInMap [Lambda, VOLUME-SERVICE-PLATFORM, Value]
        - Name: BUCKET_NAME
          Value: !Ref Bucket
        - Name: INPUT_PREFIX
          Value: input/
        - Name: CLUSTER_CONFIG_PREFIX
          Value: !FindInMap [Lambda, CLUSTER-CONFIG-PREFIX, Value]
        - Name: NODE_FEATURE_FILE
          Value: !FindInMap [Lambda, NODE-FEATURE-FILE, Value]
        - Name: ATTRIBUTE_WEIGHTS_FILE
          Value: !FindInMap [Lambda, ATTRIBUTE-WEIGHTS-FILE, Value]
        - Name: ECS_CLUSTER
          Value: !Ref ECSCluster
        - Name: LAMBDA_AGG_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaAggTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_CONTAINER_NAME
          Value: lambda
        - Name: LAMBDA_CP
          Value: !Ref LambdaEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_CP
          Value: !Ref ClusterPart1EC2CapacityProvider
        - Name: FLOW_MODEL_CP
          Value: !Ref FlowEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: FLOW_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: MODEL_ECS_CONTAINER_NAME
          Value: model
        - Name: DISABLE_SEQUENCER
          Value: !Ref DisableSequencer
        - Name: UI_HOST
          Value: !Sub "http://${UILoadBalancer.DNSName}:8000"
        - Name: UI_USERNAME
          Value: !FindInMap [UI, ServiceWorkerUsername, Value]
        - Name: UI_PASSWORD
          Value: !FindInMap [UI, ServiceWorkerPassword, Value]
        - Name: AIRFLOW_BASE_URL
          Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
        MountPoints:
        - SourceVolume: dags_volume
          ContainerPath: /opt/airflow/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /opt/airflow/logs
        - SourceVolume: config_volume
          ContainerPath: /opt/airflow/config
        - SourceVolume: plugins_volume
          ContainerPath: /opt/airflow/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        - ContainerName: airflow-init
          Condition: SUCCESS
        - ContainerName: airflow-triggerer
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        SystemControls: []
      - Name: airflow-triggerer
        Image: !FindInMap [Images, AirflowContainerImage, ImageUri]
        Cpu: 0
        PortMappings: []
        Essential: true
        Command:
        - triggerer
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS
          Value: airflow_local_settings.LOGGING_CONFIG
        - Name: VARIABLE_JSON_FILE
          Value: /opt/airflow/logs/run_vars.json
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: VOLUME_SERVICE_PLATFORM
          Value: !FindInMap [Lambda, VOLUME-SERVICE-PLATFORM, Value]
        - Name: BUCKET_NAME
          Value: !Ref Bucket
        - Name: INPUT_PREFIX
          Value: input/
        - Name: CLUSTER_CONFIG_PREFIX
          Value: !FindInMap [Lambda, CLUSTER-CONFIG-PREFIX, Value]
        - Name: NODE_FEATURE_FILE
          Value: !FindInMap [Lambda, NODE-FEATURE-FILE, Value]
        - Name: ATTRIBUTE_WEIGHTS_FILE
          Value: !FindInMap [Lambda, ATTRIBUTE-WEIGHTS-FILE, Value]
        - Name: ECS_CLUSTER
          Value: !Ref ECSCluster
        - Name: LAMBDA_AGG_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaAggTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_TASK_DEFINITION
          Value: !GetAtt LambdaTaskDefinition.TaskDefinitionArn
        - Name: LAMBDA_ECS_CONTAINER_NAME
          Value: lambda
        - Name: LAMBDA_CP
          Value: !Ref LambdaEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_CP
          Value: !Ref ClusterPart1EC2CapacityProvider
        - Name: FLOW_MODEL_CP
          Value: !Ref FlowEC2CapacityProvider
        - Name: CLUSTER_1_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
        - Name: FLOW_MODEL_ECS_TASK_DEFINITION
          Value: !GetAtt FlowTaskDefinition.TaskDefinitionArn
        - Name: MODEL_ECS_CONTAINER_NAME
          Value: model
        - Name: DISABLE_SEQUENCER
          Value: !Ref DisableSequencer
        - Name: UI_HOST
          Value: !Sub "http://${UILoadBalancer.DNSName}:8000"
        - Name: UI_USERNAME
          Value: !FindInMap [UI, ServiceWorkerUsername, Value]
        - Name: UI_PASSWORD
          Value: !FindInMap [UI, ServiceWorkerPassword, Value]
        - Name: AIRFLOW_BASE_URL
          Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
        MountPoints:
        - SourceVolume: dags_volume
          ContainerPath: /opt/airflow/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /opt/airflow/logs
        - SourceVolume: config_volume
          ContainerPath: /opt/airflow/config
        - SourceVolume: plugins_volume
          ContainerPath: /opt/airflow/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        - ContainerName: airflow-init
          Condition: SUCCESS
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        HealthCheck:
          Command:
          - CMD-SHELL
          - airflow jobs check --job-type TriggererJob --hostname "$(HOSTNAME)"
          Interval: 30
          Timeout: 10
          Retries: 5
          StartPeriod: 30
        SystemControls: []
      - Name: airflow-init
        Image: !FindInMap [Images, AirflowContainerImage, ImageUri]
        Cpu: 0
        PortMappings: []
        Essential: false
        EntryPoint:
        - /bin/bash
        Command:
        - -c
        - "mkdir -p /opt/airflow/{logs,dags,plugins} && chown -R \"${AIRFLOW_UID}:0\" /opt/airflow/{logs,dags,plugins} && exec /entrypoint airflow version"
        Environment:
        - Name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _AIRFLOW_WWW_USER_CREATE
          Value: 'true'
        - Name: AIRFLOW__CORE__LOAD_EXAMPLES
          Value: 'false'
        - Name: _AIRFLOW_WWW_USER_USERNAME
          Value: !Ref AirflowUsername
        - Name: _AIRFLOW_WWW_USER_PASSWORD
          Value: !Ref AirflowPassword
        - Name: AIRFLOW__API__AUTH_BACKENDS
          Value: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
        - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          Value: postgresql+psycopg2://airflow:airflow@localhost/airflow
        - Name: _PIP_ADDITIONAL_REQUIREMENTS
          Value: ''
        - Name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
          Value: 'true'
        - Name: AIRFLOW_UID
          Value: '50000'
        - Name: AIRFLOW__CORE__FERNET_KEY
          Value: ''
        - Name: _AIRFLOW_DB_MIGRATE
          Value: 'true'
        - Name: AIRFLOW__CELERY__BROKER_URL
          Value: redis://:@localhost:6379/0
        - Name: AIRFLOW__CELERY__RESULT_BACKEND
          Value: db+postgresql://airflow:airflow@localhost/airflow
        - Name: AIRFLOW__CORE__EXECUTOR
          Value: CeleryExecutor
        - Name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          Value: 'false'
        MountPoints:
        - SourceVolume: dags_volume
          ContainerPath: /sources/dags
        - SourceVolume: !Sub "${AWS::StackName}_logs_volume"
          ContainerPath: /sources/logs
        - SourceVolume: config_volume
          ContainerPath: /opt/airflow/config
        - SourceVolume: plugins_volume
          ContainerPath: /sources/plugins
        VolumesFrom: []
        DependsOn:
        - ContainerName: postgres
          Condition: HEALTHY
        - ContainerName: redis
          Condition: HEALTHY
        User: 0:0
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
        SystemControls: []

  AirflowEcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - AirflowTaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Sub airflow-service-${ECSCluster}
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt AirflowTaskDefinition.TaskDefinitionArn
      CapacityProviderStrategy:
      - CapacityProvider: !Ref AirflowEC2CapacityProvider
        Base: 0
        Weight: 1
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: airflow-webserver
        ContainerPort: 8080
        TargetGroupArn: !GetAtt AirflowTargetGroup.TargetGroupArn

  AirflowLoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - AirflowTargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${AirflowLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      Subnets:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      SecurityGroups:
      - !Ref SecurityGroup

  AirflowTargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${AirflowLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 0
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      VpcId: !Ref VPC
      Port: 8080
      Protocol: HTTP
      HealthCheckProtocol: HTTP
      HealthCheckPort: 8080
      HealthCheckPath: /
      HealthCheckTimeoutSeconds: 20
      HealthCheckIntervalSeconds: 60
      HealthyThresholdCount: 2
      UnhealthyThresholdCount: 5
      Matcher:
        HttpCode: 200-399
      TargetType: instance

  AirflowListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt AirflowLoadBalancer.LoadBalancerArn
      Port: 8080
      Protocol: HTTP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt AirflowTargetGroup.TargetGroupArn

  AirflowECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref DbECSOptimizedAMI
        InstanceType: !Ref AirflowECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        UserData:
          Fn::Base64:
            Fn::Sub: |
              #!/bin/bash
              yum install -y aws-cfn-bootstrap
              /opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource AirflowECSLaunchTemplate
              /opt/aws/bin/cfn-signal -e $? --region ${AWS::Region} --stack ${AWS::StackName} --resource AirflowECSAutoScalingGroup
              exec 2>>/var/log/ecs/ecs-agent-install.log
              set -x
              until curl -s http://localhost:51678/v1/metadata
              do
                sleep 1
              done
              docker plugin install rexray/ebs REXRAY_PREEMPT=true EBS_REGION=${AWS::Region} --grant-all-permissions
              stop ecs
              start ecs
      # Optionally, you can specify a LaunchTemplateName or other properties as needed

    Metadata:
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              aws-cli: []
              jq: []
              ecs-init: []
          commands:
            01_add_instance_to_cluster:
              command: !Sub echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config
            02_start_ecs_agent:
              command: start ecs
          files:
            /etc/cfn/cfn-hup.conf:
              mode: 256
              owner: root
              group: root
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.AirflowECSLaunchTemplate.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource AirflowECSLaunchTemplate
          services:
            sysvinit:
              cfn-hup:
                enabled: 'true'
                ensureRunning: 'true'
                files:
                - /etc/cfn/cfn-hup.conf
                - /etc/cfn/hooks.d/cfn-auto-reloader.conf

  AirflowECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 1
      LaunchTemplate:
        LaunchTemplateId: !Ref AirflowECSLaunchTemplate
        Version: !GetAtt AirflowECSLaunchTemplate.LatestVersionNumber
      # LaunchConfigurationName: !Ref DbECSLaunchConfiguration
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      AvailabilityZones:
      - !Select
        - '0'
        - Fn::GetAZs: !Ref AWS::Region
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-airflow-instance

  AirflowEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    DependsOn:
    - AirflowECSAutoScalingGroup
    - MyWaitCondition2
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref AirflowECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  ECSCluster:
    Type: AWS::ECS::Cluster
    DependsOn:
    - EcsInstanceProfile
    Properties:
      ClusterName: !Sub ${AWS::StackName}-cluster
      ClusterSettings:
      - Name: containerInsights
        Value: disabled
      Configuration:
        ExecuteCommandConfiguration:
          Logging: DEFAULT
      ServiceConnectDefaults:
        Namespace: !Sub ${AWS::StackName}-cluster
      Tags: []

  ECSClusterLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /ecs/${AWS::StackName}-cluster

  ClusterCPAssociation:
    Type: AWS::ECS::ClusterCapacityProviderAssociations
    DependsOn:
    - ECSCluster
    Properties:
      Cluster: !Sub ${AWS::StackName}-cluster
      CapacityProviders:
      - !Ref LambdaEC2CapacityProvider
      - !Ref AirflowEC2CapacityProvider
      - !Ref DbEC2CapacityProvider
      - !Ref UIEC2CapacityProvider
      - !Ref ClusterPart1EC2CapacityProvider
      - !Ref FlowEC2CapacityProvider
      DefaultCapacityProviderStrategy:
      - Base: 0
        Weight: 1
        CapacityProvider: !Ref DbEC2CapacityProvider

  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCidr
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-InternetGateway"

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    DependsOn:
    - VPC
    - InternetGateway
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet1Cidr
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [0, !GetAZs ""]
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSubnet1"

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet2Cidr
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [1, !GetAZs ""]
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSubnet2"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicRouteTable"

  Route:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation1:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref RouteTable

  SubnetRouteTableAssociation2:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref RouteTable

  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Allow HTTP
      VpcId: !Ref VPC
      SecurityGroupIngress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      SecurityGroupEgress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSecurityGroup"

  MyWaitHandle:
    Type: AWS::CloudFormation::WaitConditionHandle

  MyWaitCondition:
    Type: AWS::CloudFormation::WaitCondition
    DependsOn:
    - UIECSAutoScalingGroup
    Properties:
      Handle: !Ref MyWaitHandle
      Timeout: '120'  # Time in seconds to wait (e.g., 5 minutes)
      Count: 0  # No signals required, just wait for the timeout

  MyWaitCondition1:
    Type: AWS::CloudFormation::WaitCondition
    DependsOn:
    - UIEC2CapacityProvider
    - DbEC2CapacityProvider
    Properties:
      Handle: !Ref MyWaitHandle
      Timeout: '120'  # Time in seconds to wait (e.g., 5 minutes)
      Count: 0  # No signals required, just wait for the timeout

  MyWaitCondition2:
    Type: AWS::CloudFormation::WaitCondition
    DependsOn:
    - ClusterPart1EC2CapacityProvider
    - FlowEC2CapacityProvider
    Properties:
      Handle: !Ref MyWaitHandle
      Timeout: '120'  # Time in seconds to wait (e.g., 5 minutes)
      Count: 0  # No signals required, just wait for the timeout

Outputs:
  CypientaBucket:
    Description: The bucket created
    Value: !Ref Bucket

  CypientaUI:
    Description: The DNS name of the load balancer
    Value: !Sub "http://${UILoadBalancer.DNSName}:8000"

  CypientaAirflow:
    Description: The DNS name of the load balancer
    Value: !Sub "http://${AirflowLoadBalancer.DNSName}:8080"
