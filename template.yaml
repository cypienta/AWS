Transform: AWS::Serverless-2016-10-31
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
    - Label:
        default: General S3 Parameters
      Parameters:
      - BucketName

    - Label:
        default: Container images
      Parameters:
      - VRLLambdaImage
      - NginxContainerImage
      - WebContainerImage
      - TechModelContainerImage
      - ClusterModelPart1ContainerImage
      - ClusterModelPart2ContainerImage
      - FlowModelContainerImage

    - Label:
        default: UI admin credentials
      Parameters:
      - SuperuserEmail
      - SuperuserUsername
      - SuperuserPassword

    - Label:
        default: Database credentials
      Parameters:
      - DbUsername
      - DbPassword

    - Label:
        default: Load Balancer Configuration
      Parameters:
      - UILoadBalancerName
      - DbLoadBalancerName

    - Label:
        default: Input processing Parameters
      Parameters:
      - ChunkSize
      - FlowInputMaxClusters
      - TechLookupLimit

    - Label:
        default: UI lambda function Parameters
      Parameters:
      - ClusterOrFlowMapToCampaign
      - EventThreshold
      - TacticThreshold
      - MaxClusterCount
      - MaxFlowCount

    - Label:
        default: ECS AMIs
      Parameters:
      - CpuECSOptimizedAMI
      - DbECSOptimizedAMI
      - GpuECSOptimizedAMI

    - Label:
        default: UI ECS Configuration
      Parameters:
      - Cpu
      - Memory
      - UIECSClusterInstanceType

    - Label:
        default: Database ECS Configuration
      Parameters:
      - DbECSClusterInstanceType

    - Label:
        default: Technique model ECS Configuration
      Parameters:
      - TechECSClusterInstanceType
      - TechTaskMemory

    - Label:
        default: Cluster model Part 1 ECS Configuration
      Parameters:
      - ClusterPart1ECSClusterInstanceType
      - ClusterPart1TaskMemory

    - Label:
        default: Cluster model Part 2 ECS Configuration
      Parameters:
      - ClusterPart2ECSClusterInstanceType
      - ClusterPart2TaskMemory

    - Label:
        default: Flow model ECS Configuration
      Parameters:
      - FlowECSClusterInstanceType
      - FlowTaskMemory

    - Label:
        default: Auto Scaling Configuration
      Parameters:
      - ClusterAutoScalingMinSize
      - ClusterAutoScalingMaxSize

    - Label:
        default: VPC Configuration
      Parameters:
      - VpcCidr
      - Subnet1Cidr
      - Subnet2Cidr

Parameters:
  BucketName:
    Type: String
    Description: Name of the S3 bucket to create.
    MinLength: 1

  VRLLambdaImage:
    Description: VRL Lambda ECR container image URI
    Type: String
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+:[a-zA-Z0-9.-]+$

  ChunkSize:
    Type: Number
    Description: Size of single chunk of input to be processed at a time for an input
      file
    Default: 20000

  MaxClusterCount:
    Type: Number
    Description: Maximum number of clusters to keep on UI
    Default: 100000

  MaxFlowCount:
    Type: Number
    Description: Maximum number of flows to keep on UI
    Default: 100000

  TechLookupLimit:
    Type: Number
    Description: Maximum number of alert to techniques to cache
    Default: 100000

  DbUsername:
    Type: String
    Description: Database Username for lambda functions
    Default: lambda
    MinLength: 5

  DbPassword:
    Type: String
    Description: Database Passwrod for lambda functions
    Default: lambda
    MinLength: 5

  FlowInputMaxClusters:
    Type: Number
    Description: Number of clusters as input to Flow detection model to be processed
      at a time for an input file
    Default: 5000

  ClusterOrFlowMapToCampaign:
    Type: String
    Default: cluster
    AllowedValues:
    - cluster
    - flow

  EventThreshold:
    Type: Number
    Default: 2
    Description: Number of events present in the cluster or flow to create campaign
      on UI. Minimum allowed value '1'.
    MinValue: 1

  TacticThreshold:
    Type: Number
    Default: 0
    Description: Number of tactics present in an event to be part of the campaign.
      Minimum allowed value '0'.
    MinValue: 0

  UIECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for EC2
    Default: t3a.large
    AllowedValues:
    - t2.micro
    - t2.small
    - t2.medium
    - t2.large
    - t3.micro
    - t3.small
    - t3.medium
    - t3.large
    - t3a.micro
    - t3a.small
    - t3a.medium
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.medium
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  DbECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Database EC2
    Default: m5.large
    AllowedValues:
    - t2.micro
    - t2.small
    - t2.medium
    - t2.large
    - t3.micro
    - t3.small
    - t3.medium
    - t3.large
    - t3a.micro
    - t3a.small
    - t3a.medium
    - t3a.large
    - m5.large
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - m5a.large
    - m5a.xlarge
    - m5a.2xlarge
    - m5a.4xlarge
    - m6g.medium
    - m6g.large
    - m6g.xlarge
    - m6g.2xlarge

  TechECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Technique model EC2. Choose a GPU instance
      type
    Default: g4dn.xlarge
    AllowedValues:
    - g4dn.xlarge
    - g4dn.2xlarge
    - g4dn.4xlarge
    - g4dn.8xlarge
    - g4dn.12xlarge
    - g4dn.16xlarge
    - p5.48xlarge
    - p3.2xlarge
    - p3.8xlarge
    - p3.16xlarge
    - p2.xlarge
    - p2.8xlarge
    - p2.16xlarge
    - g5.xlarge
    - g5.2xlarge
    - g5.4xlarge
    - g5.8xlarge
    - g5.12xlarge
    - g5.16xlarge
    - g5.24xlarge
    - g5.48xlarge

  ClusterPart1ECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Cluster model part 1 EC2. Choose a
      GPU instance type
    Default: g4dn.xlarge
    AllowedValues:
    - g4dn.xlarge
    - g4dn.2xlarge
    - g4dn.4xlarge
    - g4dn.8xlarge
    - g4dn.12xlarge
    - g4dn.16xlarge
    - p5.48xlarge
    - p3.2xlarge
    - p3.8xlarge
    - p3.16xlarge
    - p2.xlarge
    - p2.8xlarge
    - p2.16xlarge
    - g5.xlarge
    - g5.2xlarge
    - g5.4xlarge
    - g5.8xlarge
    - g5.12xlarge
    - g5.16xlarge
    - g5.24xlarge
    - g5.48xlarge

  ClusterPart2ECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for cluster model part 2 EC2. Choose a
      GPU instance type
    Default: c5.xlarge
    AllowedValues:
    - c5.large
    - c5.xlarge
    - c5.2xlarge
    - c5.4xlarge
    - c5.9xlarge
    - c5.12xlarge
    - c5.18xlarge
    - c5.24xlarge
    - c4.large
    - c4.xlarge
    - c4.2xlarge
    - c4.4xlarge
    - c4.8xlarge

  FlowECSClusterInstanceType:
    Type: String
    Description: ECS Cluster instance type for Technique model EC2. Choose a GPU instance
      type
    Default: c5.xlarge
    AllowedValues:
    - c5.large
    - c5.xlarge
    - c5.2xlarge
    - c5.4xlarge
    - c5.9xlarge
    - c5.12xlarge
    - c5.18xlarge
    - c5.24xlarge
    - c4.large
    - c4.xlarge
    - c4.2xlarge
    - c4.4xlarge
    - c4.8xlarge

  Cpu:
    Description: Number of CPU units used by the UI and Database task. 1 vCPU = 1024
    Type: String
    Default: 1024
    AllowedValues:
    - 256
    - 512
    - 1024
    - 2048
    - 4096
    - 8192
    - 16384

  Memory:
    Description: Amount of memory (in MiB) used by the UI and Database task. 1 GB
      = 1024
    Type: String
    Default: 4096
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  TechTaskMemory:
    Description: Amount of memory (in MiB) used by the technique model task. 1 GB
      = 1024
    Type: String
    Default: 10240
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterPart1TaskMemory:
    Description: Amount of memory (in MiB) used by the cluster model part 1 task.
      1 GB = 1024
    Type: String
    Default: 10240
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterPart2TaskMemory:
    Description: Amount of memory (in MiB) used by the cluster model part 2 task.
      1 GB = 1024
    Type: String
    Default: 5120
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  FlowTaskMemory:
    Description: Amount of memory (in MiB) used by the flow model task. 1 GB = 1024
    Type: String
    Default: 5120
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  TechModelContainerImage:
    Description: Container image for technique model
    Type: String
    Default: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/flow-ecs:tech_trialv0.0.4
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+/?[a-z0-9-]+:[a-zA-Z0-9.-_]+$

  ClusterModelPart1ContainerImage:
    Description: Container image for cluster model part 1
    Type: String
    Default: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/flow-ecs:cluster1_trialv0.0.4
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+/?[a-z0-9-]+:[a-zA-Z0-9.-_]+$

  ClusterModelPart2ContainerImage:
    Description: Container image for cluster model part 2
    Type: String
    Default: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/flow-ecs:cluster2_trialv0.0.4
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+/?[a-z0-9-]+:[a-zA-Z0-9.-_]+$

  FlowModelContainerImage:
    Description: Container image for flow model
    Type: String
    Default: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/flow-ecs:flow_trialv0.0.4
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+/?[a-z0-9-]+:[a-zA-Z0-9.-_]+$

  SuperuserEmail:
    Description: Email of superuser
    Type: String
    Default: admin@admin.com
    AllowedPattern: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$
    ConstraintDescription: Must be a valid email address

  SuperuserUsername:
    Description: Superuser username. Minimum length of 3.
    Type: String
    Default: maestro
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  SuperuserPassword:
    Description: Superuser password. Minimum length of 8
    Type: String
    Default: changemenow
    MinLength: 8
    ConstraintDescription: Must be minimum length of 8

  NginxContainerImage:
    Description: Nginx container image with tag nginx-market*
    Type: String
    Default: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/cytech:nginx-marketv0.0.3
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+/?[a-z0-9-]+:[a-zA-Z0-9.-]+$
  WebContainerImage:
    Description: Container image for web app with tag market*
    Type: String
    Default: 709825985650.dkr.ecr.us-east-1.amazonaws.com/cypienta/cytech:marketv0.5
    AllowedPattern: 
      ^[0-9]{12}.dkr.ecr.[a-z]{2}-[a-z]+-[0-9].amazonaws.com/[a-z0-9-]+/?[a-z0-9-]+:[a-zA-Z0-9.-]+$

  VpcCidr:
    Description: The CIDR block for the VPC
    Type: String
    Default: 10.0.0.0/16
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  Subnet1Cidr:
    Description: The CIDR block for the first public subnet
    Type: String
    Default: 10.0.0.0/20
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  Subnet2Cidr:
    Description: The CIDR block for the second public subnet
    Type: String
    Default: 10.0.16.0/20
    AllowedPattern: ^(\d{1,3}\.){3}\d{1,3}\/\d{1,2}$
    ConstraintDescription: Must be a valid CIDR block of the form x.x.x.x/x

  UILoadBalancerName:
    Description: Name of UI Load balancer
    Type: String
    Default: cypienta-ui
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  DbLoadBalancerName:
    Description: Name of Database Load balancer
    Type: String
    Default: cypienta-db
    MinLength: 3
    ConstraintDescription: Must be minimum length of 3

  ClusterAutoScalingMinSize:
    Description: The minimum size of the auto scaling group for the ECS cluster
    Type: String
    Default: 0
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  ClusterAutoScalingMaxSize:
    Description: The maximum size of the auto scaling group for the ECS cluster
    Type: String
    Default: 5
    AllowedPattern: ^[0-9]+$
    ConstraintDescription: Must be an integer value

  CpuECSOptimizedAMI:
    Description: AMI ID for UI task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/recommended/image_id

  DbECSOptimizedAMI:
    Description: AMI ID for UI task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux/recommended/image_id

  GpuECSOptimizedAMI:
    Description: AMI ID for Database task
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/gpu/recommended/image_id

Mappings:
  RegionMap:
    af-south-1:
      lambdaLayer: arn:aws:lambda:af-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-northeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-northeast-3:336392948345:layer:AWSSDKPandas-Python311:12
    ap-south-1:
      lambdaLayer: arn:aws:lambda:ap-south-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-1:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-1:336392948345:layer:AWSSDKPandas-Python311:12
    ap-southeast-2:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-2:336392948345:layer:AWSSDKPandas-Python311:12
    ca-central-1:
      lambdaLayer: arn:aws:lambda:ca-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-central-1:
      lambdaLayer: arn:aws:lambda:eu-central-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-north-1:
      lambdaLayer: arn:aws:lambda:eu-north-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-1:
      lambdaLayer: arn:aws:lambda:eu-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-2:
      lambdaLayer: arn:aws:lambda:eu-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    eu-west-3:
      lambdaLayer: arn:aws:lambda:eu-west-3:336392948345:layer:AWSSDKPandas-Python311:12
    sa-east-1:
      lambdaLayer: arn:aws:lambda:sa-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-1:
      lambdaLayer: arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-east-2:
      lambdaLayer: arn:aws:lambda:us-east-2:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-1:
      lambdaLayer: arn:aws:lambda:us-west-1:336392948345:layer:AWSSDKPandas-Python311:12
    us-west-2:
      lambdaLayer: arn:aws:lambda:us-west-2:336392948345:layer:AWSSDKPandas-Python311:12
    ap-east-1:
      lambdaLayer: arn:aws:lambda:ap-east-1:839552336658:layer:AWSSDKPandas-Python311:14
    ap-south-2:
      lambdaLayer: arn:aws:lambda:ap-south-2:246107603503:layer:AWSSDKPandas-Python311:13
    ap-southeast-3:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-3:258944054355:layer:AWSSDKPandas-Python311:14
    ap-southeast-4:
      lambdaLayer: >-
        arn:aws:lambda:ap-southeast-4:945386623051:layer:AWSSDKPandas-Python311:13
    eu-central-2:
      lambdaLayer: arn:aws:lambda:eu-central-2:956415814219:layer:AWSSDKPandas-Python311:13
    eu-south-1:
      lambdaLayer: arn:aws:lambda:eu-south-1:774444163449:layer:AWSSDKPandas-Python311:14
    eu-south-2:
      lambdaLayer: arn:aws:lambda:eu-south-2:982086096842:layer:AWSSDKPandas-Python311:13
    il-central-1:
      lambdaLayer: arn:aws:lambda:il-central-1:263840725265:layer:AWSSDKPandas-Python311:12
    me-central-1:
      lambdaLayer: arn:aws:lambda:me-central-1:593833071574:layer:AWSSDKPandas-Python311:12
    me-south-1:
      lambdaLayer: arn:aws:lambda:me-south-1:938046470361:layer:AWSSDKPandas-Python311:14
    cn-north-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-north-1:406640652441:layer:AWSSDKPandas-Python311:10
    cn-northwest-1:
      lambdaLayer: >-
        arn:aws-cn:lambda:cn-northwest-1:406640652441:layer:AWSSDKPandas-Python311:10

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DependsOn:
    - LambdaRole
    - ExecutionRole
    - EcsInstanceExecutionRole
    - EcsInstanceProfile
    Properties:
      BucketName: !Ref BucketName

  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-AllowStepFunctionExecution"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - states:StartExecution
            Resource: arn:aws:states:*
      - PolicyName: !Sub "${AWS::StackName}-AllowECS"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ecs:*
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonSageMakerServiceCatalogProductsLambdaServiceRolePolicy
      - arn:aws:iam::aws:policy/AmazonS3FullAccess
  StepFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: states.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-StepFunction"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogDelivery
            - logs:GetLogDelivery
            - logs:UpdateLogDelivery
            - logs:DeleteLogDelivery
            - logs:ListLogDeliveries
            - logs:PutResourcePolicy
            - logs:DescribeResourcePolicies
            - logs:DescribeLogGroups
            - lambda:InvokeFunction
            - sagemaker:DescribeTransformJob
            - xray:PutTraceSegments
            - xray:PutTelemetryRecords
            - xray:GetSamplingRules
            - xray:GetSamplingTargets
            - s3:*
            - states:StartExecution
            - ecs:*
            - iam:PassRole
            Resource: '*'

  skipInput:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_skip_input
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: skipInput
      InlineCode: |
        '''
        Skip input file if one execution is running else start execution in step function.

        Input: User input file
        '''

        import os
        import json
        import glob
        import base64
        import gzip

        import urllib
        import boto3
        from botocore.exceptions import ClientError

        BUCKET = os.getenv("bucket")

        STEP_FUNCTION_ARN = os.getenv("step_function_arn")

        missing_variables = []

        if BUCKET is None:
            missing_variables.append("bucket")
        if STEP_FUNCTION_ARN is None:
            missing_variables.append("step_function_arn")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")
        STEP_CLIENT = boto3.client('stepfunctions')

        SCRATCH_DIR = "scratch"

        QUEUE_LOOKUP_FILE = f"{SCRATCH_DIR}/queue_lookup.json"

        TEMP_QUEUE_LOOKUP_FILE = "/tmp/queue_lookup.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE
            func = "lambda_handler"

            try:

                clear_temp_dir()

                fetch_or_create_queue_lookup_file(BUCKET)

                if 'Records' in event and 's3' in event['Records'][0]:
                    print(f"{func}: Triggered by S3")
                    handle_s3_input_trigger(event)
                else:
                    print(f"{func}: Triggered by cloudwatch")
                    handle_cloud_watch_trigger(event)
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(BUCKET, error_message)
                raise ex


        def handle_s3_input_trigger(event):
            '''
            Handle s3 input trigger
            Args:
                event: event from s3 object created trigger
            '''
            global S3_CLIENT
            global STEP_CLIENT
            global BUCKET
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE
            global STEP_FUNCTION_ARN

            func = "handle_s3_input_trigger"

            input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            skip_input = skip_processing_input_file(BUCKET, input_filename)

            save_queue_lookup_file()

            if skip_input:
                print(f"{func}: Skipping current input")
            else:
                print(f"{func}: Process current input")

                execution_input = {
                    "bucket": BUCKET,
                    "key": input_filename
                }

                try:
                    response = STEP_CLIENT.start_execution(
                        stateMachineArn=STEP_FUNCTION_ARN,
                        input=json.dumps(execution_input)
                    )

                    print(f"Step Function execution started: {response['executionArn']}")

                except Exception as e:
                    print(f"{func}: Error starting Step Function execution")
                    raise e


        def handle_cloud_watch_trigger(event):
            '''
            Handle cloud watch input trigger
            Args:
                event: event from s3 object created trigger
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE

            func = "handle_cloud_watch_trigger"

            aws_log = event['awslogs']['data']
            compressed_payload = base64.b64decode(aws_log)
            uncompressed_payload = gzip.decompress(compressed_payload)
            decoded_payload = json.loads(uncompressed_payload.decode('utf-8'))

            print("Decoded CloudWatch data: ", decoded_payload)

            message = decoded_payload["logEvents"][0]["message"]

            try:
                message_json = json.loads(message)
            except json.JSONDecodeError as json_error:
                print(f"Error parsing JSON from message: {message}")
                raise json_error
            except KeyError as key_error:
                print("Missing key in JSON message")
                raise key_error

            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            if len(queue_lookup_json["input_queue"]) > 0:
                print(f"{func}: Remove current input from input queue.")
                queue_lookup_json["input_queue"].pop(0)
                queue_lookup_json["prev"] = message_json["payload"]["prev"]
                if "first" in message_json["payload"]:
                    queue_lookup_json["first"] = message_json["payload"]["first"]
                json.dump(queue_lookup_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))

            save_queue_lookup_file()


        def save_queue_lookup_file():
            '''
            Save queue lookup file to S3
            '''
            global BUCKET
            global S3_CLIENT
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE
            func = "save_queue_lookup_file"
            print("Save queue lookup file")
            try:
                S3_CLIENT.upload_file(TEMP_QUEUE_LOOKUP_FILE, BUCKET, QUEUE_LOOKUP_FILE)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {BUCKET}/{QUEUE_LOOKUP_FILE}")
                raise e


        def skip_processing_input_file(bucket, input_filename):
            '''
            Check if current input file should be processed or skipped
            Args:
                bucket: Bucket name
                input_filename: S3 object key of the input file on which lambda function is triggered
            Returns:
                true/false if current input processed should be skipped
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_QUEUE_LOOKUP_FILE
            func = "skip_processing_input_file"

            skip_input = False

            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            previous_input_last_batch_unique_id = queue_lookup_json["prev"]

            input_queue_entry = f"{bucket}/{input_filename}"
            print(f"{func}: Input queue entry: {input_queue_entry}")

            if previous_input_last_batch_unique_id is None:
                print(f"{func}: There are no previous batches to check")
                skip_input = False

                if len(queue_lookup_json["input_queue"]) > 0:
                    print(f"{func}: Current input queue is not empty. Skip input processing.")
                    skip_input = True
                    if input_queue_entry not in queue_lookup_json["input_queue"]:
                        print(f"{func}: Current input is not present in queue, push to queue.")
                        queue_lookup_json["input_queue"].append(input_queue_entry)
                else:
                    print(f"{func}: Current input is not present in queue, push to queue.")
                    queue_lookup_json["input_queue"].append(input_queue_entry)

                print(f"{func}: Save any updates to queue lookup file")
                json.dump(queue_lookup_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))
                return skip_input

            flow_output_for_previous_input_filename = f"{SCRATCH_DIR}/intermediate/{previous_input_last_batch_unique_id}/flow.json"
            flow_output_for_previous_input_present = False
            try:
                print(f"{func}: Check if the flow output for previous input is available: {bucket}/{flow_output_for_previous_input_filename}")
                S3_CLIENT.head_object(Bucket=bucket, Key=flow_output_for_previous_input_filename)
                flow_output_for_previous_input_present = True
            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Flow output is not present for previous input. Cannot start processing current input.")
                flow_output_for_previous_input_present = False

            if not flow_output_for_previous_input_present:
                if input_queue_entry not in queue_lookup_json["input_queue"]:
                    print(f"{func}: Current input is not present in queue, push to queue.")
                    queue_lookup_json["input_queue"].append(input_queue_entry)

                skip_input = True

            else:
                print(f"{func}: Flow output for previous input is present. Start processing next input.")

                if len(queue_lookup_json["input_queue"]) > 0:
                    print(f"{func}: Next input to process is: {queue_lookup_json['input_queue'][0]}")
                    if input_queue_entry == queue_lookup_json["input_queue"][0]:
                        print(f"{func}: Current input is to be processed next.")
                        skip_input = False
                    else:
                        print(f"{func}: Current input is not to be processed next.")
                        skip_input = True
                        if input_queue_entry not in queue_lookup_json["input_queue"]:
                            print(f"{func}: Current input is not present in queue, push to queue.")
                            queue_lookup_json["input_queue"].append(input_queue_entry)
                else:
                    print(f"{func}: There are no items in queue input. Current input is to be processed next.")
                    skip_input = False
                    print(f"{func}: Current input is not present in queue, push to queue.")
                    queue_lookup_json["input_queue"].append(input_queue_entry)

            print(f"{func}: Save any updated to queue lookup file")
            json.dump(queue_lookup_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))

            return skip_input


        def fetch_or_create_queue_lookup_file(bucket):
            '''
            Check if the queue lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global QUEUE_LOOKUP_FILE
            global S3_CLIENT
            global TEMP_QUEUE_LOOKUP_FILE

            func = "fetch_or_create_queue_lookup_file"

            try:
                print(
                    f"{func}: Checking if queue lookup file exists on S3 path {bucket}/{QUEUE_LOOKUP_FILE}")

                S3_CLIENT.head_object(Bucket=bucket, Key=QUEUE_LOOKUP_FILE)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Queue lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = {"first": None, "prev": None, "input_queue": []}
                json.dump(internal_id_to_unique_id_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))

                print(f"{func}: Initiated queue lookup")
            else:
                try:
                    print(f"{func}: Download available queue lookup file from S3")

                    S3_CLIENT.download_file(bucket, QUEUE_LOOKUP_FILE, TEMP_QUEUE_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{QUEUE_LOOKUP_FILE}")
                    raise e


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
          step_function_arn: !GetAtt stepFunction.Arn
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: input/
                - Name: suffix
                  Value: .json
        MyLogGroupTrigger:
          Type: CloudWatchLogs
          Properties:
            LogGroupName: !Ref processFlowLogGroup
            FilterPattern: update queue lookup
  skipInputLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${skipInput}
  skipInputLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref skipInput
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  splunkInput:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_splunk_input
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: splunkInput
      InlineCode: |
        '''
        Chunk input from splunk to create input for VRL transform.
        Merge back chunks for single input file back to one and send as input to pipeline.

        Input: Input from splunk Add-on Amazon S3 Uploader for Splunk (json format)
        '''

        import os
        import sys
        import subprocess
        import json
        import glob
        import urllib
        import boto3
        from itertools import islice
        import gc

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        subprocess.call('pip install ijson==3.3.0 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import ijson

        CHUNK_SIZE = int(os.getenv("chunk_size"))

        missing_variables = []
        if CHUNK_SIZE is None:
            missing_variables.append("chunk_size")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        BUF_SIZE = 1*1024*1024

        TEMP_INPUT_DIR = "/tmp/input/"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)


        def lambda_handler(event, context):
            '''
            Read input file and split in chunks
            '''
            global S3_CLIENT
            global CHUNK_SIZE

            func = "lambda_handler"

            input_file_object = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            clear_temp_dir()

            print(f"{func}: Input file object: {input_file_object}")

            prefix = "/".join(input_file_object.split("/")[:2])
            if prefix == "splunk_input/input":
                process_splunk_input(event, context)
            else:
                merge_transformed_input(event, context)


        def merge_transformed_input(event, context):
            '''
            Read chunk transformed input file and merge it to be input to pipeline
            '''
            global S3_CLIENT
            func = "merge_transformed_input"
            bucket = event["Records"][0]["s3"]["bucket"]["name"]
            input_file_object = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            input_file = "/tmp/chunk_input.json"

            clear_temp_dir()

            try:
                print(f"{func}: Download input file: {bucket}/{input_file_object}")
                S3_CLIENT.download_file(bucket, input_file_object, input_file)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{input_file_object}")
                raise e

            print(f"{func}: Check if all splits are processed")
            input_file_name = input_file_object.split("/")[-2]
            input_file_splits_length_json_filename = f"splunk_input/{SCRATCH_DIR}/count/{input_file_name}/split_length.json"
            temp_input_file_splits_length_json_filename = f"/tmp/split_length.json"
            try:
                print(f"{func}: Download length file: {bucket}/{input_file_splits_length_json_filename}")
                S3_CLIENT.download_file(bucket, input_file_splits_length_json_filename, temp_input_file_splits_length_json_filename)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{input_file_splits_length_json_filename}")
                raise e

            length_json = json.load(open(temp_input_file_splits_length_json_filename, "r"))

            prefix = f"splunk_input/{SCRATCH_DIR}/transformed/{input_file_name}/"
            print(f"{func}: Prefix to search: {prefix}")

            paginator = S3_CLIENT.get_paginator('list_objects_v2')
            page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)

            completed_chunk_keys = []

            print(f"{func}: Iterate through each page")
            for page in page_iterator:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        completed_chunk_keys.append(obj['Key'])

            print(f"{func}: Total files in prefix: {len(completed_chunk_keys)}")

            if len(completed_chunk_keys) != length_json["count"]:
                print(f"{func}: All chunk did not complete processing. Skip merge")
                return

            print(f"{func}: All chunks completed processing. Merge chunks and upload to S3")

            input_to_pipeline_filename = "/tmp/input_pipe.json"
            with open(input_to_pipeline_filename, "w") as f_write:
                f_write.write("""{"input": [""")
                first_line_present = False

                for chunk_key in completed_chunk_keys:
                    completed_chunk_filename = f"{TEMP_INPUT_DIR}chunk.json"
                    try:
                        print(f"{func}: Download input file: {bucket}/{chunk_key}")
                        S3_CLIENT.download_file(bucket, chunk_key, completed_chunk_filename)
                    except Exception as e:
                        print(f"{func}: Failed to download file from S3: {bucket}/{chunk_key}")
                        raise e

                    print("read and add chunk contents")
                    with open(completed_chunk_filename, "r") as f_read:
                        for line in f_read:
                            if first_line_present:
                                f_write.write(",\n" + line.strip("\n"))
                            else:
                                f_write.write("\n" + line.strip("\n"))
                                first_line_present = True

                f_write.write("]}")

            print("upload output file")
            output_file_object = f"input/{input_file_name}.json"

            try:
                S3_CLIENT.upload_file(input_to_pipeline_filename, bucket, output_file_object)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{output_file_object}")
                raise e


        def process_splunk_input(event, context):
            '''
            Read input file and split in chunks
            '''
            global S3_CLIENT
            global CHUNK_SIZE

            func = "process_splunk_input"

            bucket = event["Records"][0]["s3"]["bucket"]["name"]
            input_file_object = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            input_file = "/tmp/input.json"

            clear_temp_dir()

            try:
                print(f"{func}: Download input file: {bucket}/{input_file_object}")
                S3_CLIENT.download_file(bucket, input_file_object, input_file)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{input_file_object}")
                raise e
            
            print(f"{func}: Download completed")

            chunks_of_large_input_file = create_chunk_for_large_input_file(input_file)

            input_file_name = input_file_object.split("/")[-1].split(".")[0]

            input_file_splits_length_json = {"count": len(chunks_of_large_input_file)}

            input_file_splits_length_json_filename = f"splunk_input/{SCRATCH_DIR}/count/{input_file_name}/split_length.json"
            temp_input_file_splits_length_json_filename = f"/tmp/split_length.json"
            json.dump(input_file_splits_length_json, open(temp_input_file_splits_length_json_filename, "w"))

            try:
                S3_CLIENT.upload_file(temp_input_file_splits_length_json_filename, bucket, input_file_splits_length_json_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{input_file_splits_length_json_filename}")
                raise e

            print(f"{func}: Upload chunks. Number of chunks: {len(chunks_of_large_input_file)}")
            for i, chunk_of_large_input_file in enumerate(chunks_of_large_input_file, start=1):
                output_file_object = f"splunk_input/{SCRATCH_DIR}/chunk/{input_file_name}/split_{i}.json"
                try:
                    S3_CLIENT.upload_file(chunk_of_large_input_file, bucket, output_file_object)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{output_file_object}")
                    raise e

            print(f"{func}: Upload completed")


        def create_chunk_for_large_input_file(temp_input_file):
            '''
            Create chunks for a large input file
            Returns:
                List of split file names
            '''
            global CHUNK_SIZE
            global BUF_SIZE
            global TEMP_INPUT_DIR

            split_files = []
            with open(temp_input_file, 'rb') as f:
                split_files = []
                item = 1
                input_list_iter = ijson.items(f, 'item', use_float=True, buf_size=BUF_SIZE)
                while True:
                    input_list = list(islice(input_list_iter, CHUNK_SIZE))
                    split_filename = TEMP_INPUT_DIR + "input" + '_' + str(item) + '.json'
                    if not input_list:
                        break

                    with open(split_filename, 'w') as outfile:
                        for alert in input_list:
                            outfile.write(json.dumps(alert) + "\n")
                        split_files.append(split_filename)

                    item += 1
                    del input_list
                    gc.collect()

            del input_list_iter
            gc.collect()

            return split_files


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          chunk_size: '5000'
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: splunk_input/input/
                - Name: suffix
                  Value: .json
        S3ObjectCreatedTransformed:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: splunk_input/scratch/transformed/
                - Name: suffix
                  Value: .json
  splunkInputLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${splunkInput}
  splunkInputLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref splunkInput
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  vrlLambda:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_vrl_lambda
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: vrlLambda
      ImageUri: !Ref VRLLambdaImage
      PackageType: Image
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Environment:
        Variables:
          vrl_program_bucket: !Ref BucketName
          vrl_program_s3_key: splunk_input/program/program.vrl
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: splunk_input/scratch/chunk/
                - Name: suffix
                  Value: .json
  vrlLambdaLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${vrlLambda}
  vrlLambdaLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref vrlLambda
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  enrichWithTechnique:
    Type: AWS::Serverless::Function
    DependsOn:
    - DbLoadBalancer
    Properties:
      FunctionName: !Sub ${AWS::StackName}_enrich_with_technique
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: enrichWithTechnique
      InlineCode: |
        '''
        Chunk input file, sanititze in required format, encode attributes, encode node_features if present, generate internal id.
        Create batch transform job for technique classification.

        Input: User input file
        '''

        import os
        import sys
        import subprocess
        import gc
        import re
        from datetime import datetime, timezone
        from dateutil import parser
        import copy
        import zipfile

        import urllib
        import uuid
        import json
        import glob
        from itertools import islice
        import sqlite3
        from contextlib import closing

        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd
        import numpy as np

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        os.makedirs("/tmp/nltk_download", exist_ok=True)
        subprocess.call('pip install ijson==3.3.0 nltk==3.8.1 psycopg2-binary==2.9.9 sqlalchemy==2.0.32 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import psycopg2
        from psycopg2 import sql
        from psycopg2.extras import DictCursor

        import ijson
        import nltk
        from nltk.corpus import words


        TECHNIQUE_LOOKUP_OBJECT = os.getenv("technique_lookup_object")

        CHUNK_SIZE = os.getenv("chunk_size")

        ENCODE_OTHER_ATTRS = os.getenv("encode_other_attrs")

        MAP_CEF_TO_INTERNAL = os.getenv("map_cef_to_internal")

        ENCODE_NODE_FEATURE = os.getenv("encode_node_feature")

        DB_CONNECTION_STRING = os.getenv("db_connection_string")

        missing_variables = []

        if TECHNIQUE_LOOKUP_OBJECT is None:
            missing_variables.append("technique_lookup_object")
        if CHUNK_SIZE is None:
            missing_variables.append("chunk_size")
        if ENCODE_OTHER_ATTRS is None:
            missing_variables.append("encode_other_attrs")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        CHUNK_SIZE = int(CHUNK_SIZE)

        ENCODE_OTHER_ATTRS = ENCODE_OTHER_ATTRS.lower() == "true"

        if MAP_CEF_TO_INTERNAL is not None:
            MAP_CEF_TO_INTERNAL = (MAP_CEF_TO_INTERNAL == "true")
        else:
            MAP_CEF_TO_INTERNAL = True

        if ENCODE_NODE_FEATURE is not None:
            ENCODE_NODE_FEATURE = (ENCODE_NODE_FEATURE == "true")
        else:
            ENCODE_NODE_FEATURE = True

        S3_CLIENT = boto3.client("s3")

        BUF_SIZE = 1*1024*1024

        TEMP_INPUT_DIR = "/tmp/input/"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)

        SCRATCH_DIR = "scratch"

        INTERNAL_ID_TO_USER_ID_OBJECT_KEY = f"{SCRATCH_DIR}/internal_id_to_user_id.json"
        INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY = f"{SCRATCH_DIR}/internal_id_to_unique_id.json"

        QUEUE_LOOKUP_FILE = f"{SCRATCH_DIR}/queue_lookup.json"
        NODE_FEATURE_LOOKUP_FILE = f"{SCRATCH_DIR}/node_feature_lookup.json"

        TEMP_INPUT_FILENAME = "/tmp/input_all.json"
        TEMP_ALERTS_INPUT_FILENAME = f"{TEMP_INPUT_DIR}input.json"
        TEMP_CLUSTER_CONFIG_INPUT_FILENAME = f"{TEMP_INPUT_DIR}cluster_config.json"
        TEMP_CONFIG_INPUT_FILENAME = f"{TEMP_INPUT_DIR}config.json"
        TEMP_NODE_FEATURE_INPUT_FILENAME = f"{TEMP_INPUT_DIR}node_feature.json"
        TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME = f"{TEMP_INPUT_DIR}node_feature_original.json"
        TEMP_USER_FEEDBACK_WEIGHTS = f"{TEMP_INPUT_DIR}user_feedback_weights.json"

        TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE = "/tmp/data.csv"

        TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE = "/tmp/internal_id_to_user_id.json"
        TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE = "/tmp/internal_id_to_unique_id.json"
        TEMP_QUEUE_LOOKUP_FILE = "/tmp/queue_lookup.json"

        TEMP_NODE_FEATURE_LOOKUP_FILE = "/tmp/node_feature_lookup.json"

        TEMP_SKIPPED_EVENTS_FILENAME = "/tmp/skip_events.json"

        TEMP_OVERRIDE_AGG_OUTPUT = "/tmp/override_agg_output.json"
        TEMP_OVERRIDE_CLUSTER_OUTPUT = "/tmp/override_cluster_output.json"
        TEMP_OVERRIDE_FLOW_OUTPUT = "/tmp/override_flow_output.json"
        TEMP_OVERRIDE_FLOW_DICT = "/tmp/override_flow_dict.json"
        TEMP_CLUSTER_OUTPUT_ZIP = "/tmp/cluster_output.zip"

        TEMP_ALIASES_FILE = "/tmp/aliases.json"
        TEMP_FIELD_TYPES_FILE = "/tmp/field_types.json"
        TEMP_FEATURE_MAPPING_FILE = "/tmp/feature_mapping.json"

        INITIAL_WEIGHTS_VALUE = 100

        CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
        CONFIG_EXISTS_IN_REQUEST = False
        NODE_FEATURE_EXISTS_IN_REQUEST = False
        USER_OVERRIDE_CLUSTER = False
        NODE_TRANSLATION = False
        NODE_ENCODING = False

        SKIP_ENCODING_ATTRIBUTE = ["priority", "port", "url", "user_agent", "cert", "user_feedback"]

        ENCODE_TABLE = {}

        nltk.data.path.append("/tmp/nltk_download")
        nltk.download("punkt", download_dir="/tmp/nltk_download")
        nltk.download('words', download_dir="/tmp/nltk_download")

        ENGLISH_WORDS = set(words.words())

        BLACK_LIST_KEYS = [
            '_raw',
            '_si',
            'splunk_server',
            'ComputerName',
            'source',
            'tag',
            'pa',
            'ssl_issuer',
            'ssl_subject',
            'query',
            'FriendlyName',
            'service_name',
            'content{}',
            'Mandatory_Label',
            'vendor_definition'
        ]


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global TEMP_INPUT_FILENAME
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE
            global TEMP_CONFIG_INPUT_FILENAME
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global CONFIG_EXISTS_IN_REQUEST
            global MAP_CEF_TO_INTERNAL
            global USER_OVERRIDE_CLUSTER
            global NODE_TRANSLATION
            global NODE_ENCODING
            func = "lambda_handler"

            CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
            CONFIG_EXISTS_IN_REQUEST = False
            NODE_FEATURE_EXISTS_IN_REQUEST = False
            USER_OVERRIDE_CLUSTER = False
            NODE_TRANSLATION = False
            NODE_ENCODING = False

            bucket = event["bucket"]
            try:
                input_filename = event["key"]

                # delete only the error txt files from output folder if present
                delete_error_log_s3(bucket)

                clear_temp_dir()

                initialize_sqlite()

                fetch_or_create_queue_lookup_file(bucket)

                try:
                    S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                    print(f"{func}: Input file download completed")
                except Exception as e:
                    print(f"{func}: Failed to download input file")
                    raise e

                fetch_or_create_technique_classification_lookup_file(bucket)
                fetch_or_create_internal_id_to_user_id_lookup_file(bucket)
                fetch_or_create_internal_id_to_unique_id_lookup_file(bucket)
                get_node_translation_encoder_files(bucket)

                aliases_json = None
                field_types_json = None
                feature_mapping_json = None

                if NODE_TRANSLATION:
                    aliases_json = json.load(open(TEMP_ALIASES_FILE, "r"))
                    field_types_json = json.load(open(TEMP_FIELD_TYPES_FILE, "r"))
                    all_keys = {}
                    all_keys['src'] = set([key for key, value in field_types_json.items() if value == 'src'])
                    all_keys['dst'] = set([key for key, value in field_types_json.items() if value == 'dst'])
                    field_types_json = all_keys

                if NODE_ENCODING:
                    feature_mapping_json = json.load(open(TEMP_FEATURE_MAPPING_FILE, "r"))

                path_to_global_attribute_weights = f"{SCRATCH_DIR}/attribute_weights.json"
                fetch_or_create_global_attribute_weights(bucket, path_to_global_attribute_weights)

                get_static_cluster_config()

                process_input_file_to_extract_config_and_node_feature()

                try:
                    print(f"{func}: Create chunk for large input file.")
                    chunks_of_large_input_file = create_chunk_for_large_input_file()
                    if len(chunks_of_large_input_file) < 1:
                        raise ValueError(f"Number of chunks created for input file: {len(chunks_of_large_input_file)}. There should be atleast 1 chunk created. Check your input file.")
                except Exception as e:
                    print("Failure during creating chunks for input file.")
                    raise e

                print(f"{func}: Number of chunks created for large input file: {len(chunks_of_large_input_file)}")

                unique_ids = [str(uuid.uuid4()) for _ in chunks_of_large_input_file]
                print(f"Unique id(s) generated for the chunk(s): {unique_ids}")

                process_chunks = chunks_of_large_input_file
                if MAP_CEF_TO_INTERNAL:
                    cef_to_internal_mappings = get_cef_to_internal_mappings()

                    create_event_id(chunks_of_large_input_file)

                for chunk_of_large_input_file, unique_id in zip(chunks_of_large_input_file, unique_ids):
                    try:
                        print(f"{func}: Save original chunk")
                        path_to_original_chunk_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cef_input.json"
                        S3_CLIENT.upload_file(chunk_of_large_input_file, bucket, path_to_original_chunk_input)
                    except Exception as e:
                        print(f"{func}: Failed to save file to S3: {bucket}/{path_to_original_chunk_input}")
                        raise e

                if MAP_CEF_TO_INTERNAL:
                    print(f"{func}: Map events to internal structure")
                    process_chunks, unique_ids = insert_event_sql(bucket, chunks_of_large_input_file, cef_to_internal_mappings, unique_ids,
                                                                  aliases_json, field_types_json)

                print(f"{func}: Chunks to process: {process_chunks}")

                queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

                if not process_chunks:
                    # need to send update queue log to remove current input from input queue.
                    print(f"{func}: Input file does not have any tranformed events that could be processed. Skip input file.")
                    payload_json = {"prev": queue_lookup_json["prev"]}
                    log_json = {"message": "update queue lookup", "payload": payload_json}
                    print(f"{json.dumps(log_json)}")

                    if len(queue_lookup_json["input_queue"]) > 1:
                        print(f"{func}: Next input file to process: {queue_lookup_json['input_queue'][1]}")
                        try:
                            print(f"{func}: Upload next input file in S3")
                            next_input_queue_entry = queue_lookup_json["input_queue"][1]
                            next_input_bucket = next_input_queue_entry.split("/")[0]
                            next_input_key = "/".join(next_input_queue_entry.split("/")[1:])
                            copy_source = {
                                'Bucket': next_input_bucket,
                                'Key': next_input_key
                            }
                            S3_CLIENT.copy_object(CopySource=copy_source, Bucket=next_input_bucket, Key=next_input_key,
                                                  Metadata={'UpdatedAt': str(datetime.now())}, MetadataDirective='REPLACE')
                        except Exception as e:
                            print(f"{func}: Failed to save file to S3: {next_input_bucket}/{next_input_key}")
                            raise e

                    lambda_response = {"TransformJobName": "skipped_input"}
                    return lambda_response

                print(f"Unique id(s) for the chunk(s) to process: {unique_ids}")

                node_encoding_node_feature = {}
                for chunk_of_large_input_file in process_chunks:
                    chunk_json = json.load(open(chunk_of_large_input_file, "r"))

                    for event in chunk_json:

                        if "other_attributes_dict" not in event or len(event["other_attributes_dict"].keys()) == 0:
                            event["other_attributes_dict"] = {"empty": True}

                        if not MAP_CEF_TO_INTERNAL and NODE_TRANSLATION:
                            event = node_translation(event, aliases_json, field_types_json)

                        if not NODE_FEATURE_EXISTS_IN_REQUEST and NODE_ENCODING:
                            node_encoding_node_feature = node_feature_encoding(event, feature_mapping_json, node_encoding_node_feature)

                    json.dump(chunk_json, open(chunk_of_large_input_file, "w"))

                if not NODE_FEATURE_EXISTS_IN_REQUEST and NODE_ENCODING:
                    json.dump(node_encoding_node_feature, open(TEMP_NODE_FEATURE_INPUT_FILENAME, "w"))
                    json.dump(node_encoding_node_feature, open(TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME, "w"))

                if NODE_FEATURE_EXISTS_IN_REQUEST and ENCODE_NODE_FEATURE:
                    process_node_features()
                    print(f"{func}: Upload node feature lookup file to S3")
                    try:
                        S3_CLIENT.upload_file(TEMP_NODE_FEATURE_LOOKUP_FILE, bucket, NODE_FEATURE_LOOKUP_FILE)
                    except Exception as e:
                        print(f"{func}: Failed to save file to S3: {bucket}/{NODE_FEATURE_LOOKUP_FILE}")
                        raise e

                start_batch_transform_job = False
                # lambda_response = {"TransformJobName": 'transform-job-cluster', "bucket": bucket, "input_first_unique_id": unique_ids[0]}

                lambda_response = {
                    # for step function
                    "TransformJobName": f'transform-job-cluster-{unique_ids[0]}',
                    "bucket": bucket,
                    "prefix": f"{SCRATCH_DIR}/output/classification/{unique_ids[0]}/",
                }

                for i, chunk_of_large_input_file in enumerate(process_chunks):

                    unique_id = unique_ids[i]

                    next_unique_id = None
                    if (i+1) < len(unique_ids):
                        next_unique_id = unique_ids[i+1]

                    if i > 0:
                        previous_unique_id = unique_ids[i-1]
                    else:
                        previous_unique_id = queue_lookup_json["prev"]
                        # previous_unique_id = None

                    # first - unique id of first batch of current input
                    # prevous - unique id of previous batch that needs to be run (inter file handled)
                    # next - unique id of next batch that needs to be run (last batch of current input will always be null)
                    queue_json = {
                        "first": unique_ids[0],
                        "previous": previous_unique_id,
                        "next": next_unique_id
                    }

                    print(f"{func}: Processing chunk file: {chunk_of_large_input_file}; unique ID for processing chunk file: {unique_id}")

                    print(f"{func}: Starting batch job for enriching alerts with techniques")
                    path_to_input_for_batch_technique_classification = f"{SCRATCH_DIR}/response/classification_in/{unique_ids[0]}/input_classification_{unique_id}.json"
                    response = enrich_alerts_with_techniques(bucket, chunk_of_large_input_file, unique_id, unique_ids[0], queue_json,
                                                             path_to_input_for_batch_technique_classification)

                    if response:
                        start_batch_transform_job = True

                if start_batch_transform_job:
                    lambda_response = {
                        # for step function
                        "TransformJobName": f'transform-job-tech-{unique_ids[0]}',
                        "bucket": bucket,
                        "prefix": f"{SCRATCH_DIR}/response/classification_in/{unique_ids[0]}/",
                    }

                if USER_OVERRIDE_CLUSTER:
                    create_flow_output(bucket, unique_ids[-1])

                    lambda_response = {
                        # for step function
                        "TransformJobName": "user_override_cluster",
                        "S3InputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/cluster_out/{unique_ids[0]}/cluster_output.zip.out",
                        "S3OutputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/cluster_out/{unique_ids[0]}/cluster_output.zip.out",
                    }

                # update_queue_lookup(bucket, unique_ids)
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def enrich_alerts_with_techniques(bucket, chunk_of_large_input_file, unique_id, first_unique_id, queue_json,
                                          path_to_input_for_batch_technique_classification):
            '''
            Enriches alerts with techniques. Filter out previously classified text. Sanitize input. create batch transform job for technique classification
            Args:
                bucket: Bucket name
                chunk_of_large_input_file: Chunk of input file
                unique_id: The unique identifier for the data chunk,
                first_unique_id: first unique id for chunk of input
                queue_json: JSON object containing queue information
                path_to_input_for_batch_technique_classification: path to store input for classification
            Returns: False if no new alerts to classify, else True if there are alerts to classfiy.
            '''

            global SCRATCH_DIR
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE
            global ENCODE_TABLE
            global ENCODE_OTHER_ATTRS
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global CONFIG_EXISTS_IN_REQUEST
            global S3_CLIENT
            global TEMP_CONFIG_INPUT_FILENAME
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE
            global INTERNAL_ID_TO_USER_ID_OBJECT_KEY
            global TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE
            global INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY
            global SKIP_ENCODING_ATTRIBUTE
            global TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME
            global USER_OVERRIDE_CLUSTER

            func = "enrich_alerts_with_techniques"

            path_to_intermediate_input_before_technique_classification = f"{SCRATCH_DIR}/intermediate/{unique_id}/input.json"
            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            path_to_cluster_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_config.json"
            path_to_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/config.json"
            path_to_node_feature_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/node_feature.json"
            path_to_node_feature_original_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/node_feature_original.json"

            technique_classification_lookup_df = pd.read_csv(TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE)

            try:
                print(f"{func}: Save original chunk")
                path_to_original_chunk_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/original_input.json"
                S3_CLIENT.upload_file(chunk_of_large_input_file, bucket, path_to_original_chunk_input)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_original_chunk_input}")
                raise e

            # From the input, filter alerts that do not have recognized techniques in the technique lookup table

            # drop dulicates from chunk input
            chunk_input_df = pd.read_json(chunk_of_large_input_file)
            chunk_input_df = chunk_input_df.rename(columns={"name": "alerts"})
            chunk_input_df = chunk_input_df.drop_duplicates("alerts")

            # if tech is present in the input chunk then rename it to techniques, else initialize to None
            if "tech" in chunk_input_df.columns:
                chunk_input_df = chunk_input_df.rename(columns={"tech": "techniques"})
            else:
                chunk_input_df["techniques"] = None

            # If there are any empty techniques in the input chunk, make it None.
            chunk_input_df['techniques'] = chunk_input_df['techniques'].apply(lambda x: None if x == [] else x)
            chunk_input_df = chunk_input_df[["alerts", "techniques"]]

            # Get only those rows from chunk input which did not have tech field, or it was empty.
            filtered_chunk_input_df = chunk_input_df[chunk_input_df["techniques"].isna()]

            # merge the technique lookup table with filtered chunk input.
            # merge on right means it will get None for the alerts not present in the lookup table
            merged_technique_classification_df = pd.merge(technique_classification_lookup_df, filtered_chunk_input_df, on="alerts", how="right")

            # Get all the alerts that are not present in the lookup table by filtering techniques from lookup table as None
            unique_alerts_to_classify = merged_technique_classification_df[merged_technique_classification_df["techniques_x"].isna()]
            unique_alerts_to_classify = unique_alerts_to_classify.drop_duplicates("alerts")
            unique_alerts_to_classify = unique_alerts_to_classify[unique_alerts_to_classify["alerts"].notna() & (unique_alerts_to_classify["alerts"] != "")]
            unique_alerts_to_classify = unique_alerts_to_classify["alerts"].to_list()

            # create dictionary for alerts: recognized techniques
            merged_technique_classification_df["techniques_x"] = merged_technique_classification_df["techniques_x"].fillna("[]")
            merged_technique_classification_df["techniques_y"] = merged_technique_classification_df["techniques_x"]
            merged_technique_classification_df = merged_technique_classification_df[["alerts", "techniques_y"]]

            merged_technique_classification_df = merged_technique_classification_df[merged_technique_classification_df["alerts"].notna() &
                                                                                    (merged_technique_classification_df["alerts"] != "")]
            merged_technique_classification_dict = merged_technique_classification_df.set_index("alerts").to_dict()["techniques_y"]

            print(f"{func}: Start enriching alerts with techniques")

            chunk_input_json = json.load(open(chunk_of_large_input_file, "r"))
            batch_request_for_technique_classification_list = []

            # sanitize input. encode other_attributes_dict and update recognized techniques
            for chunk_input_alert in chunk_input_json:
                if "cluster_id" in chunk_input_alert:
                    USER_OVERRIDE_CLUSTER = True

                if "other_attributes_dict" not in chunk_input_alert or len(chunk_input_alert["other_attributes_dict"].keys()) == 0:
                    chunk_input_alert["other_attributes_dict"] = {"empty": True}
                else:
                    if ENCODE_OTHER_ATTRS:
                        # it will also encode special other_Attributes liek priority
                        for key in chunk_input_alert['other_attributes_dict'].keys():
                            if key in SKIP_ENCODING_ATTRIBUTE:
                                continue
                            encoded, ENCODE_TABLE = custom_encoder(chunk_input_alert['other_attributes_dict'][key], ENCODE_TABLE)
                            chunk_input_alert["other_attributes_dict"][key] = encoded
                    chunk_input_alert["other_attributes_dict"]["empty"] = False

                if "label" not in chunk_input_alert:
                    chunk_input_alert["label"] = -1

                alert_text = chunk_input_alert["name"]

                # if alert text is empty, continue. If tech field does not exist, initialize empty list
                if alert_text.strip() == "":
                    if "tech" not in chunk_input_alert:
                        chunk_input_alert["tech"] = []
                    continue

                # if the tech is not in input, put tech recognized in lookup table if present
                if "tech" not in chunk_input_alert:
                    chunk_input_alert["tech"] = []
                    if alert_text in merged_technique_classification_dict:
                        tech = json.loads(merged_technique_classification_dict[alert_text])
                        chunk_input_alert["tech"] = tech

            print(f"{func}: Save intermediate inputs to S3.")
            if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                try:
                    S3_CLIENT.upload_file(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, bucket, path_to_cluster_config_from_input)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_cluster_config_from_input}")
                    raise e

            if CONFIG_EXISTS_IN_REQUEST:
                try:
                    S3_CLIENT.upload_file(TEMP_CONFIG_INPUT_FILENAME, bucket, path_to_config_from_input)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_config_from_input}")
                    raise e

            if NODE_FEATURE_EXISTS_IN_REQUEST:
                try:
                    S3_CLIENT.upload_file(TEMP_NODE_FEATURE_INPUT_FILENAME, bucket, path_to_node_feature_from_input)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_node_feature_from_input}")
                    raise e

                try:
                    S3_CLIENT.upload_file(TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME, bucket, path_to_node_feature_original_from_input)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_node_feature_original_from_input}")
                    raise e

            chunk_intermediate_input_local_filename = f"{chunk_of_large_input_file}_intermediate.json"

            json.dump(chunk_input_json, open(chunk_intermediate_input_local_filename, "w"))
            preprocess_chunk_input(unique_id, chunk_intermediate_input_local_filename)
            chunk_input_json = json.load(open(chunk_intermediate_input_local_filename, "r"))
            try:
                S3_CLIENT.upload_file(chunk_intermediate_input_local_filename, bucket, path_to_intermediate_input_before_technique_classification)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_intermediate_input_before_technique_classification}")
                raise e

            # Save internal id to user id mapping, internal id to unique id mapping
            try:
                S3_CLIENT.upload_file(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, bucket, INTERNAL_ID_TO_USER_ID_OBJECT_KEY)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{INTERNAL_ID_TO_USER_ID_OBJECT_KEY}")
                raise e

            try:
                S3_CLIENT.upload_file(TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE, bucket, INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY}")
                raise e

            # Save queue for interbatch
            chunk_intermediate_queue_filename = f"{chunk_of_large_input_file}_queue.json"
            json.dump(queue_json, open(chunk_intermediate_queue_filename, "w"))

            try:
                S3_CLIENT.upload_file(chunk_intermediate_queue_filename, bucket, path_to_intermediate_queue)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_intermediate_queue}")
                raise e

            if USER_OVERRIDE_CLUSTER:
                print(f"{func}: User override cluster. Skip batch transform job for {unique_id}")
                create_agg_cluster_flow_output(bucket, unique_id, chunk_intermediate_input_local_filename)
                return False
            if not unique_alerts_to_classify:
                print(f"{func}: No new alerts to classify. Skip batch transform job for {unique_id}")
                skip_creating_batch_transform_job(bucket, unique_id, first_unique_id, chunk_input_json)
                return False
            else:
                # create request for technique classification model
                for request_id, alert_text in enumerate(unique_alerts_to_classify, start=1):
                    individual_request_body_for_technique_classification_batch = {"request_id": str(request_id), "input": alert_text}

                    batch_request_for_technique_classification_list.append(json.dumps(individual_request_body_for_technique_classification_batch))

                batch_request_for_technique_classification_jsonl = "\n".join(batch_request_for_technique_classification_list)

                temp_batch_request_for_technique_classification_jsonl_filename = f"{chunk_of_large_input_file}_classification.txt"

                print(f"{func}: Save batch request for technique classification")
                with open(temp_batch_request_for_technique_classification_jsonl_filename, "w") as file:
                    file.writelines(batch_request_for_technique_classification_jsonl)
                try:
                    S3_CLIENT.upload_file(temp_batch_request_for_technique_classification_jsonl_filename,
                                          bucket, path_to_input_for_batch_technique_classification)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_input_for_batch_technique_classification}")
                    raise e

                return True


        def insert_event_sql(bucket, chunks_of_large_input_file, cef_to_internal_mappings, unique_ids,
                             aliases_json=None, field_types_json=None):
            '''
            Map chunk of input from CEF to internal and insert to event table.
            save skipped events to s3.
            Args:
                bucket: Bucket name
                chunks_of_large_input_file: Chunks of input file
                cef_to_internal_mappings: cef to internal mappings list
                unique_ids: list of unique ids for each chunk
                aliases_json: node to entitiy mapping for node translation
                field_types_json: field mapping for node translation
            Returns:
                list of chunks to process
            '''
            global S3_CLIENT
            global TEMP_SKIPPED_EVENTS_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME
            func = "insert_event_sql"

            process_chunks = []
            node_feature = {}
            keep_unique_ids = []
            for chunk_of_large_input_file, unique_id in zip(chunks_of_large_input_file, unique_ids):

                tranformed_events = []
                skipped_events = []

                print(f"{func}: Read cef mapped chunk events")
                events = json.load(open(chunk_of_large_input_file, "r"))

                print(f"{func}: Transform cef mapped chunk events")
                for event in events:
                    tranformed_event, cef_mapped_keys_to_event, event_node_feature = map_cef_to_internal(event, cef_to_internal_mappings,
                                                                                                         aliases_json, field_types_json)

                    # gather skipped events
                    if tranformed_event is None:
                        skipped_events.append(event)
                        continue

                    # skip insert to table for now

                    tranformed_events.append(tranformed_event)

                    # update node feature
                    for key, value in event_node_feature.items():
                        if key in node_feature:
                            # if its existing node, update node features
                            node_feature[key].update(value)
                        else:
                            # if its new node. get node features
                            node_feature[key] = value

                if len(skipped_events) > 0:
                    print(f"{func}: Save skipped events")
                    json.dump(skipped_events, open(TEMP_SKIPPED_EVENTS_FILENAME, "w"))

                    print(f"{func}: Upload skipped events to S3")
                    current_timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %z")
                    skipped_chunk_object_key = f"skipped_events/{current_timestamp}/input.json"
                    try:
                        S3_CLIENT.upload_file(TEMP_SKIPPED_EVENTS_FILENAME, bucket, skipped_chunk_object_key)
                    except Exception as e:
                        print(f"{func}: Failed to save file to S3: {bucket}/{skipped_chunk_object_key}")
                        print(e)

                if len(tranformed_events) > 0:
                    print(f"{func}: save transformed cef mapped chunk events")
                    json.dump(tranformed_events, open(chunk_of_large_input_file, "w"))
                    process_chunks.append(chunk_of_large_input_file)
                    keep_unique_ids.append(unique_id)
                else:
                    print(f"{func}: No events in transformed events, skip batch")

                try:
                    del events
                    del tranformed_events
                    del skipped_events
                    gc.collect()
                except Exception as e:
                    print(f"{func}: Failed to clear local variables. Continue anyways.")

            # save node features
            if node_feature.keys():
                print(f"{func}: Save node features")
                NODE_FEATURE_EXISTS_IN_REQUEST = True
                json.dump(node_feature, open(TEMP_NODE_FEATURE_INPUT_FILENAME, "w"))
                json.dump(node_feature, open(TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME, "w"))

            return process_chunks, keep_unique_ids


        def get_3_words(dict_obj):
            """
            Extracts keys and values from a dictionary based on having three words and returns a concatenated string of values.

            Parameters:
            dict_obj (dict): The input dictionary containing key-value pairs to be processed.
            black_list (list): A list of keys to be ignored during processing.
            english_words (list): A list of valid English words to be used for counting.

            Returns:
            str: A concatenated string of values from `dict_obj` that meet the specified conditions.
            """
            global BLACK_LIST_KEYS
            global ENGLISH_WORDS

            keys = []
            output_str = ""
            for key in dict_obj.keys():
                if key in BLACK_LIST_KEYS:
                    continue

                target_str  = str(dict_obj[key]).lower()

                if len(target_str) < 4:
                    continue

                # Split the sentence into words
                words = re.findall(r'(?<=\s)\w+|\w+(?=\s)|^\w+|\w+$', target_str)

                # Count English words
                english_word_count = sum(1 for word in words if word in ENGLISH_WORDS)

                if english_word_count >= 3:
                    if output_str:
                        output_str += ' '
                    output_str += f'{target_str}'
                    keys.append([key,dict_obj[key]])
            return output_str


        def map_cef_to_internal(event, cef_to_internal_mappings, aliases_json=None, field_types_json=None):
            '''
            Map cef fields to internal
            Args:
                event: event from chunk of input
                cef_to_internal_mappings: cef to internal mappings list
                aliases_json: node to entitiy mapping for node translation
                field_types_json: field mapping for node translation
            Returns:
                tranformed event mapped from cef to internal, cef mapped keys to internal event, event_node_feature
            '''

            global NODE_TRANSLATION
            global NODE_ENCODING

            self_loop = None
            self_loop_key = None

            transformed_event = {
                "id": "",
                "src": "",
                "dst": "",
                "time": "",
                "name": "",
                "other_attributes_dict": {}
            }

            other_attrs = {}
            event_node_feature = {}

            cef_mapped_keys_to_event = {
                "time": "",
                "src": "",
                "dst": "",
                "name": ""
            }

            for mapping in cef_to_internal_mappings:
                src_field = mapping["source_field"]
                dst_field = mapping["dest_field"]
                if src_field not in event:
                    continue
                src_field_value = event[src_field]
                if src_field_value and src_field_value != "":
                    if dst_field in ["id"]:
                        transformed_event["id"] = str(src_field_value)
                    elif dst_field in ["dst"]:
                        transformed_event["dst"] = src_field_value
                        cef_mapped_keys_to_event["dst"] = src_field
                    elif dst_field in ["src"]:
                        transformed_event["src"] = src_field_value
                        cef_mapped_keys_to_event["src"] = src_field
                    elif dst_field in ["time"]:
                        transformed_event["time"] = src_field_value
                        cef_mapped_keys_to_event["time"] = src_field
                    elif dst_field in ["name"]:
                        transformed_event["name"] = str(src_field_value)
                        cef_mapped_keys_to_event["name"] = src_field

                    elif dst_field in ["self_loop_src (when both dst & src r empty) "]:
                        self_loop = src_field_value
                        self_loop_key = src_field

                    elif dst_field in ["event_ftr_IMPRTNT_priority"]:
                        try:
                            src_field_value = int(src_field_value)
                            other_attrs["priority"] = src_field_value
                        except Exception as e:
                            pass
                    elif dst_field in ["src_port", "dst_port"]:
                        try:
                            src_field_value = int(src_field_value)
                            other_attrs["port"] = src_field_value
                        except Exception as e:
                            pass
                    elif dst_field in ["event_ftr", "event_ftr_IMPRTNT"]:
                        other_attrs[src_field] = src_field_value

            # add other attributes for node translation
            if NODE_TRANSLATION:
                for src_field in field_types_json["src"]:
                    if src_field not in event:
                        continue

                    src_field_value = event[src_field]
                    if src_field_value and src_field_value != "":
                        other_attrs[src_field] = src_field_value

                for src_field in field_types_json["dst"]:
                    if src_field not in event:
                        continue

                    src_field_value = event[src_field]
                    if src_field_value and src_field_value != "":
                        other_attrs[src_field] = src_field_value

            transformed_event["other_attributes_dict"].update(other_attrs)

            # override src, dst if present. will lose track of cef mapped key for src, dst
            if NODE_TRANSLATION:
                transformed_event = node_translation(event=transformed_event, aliases=aliases_json, all_keys=field_types_json)

            for mapping in cef_to_internal_mappings:
                src_field = mapping["source_field"]
                dst_field = mapping["dest_field"]

                if dst_field not in ["dst_node_ftr", "src_node_ftr", "self_loop_node_ftr"]:
                    continue

                if src_field not in event:
                    continue

                src_field_value = event[src_field]
                if src_field_value != "":
                    transformed_event["other_attributes_dict"][src_field] = src_field_value

            # if none of the src, dst is present, then use self loop
            if transformed_event["src"] and not transformed_event["dst"]:
                transformed_event["dst"] = transformed_event["src"]
                cef_mapped_keys_to_event["dst"] = cef_mapped_keys_to_event["src"]

            elif not transformed_event["src"] and transformed_event["dst"]:
                transformed_event["src"] = transformed_event["dst"]
                cef_mapped_keys_to_event["src"] = cef_mapped_keys_to_event["dst"]

            elif not transformed_event["src"] and not transformed_event["dst"]:
                # self loop get the cef field for the self loop
                if self_loop is not None:
                    transformed_event["src"] = self_loop
                    transformed_event["dst"] = self_loop
                    cef_mapped_keys_to_event["src"] = self_loop_key
                    cef_mapped_keys_to_event["dst"] = self_loop_key
                else:
                    # if self loop is absent too, then required fields src, dest cannot be mapped. skip event
                    return None, cef_mapped_keys_to_event, event_node_feature

            # if required fields are empty, skip event:
            if not transformed_event["time"]:
                return None, cef_mapped_keys_to_event, event_node_feature

            # get name

            if not transformed_event["name"]:
                transformed_event["name"] = get_3_words(event)

            # convert to unix timestamp
            unix_timestamp = convert_to_unix_timestamp(transformed_event["time"])
            if unix_timestamp is None:
                return None, cef_mapped_keys_to_event, event_node_feature
            transformed_event["time"] = unix_timestamp

            self_loop_exists = (transformed_event["src"] == transformed_event["dst"])

            # get tech
            tech_list = get_tech(event)
            if tech_list:
                transformed_event["tech"] = tech_list

            # # create node feature

            # event_node_feature[transformed_event["src"]] = {}
            # event_node_feature[transformed_event["dst"]] = {}

            # for mapping in cef_to_internal_mappings:
            #     src_field = mapping["source_field"]
            #     dst_field = mapping["dest_field"]

            #     if dst_field not in ["dst_node_ftr", "src_node_ftr", "self_loop_node_ftr"]:
            #         continue

            #     if src_field not in event:
            #         continue

            #     src_field_value = event[src_field]
            #     if src_field_value != "":
            #         if dst_field in ["dst_node_ftr"]:
            #             event_node_feature[transformed_event["dst"]][src_field] = src_field_value
            #         elif dst_field in ["src_node_ftr"]:
            #             event_node_feature[transformed_event["src"]][src_field] = src_field_value
            #         elif dst_field in ["self_loop_node_ftr"]:
            #             if self_loop_exists:
            #                 event_node_feature[transformed_event["src"]][src_field] = src_field_value

            # get id
            source_type = None
            source = None
            event_time = None

            if '_sourceType' in event:
                source_type = str(event['_sourceType'])
                source_type = source_type.replace("'", "\"")
                source_type = source_type.replace("\\", "\\\\")

            if '_source' in event:
                source = str(event['_source'])
                source = source.replace("'", "\"")
                source = source.replace("\\", "\\\\")

            if '_time' in event:
                event_time = str(event["_time"])

            # event_uuid = str(uuid.uuid4())
            event_uuid = str(event["_event_id"])

            transformed_event["id"] = f'''"{transformed_event['src']}" "{transformed_event['dst']}"'''

            if event_time and event_time != "":
                transformed_event["id"] = f'''{transformed_event["id"]} _time="{event_time}"'''

            if source and source != "":
                transformed_event["id"] = f'''{transformed_event["id"]} source="{source}"'''

            if source_type and source_type != "":
                transformed_event["id"] = f'''{transformed_event["id"]} sourcetype="{source_type}"'''

            transformed_event["id"] = f'''{transformed_event["id"]} id:{event_uuid}'''

            return transformed_event, cef_mapped_keys_to_event, event_node_feature


        def get_tech(event):
            """
            Extracts tech using regex if present in raw data

            Args:
                event: The input dictionary containing key-value pairs to be processed.

            Returns: List of strings of tech
            """

            tech_list = []
            for key in event.keys():

                if not isinstance(event[key], str):
                    continue

                # Find tech in value
                words = re.findall(r'T[0-9]{4}|t[0-9]{4}', event[key])

                tech_list.extend(words)

            tech_list = [f"T{tech[1:]}" for tech in tech_list]
            tech_list = list(set(tech_list))
            return tech_list


        def preprocess_chunk_input(unique_id, chunk_of_large_input_file):
            '''
            Generate internal id and update chunk input
            Args:
                unique_id: The unique identifier for the data chunk.
                chunk_of_large_input_file: Chunk of input file
            '''

            chunk_input_df = pd.read_json(chunk_of_large_input_file)

            start_id, end_id = create_internal_id(unique_id, chunk_input_df["id"].to_list())

            chunk_input_df["id"] = list(range(start_id, end_id + 1))

            chunk_input_df.to_json(chunk_of_large_input_file, orient="records")


        def create_event_id(chunks_of_large_input_file):
            '''
            Create event id for user input
            Args:
                chunks_of_large_input_file
            '''
            func = "create_event_id"

            for chunk_of_large_input_file in chunks_of_large_input_file:
                print(f"{func}: Processing chunk: {chunk_of_large_input_file}")
                chunk_json = json.load(open(chunk_of_large_input_file, "r"))
                for alert in chunk_json:
                    alert["_event_id"] = str(uuid.uuid4())
                json.dump(chunk_json, open(chunk_of_large_input_file, "w"))


        def create_internal_id(unique_id, user_id_list):
            '''
            Create mapping from internal id to user id and internal id to unique id
            Args:
                unique_id: The unique identifier for the data chunk.
                user_id_list: List of user ids to map to internal id.
            Returns:
                Tuple containing the start and end internal id.
            '''
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE
            global TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE

            func = "create_internal_id"

            print(f"{func}: Create mapping from internal id to user id")
            temp_internal_id_to_user_id_json = json.load(open(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, "r"))

            start_id = len(temp_internal_id_to_user_id_json)
            temp_internal_id_to_user_id_json.extend(user_id_list)
            end_id = len(temp_internal_id_to_user_id_json) - 1

            print(f"{func}: Save mapping from internal id to user id")
            json.dump(temp_internal_id_to_user_id_json, open(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, "w"))

            print(f"{func}: Create mapping from internal id to unique id")
            temp_internal_id_to_unique_id_json = json.load(open(TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE, "r"))

            temp_internal_id_to_unique_id_json = temp_internal_id_to_unique_id_json + [unique_id] * len(user_id_list)

            print(f"{func}: Save mapping from internal id to unique id")
            json.dump(temp_internal_id_to_unique_id_json, open(TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE, "w"))

            return start_id, end_id


        def skip_creating_batch_transform_job(bucket, unique_id, first_unique_id, chunk_input_json):
            '''
            Skip creating a batch transform job and upload enriched alerts with techniques as input to cluster detection.
            Args:
                bucket: The S3 bucket name.
                unique_id: The unique id for classification.
                first_unique_id: first unique id of batch of input
                chunk_input_json: JSON data for chunk input.
            '''
            global SCRATCH_DIR
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_USER_FEEDBACK_WEIGHTS
            global S3_CLIENT

            func = "skip_creating_batch_transform_job"

            skip_batch_dir = "/tmp/skip_batch/"
            os.makedirs(skip_batch_dir, exist_ok=True)
            temp_enriched_alerts_with_techniques_output_filename = f"{skip_batch_dir}input.json"

            path_to_enriched_alerts_with_techniques = f"{SCRATCH_DIR}/output/classification/{first_unique_id}/input_{unique_id}.json"

            user_feedback_weights_json = json.load(open(TEMP_USER_FEEDBACK_WEIGHTS, "r"))
            edge_weights = user_feedback_weights_json["event"]

            with open(temp_enriched_alerts_with_techniques_output_filename, "w") as of:
                req = {"request_id": "1", "input": chunk_input_json, "user_feedback_weights": edge_weights}

                if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                    req["cluster_config"] = json.load(open(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, "r"))

                if NODE_FEATURE_EXISTS_IN_REQUEST:
                    req["node_feature"] = json.load(open(TEMP_NODE_FEATURE_INPUT_FILENAME, "r"))
                json.dump(req, of)

            try:
                print(f"{func}: Save enriched alerts with techniques as input to cluster detection: {bucket}/{path_to_enriched_alerts_with_techniques}")
                S3_CLIENT.upload_file(temp_enriched_alerts_with_techniques_output_filename, bucket, path_to_enriched_alerts_with_techniques)
            except Exception as e:
                print(f"{func}: Failed to save enriched alert with technique: {bucket}/{path_to_enriched_alerts_with_techniques}")
                raise e


        def create_agg_cluster_flow_output(bucket, unique_id, chunk_intermediate_input_local_filename):
            '''
            Create agg alerts, cluster output and flow output for user override cluster
            Args:
                bucket: Bucket name
                unique_id: unique id of current batch
                chunk_intermediate_input_local_filename: batch input file
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_OVERRIDE_AGG_OUTPUT
            global TEMP_OVERRIDE_CLUSTER_OUTPUT
            global TEMP_OVERRIDE_FLOW_OUTPUT
            global TEMP_OVERRIDE_FLOW_DICT
            global TEMP_CLUSTER_OUTPUT_ZIP
            func = "create_agg_cluster_flow_output"

            chunk_input_json = json.load(open(chunk_intermediate_input_local_filename, "r"))

            agg_json = []
            cluster_output_json = []
            cluster_output_dict = {}
            flow_output_dict = {}
            if os.path.exists(TEMP_OVERRIDE_FLOW_DICT):
                flow_output_dict = json.load(open(TEMP_OVERRIDE_FLOW_DICT, "r"))

            for alert in chunk_input_json:
                agg_alert = copy.copy(alert)
                agg_alert["aggalert_id"] = agg_alert["id"]
                agg_alert["ids"] = [agg_alert["id"]]
                agg_json.append(agg_alert)

                if agg_alert["cluster_id"] not in cluster_output_dict:
                    cluster_output_dict[agg_alert["cluster_id"]] = {
                        "cluster_aggalertids": [agg_alert["aggalert_id"]]
                    }
                else:
                    cluster_output_dict[agg_alert["cluster_id"]]["cluster_aggalertids"].append(agg_alert["aggalert_id"])

                if "flow_ids" in agg_alert:
                    for flow_id in agg_alert["flow_ids"]:
                        flow_id = str(flow_id)
                        if flow_id not in flow_output_dict:
                            flow_output_dict[flow_id] = {
                                "cluster_ids": [agg_alert["cluster_id"]]
                            }
                        else:
                            flow_output_dict[flow_id]["cluster_ids"].append(agg_alert["cluster_id"])

                        flow_output_dict[flow_id]["cluster_ids"] = list(set(flow_output_dict[flow_id]["cluster_ids"]))

            for k, v in cluster_output_dict.items():
                cluster_output = {
                    "cluster_id": k,
                    "cluster_aggalertids": v["cluster_aggalertids"]
                }
                cluster_output_json.append(cluster_output)

            json.dump(agg_json, open(TEMP_OVERRIDE_AGG_OUTPUT, "w"))
            json.dump(cluster_output_json, open(TEMP_OVERRIDE_CLUSTER_OUTPUT, "w"))
            json.dump(flow_output_dict, open(TEMP_OVERRIDE_FLOW_DICT, "w"))

            with zipfile.ZipFile(TEMP_CLUSTER_OUTPUT_ZIP, "w") as z:
                z.write(TEMP_OVERRIDE_AGG_OUTPUT, arcname="alert_output.json")
                z.write(TEMP_OVERRIDE_CLUSTER_OUTPUT, arcname="cluster_output.json")

            path_to_cluster_output = f"{SCRATCH_DIR}/response/cluster_out/{unique_id}/cluster_output.zip.out"
            S3_CLIENT.upload_file(TEMP_CLUSTER_OUTPUT_ZIP, bucket, path_to_cluster_output)


        def create_flow_output(bucket, last_unique_id):
            '''
            Create flow output and upload to output folder
            Args:
                bucket: Bucket name
                last_unique_id: unique id of last batch of input
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_OVERRIDE_FLOW_DICT
            global TEMP_OVERRIDE_FLOW_OUTPUT
            func = "create_flow_output"

            flow_output_dict = json.load(open(TEMP_OVERRIDE_FLOW_DICT, "r"))

            flow_output_json = []
            for k, v in flow_output_dict.items():
                flow = {
                    "Flow_id": int(k),
                    "cluster_ids": v["cluster_ids"]
                }
                flow_output_json.append(flow)

            flow_output_json_response = [
                {
                    "output": {
                        "flow_output": flow_output_json
                    }
                }
            ]

            json.dump(flow_output_json_response, open(TEMP_OVERRIDE_FLOW_OUTPUT, "w"))

            path_to_flow_output = f"{SCRATCH_DIR}/response/flow_out/{last_unique_id}/flow_output.json.out"
            S3_CLIENT.upload_file(TEMP_OVERRIDE_FLOW_OUTPUT, bucket, path_to_flow_output)


        def fetch_or_create_technique_classification_lookup_file(bucket):
            '''
            Check if the technique classification lookup file for alerts exists on S3. Create if it does not exist.
            Args:
                bucket: Bucket name
            '''
            global TECHNIQUE_LOOKUP_OBJECT
            global S3_CLIENT
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE

            func = "fetch_or_create_technique_classification_lookup_file"
            try:
                print(
                    f"{func}: Checking if technique classification lookup file for alerts exists on S3 path {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")

                S3_CLIENT.head_object(Bucket=bucket, Key=TECHNIQUE_LOOKUP_OBJECT)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Technique classification lookup file for alerts does not exist on S3. Create empty table")

                technique_classification_lookup_df = pd.DataFrame(columns=["alerts", "techniques"])
                technique_classification_lookup_df.to_csv(TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE, index=False)

                print(f"{func}: Empty table created")
            else:
                try:
                    print(f"{func}: Download available technique classification lookup file for alerts from S3")

                    S3_CLIENT.download_file(bucket, TECHNIQUE_LOOKUP_OBJECT, TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup table from S3: {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")
                    raise e


        def fetch_or_create_internal_id_to_user_id_lookup_file(bucket):
            '''
            Check if the internal id to user id lookup file exists on S3. Create if it does not
            Args:
                bucket: Bucket name
            '''
            global INTERNAL_ID_TO_USER_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE

            func = "fetch_or_create_internal_id_to_user_id_lookup_file"

            try:
                print(
                    f"{func}: Checking if internal id to user id lookup file exists on S3 path {bucket}/{INTERNAL_ID_TO_USER_ID_OBJECT_KEY}")

                S3_CLIENT.head_object(Bucket=bucket, Key=INTERNAL_ID_TO_USER_ID_OBJECT_KEY)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Internal id to user id lookup file does not exist on S3. Create empty table")

                internal_id_to_user_id_json = []
                json.dump(internal_id_to_user_id_json, open(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, "w"))

                print(f"{func}: Empty table created")
            else:
                try:
                    print(f"{func}: Download available internal id to user id lookup file from S3")

                    S3_CLIENT.download_file(bucket, INTERNAL_ID_TO_USER_ID_OBJECT_KEY, TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{INTERNAL_ID_TO_USER_ID_OBJECT_KEY}")
                    raise e


        def fetch_or_create_internal_id_to_unique_id_lookup_file(bucket):
            '''
            Check if the internal id to unique id lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE

            func = "fetch_or_create_internal_id_to_unique_id_lookup_file"

            try:
                print(
                    f"{func}: Checking if internal id to unique id lookup file exists on S3 path {bucket}/{INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY}")

                S3_CLIENT.head_object(Bucket=bucket, Key=INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Internal id to unique id lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = []
                json.dump(internal_id_to_unique_id_json, open(TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE, "w"))

                print(f"{func}: Empty table created")
            else:
                try:
                    print(f"{func}: Download available uinternal id to unique id lookup file from S3")

                    S3_CLIENT.download_file(bucket, INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY, TEMP_INTERNAL_ID_TO_UNIQUE_ID_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{INTERNAL_ID_TO_UNIQUE_ID_OBJECT_KEY}")
                    raise e


        def fetch_or_create_queue_lookup_file(bucket):
            '''
            Check if the queue lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global QUEUE_LOOKUP_FILE
            global S3_CLIENT
            global TEMP_QUEUE_LOOKUP_FILE

            func = "fetch_or_create_queue_lookup_file"

            try:
                print(
                    f"{func}: Checking if queue lookup file exists on S3 path {bucket}/{QUEUE_LOOKUP_FILE}")

                S3_CLIENT.head_object(Bucket=bucket, Key=QUEUE_LOOKUP_FILE)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Queue lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = {"first": None, "prev": None, "input_queue": []}
                json.dump(internal_id_to_unique_id_json, open(TEMP_QUEUE_LOOKUP_FILE, "w"))

                print(f"{func}: Initiated queue lookup")
            else:
                try:
                    print(f"{func}: Download available queue lookup file from S3")

                    S3_CLIENT.download_file(bucket, QUEUE_LOOKUP_FILE, TEMP_QUEUE_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{QUEUE_LOOKUP_FILE}")
                    raise e


        def fetch_or_create_global_attribute_weights(bucket, path_to_global_attribute_weights):
            '''
            Get or initialize the global attribute weights file
            Args:
                path_to_global_attribute_weights: global attribute weights object key
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_USER_FEEDBACK_WEIGHTS
            func = "fetch_or_create_global_attribute_weights"

            try:
                print(
                    f"{func}: Checking if file exists on S3 path {bucket}/{path_to_global_attribute_weights}")

                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_global_attribute_weights)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: File does not exist on S3. Initialize file")

                global_attribute_weights_json = {
                    "event": {
                        "technique": INITIAL_WEIGHTS_VALUE,
                        "tactic": INITIAL_WEIGHTS_VALUE,
                        "stage": INITIAL_WEIGHTS_VALUE,
                        "count": INITIAL_WEIGHTS_VALUE,
                        "priority": INITIAL_WEIGHTS_VALUE,
                        "port": INITIAL_WEIGHTS_VALUE,
                        "url": INITIAL_WEIGHTS_VALUE,
                        "user_agent": INITIAL_WEIGHTS_VALUE,
                        "cert": INITIAL_WEIGHTS_VALUE
                    },
                    "node": {
                        "os": INITIAL_WEIGHTS_VALUE,
                        "risk": INITIAL_WEIGHTS_VALUE,
                        "user": INITIAL_WEIGHTS_VALUE,
                        "domain": INITIAL_WEIGHTS_VALUE,
                        "subnet": INITIAL_WEIGHTS_VALUE,
                        "usergroup": INITIAL_WEIGHTS_VALUE,
                        "geolocation": INITIAL_WEIGHTS_VALUE
                    }
                }
                json.dump(global_attribute_weights_json, open(TEMP_USER_FEEDBACK_WEIGHTS, "w"))
                print(f"{func}: Initiated file")
                try:
                    print(f"{func}: Upload file to S3: {bucket}/{path_to_global_attribute_weights}")
                    S3_CLIENT.upload_file(TEMP_USER_FEEDBACK_WEIGHTS, bucket, path_to_global_attribute_weights)
                except Exception as ex:
                    print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_global_attribute_weights}")
                    raise ex
            else:
                try:
                    print(f"{func}: Download available file from S3")

                    S3_CLIENT.download_file(bucket, path_to_global_attribute_weights, TEMP_USER_FEEDBACK_WEIGHTS)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{path_to_global_attribute_weights}")
                    raise e


        def get_node_translation_encoder_files(bucket):
            '''
            Get node translation, encoder files if present in S3
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_ALIASES_FILE
            global TEMP_FIELD_TYPES_FILE
            global TEMP_FEATURE_MAPPING_FILE
            global NODE_TRANSLATION
            global NODE_ENCODING
            func = "fetch_or_create_global_attribute_weights"

            path_to_aliases = "node/aliases.json"
            path_to_field_types = "node/field_types.json"
            path_to_feature_mapping = "node/feature_mapping.json"

            try:
                print(
                    f"{func}: Checking if file exists on S3 path {bucket}/{path_to_aliases}")

                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_aliases)

                print(
                    f"{func}: Checking if file exists on S3 path {bucket}/{path_to_field_types}")

                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_field_types)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

            else:
                try:
                    print(f"{func}: Download available file from S3")

                    S3_CLIENT.download_file(bucket, path_to_aliases, TEMP_ALIASES_FILE)
                    S3_CLIENT.download_file(bucket, path_to_field_types, TEMP_FIELD_TYPES_FILE)

                    NODE_TRANSLATION = True

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file from S3")
                    raise e

            try:
                print(
                    f"{func}: Checking if file exists on S3 path {bucket}/{path_to_feature_mapping}")

                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_feature_mapping)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

            else:
                try:
                    print(f"{func}: Download available file from S3")

                    S3_CLIENT.download_file(bucket, path_to_feature_mapping, TEMP_FEATURE_MAPPING_FILE)

                    NODE_ENCODING = True

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file from S3")
                    raise e


        def update_queue_lookup(bucket, unique_ids):
            '''
            Update queue lookup file
            Args:
                bucket: Bucket name
                unique_ids: Unique ids generated for the current input file
            '''
            global TEMP_QUEUE_LOOKUP_FILE
            global QUEUE_LOOKUP_FILE
            global S3_CLIENT
            func = "update_queue_lookup"

            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            payload_json = {}

            if queue_lookup_json["first"] is None:
                payload_json["first"] = unique_ids[0]
                queue_lookup_json["first"] = unique_ids[0]

            queue_lookup_json["prev"] = unique_ids[-1]
            payload_json["prev"] = unique_ids[-1]

            log_json = {"message": "update queue lookup", "payload": payload_json}
            print(f"{json.dumps(log_json)}")


        def create_chunk_for_large_input_file():
            '''
            Create chunks for a large input file
            Returns:
                List of split file names
            '''
            global CHUNK_SIZE
            global BUF_SIZE
            global TEMP_INPUT_DIR
            global TEMP_INPUT_FILENAME

            split_files = []
            with open(TEMP_INPUT_FILENAME, 'rb') as f:
                split_files = []
                item = 0
                input_list_iter = ijson.items(f, 'input.item', use_float=True, buf_size=BUF_SIZE)
                while True:
                    input_list = list(islice(input_list_iter, CHUNK_SIZE))
                    split_filename = TEMP_INPUT_DIR + "input" + '_' + str(item) + '.json'
                    if not input_list:
                        break

                    with open(split_filename, 'w') as outfile:
                        json.dump(input_list, outfile)
                        split_files.append(split_filename)

                    item += 1
                    del input_list
                    gc.collect()

            del input_list_iter
            gc.collect()

            return split_files


        def process_input_file_to_extract_config_and_node_feature():
            '''
            Process the input file to extract config and node feature
            '''
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global CONFIG_EXISTS_IN_REQUEST
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global BUF_SIZE
            global TEMP_INPUT_FILENAME
            global TEMP_CONFIG_INPUT_FILENAME
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME

            func = "process_input_file_to_extract_config_and_node_feature"

            with open(TEMP_INPUT_FILENAME, 'rb') as f:
                cluster_config_json_object = next(ijson.items(f, 'cluster_config', use_float=True, buf_size=BUF_SIZE), None)
                if cluster_config_json_object:
                    print(f"{func}: Cluster config exists in request")
                    CLUSTER_CONFIG_EXISTS_IN_REQUEST = True
                    if not isinstance(cluster_config_json_object, dict):
                        raise ValueError("The 'cluster_config' object in input file must be a json object")
                    json.dump(cluster_config_json_object, open(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, "w"))

            del cluster_config_json_object
            gc.collect()

            with open(TEMP_INPUT_FILENAME, 'rb') as f:
                config_json_object = next(ijson.items(f, 'config', use_float=True, buf_size=BUF_SIZE), None)
                if config_json_object:
                    print(f"{func}: Config exists in request")
                    CONFIG_EXISTS_IN_REQUEST = True
                    if not isinstance(config_json_object, dict):
                        raise ValueError("The 'config' object in input file must be a json object")
                    json.dump(config_json_object, open(TEMP_CONFIG_INPUT_FILENAME, "w"))

            del config_json_object
            gc.collect()

            with open(TEMP_INPUT_FILENAME, 'rb') as f:
                node_feature = next(ijson.items(f, 'node_feature', use_float=True, buf_size=BUF_SIZE), None)
                if node_feature:
                    print(f"{func}: node feature exists in request")
                    NODE_FEATURE_EXISTS_IN_REQUEST = True
                    if not isinstance(node_feature, dict):
                        raise ValueError("The 'node_feature' object in input file must be a json object")
                    json.dump(node_feature, open(TEMP_NODE_FEATURE_INPUT_FILENAME, "w"))
                    json.dump(node_feature, open(TEMP_NODE_FEATURE_ORIGINAL_INPUT_FILENAME, "w"))

            del node_feature
            gc.collect()


        def initialize_sqlite():
            '''
            Initialize sqlite if database is not present
            '''
            global DB_CONNECTION_STRING

            func = "initialize_sqlite"

            print(f"{func}: Initialize database.")

            create_cluster_output_table = """
            CREATE TABLE IF NOT EXISTS cluster_output (
                cluster_id INTEGER PRIMARY KEY,
                campaign_id INTEGER UNIQUE,
                cluster_starttime DOUBLE PRECISION,
                cluster_endtime DOUBLE PRECISION,
                cluster_srcips TEXT,
                cluster_dstips TEXT,
                cluster_techs TEXT,
                cluster_tacs TEXT,
                cluster_stages TEXT
            );
            """

            create_cluster_ticket_output_table = """
            CREATE TABLE IF NOT EXISTS cluster_ticket_output (
                cluster_id  INTEGER PRIMARY KEY,
                ticket_id INTEGER UNIQUE,
                metrics TEXT,
                FOREIGN KEY (cluster_id) REFERENCES cluster_output(cluster_id) ON DELETE CASCADE
            );
            """

            create_event_table = """
            CREATE TABLE IF NOT EXISTS event (
                alert_id TEXT PRIMARY KEY,
                event_id TEXT UNIQUE,
                internal_id INTEGER,
                unique_id TEXT,
                cluster_id INTEGER,
                cluster_added_to_ui INTEGER DEFAULT 0,
                flow_added_to_ui INTEGER DEFAULT 0,
                src TEXT,
                dst TEXT,
                time DOUBLE PRECISION,
                name TEXT,
                tech TEXT,
                tac TEXT,
                stage TEXT,
                other_attributes TEXT,
                raw_data TEXT,
                FOREIGN KEY (cluster_id) REFERENCES cluster_output(cluster_id) ON DELETE SET NULL
            );
            """

            create_flow_output_table = """
            CREATE TABLE IF NOT EXISTS flow_output (
                flow_id  INTEGER PRIMARY KEY,
                campaign_id INTEGER UNIQUE,
                cluster_prob TEXT,
                alert_ids TEXT
            );
            """

            create_operation_on_cluster_table = """
            CREATE TABLE IF NOT EXISTS operation_on_cluster (
                operation_id SERIAL PRIMARY KEY,
                cluster_id INTEGER,
                alert_ids TEXT,
                operation_type TEXT,
                FOREIGN KEY (cluster_id) REFERENCES cluster_output(cluster_id) ON DELETE CASCADE
            );
            """

            create_operation_on_flow_table = """
            CREATE TABLE IF NOT EXISTS operation_on_flow (
                operation_id SERIAL PRIMARY KEY,
                flow_id INTEGER,
                alert_ids TEXT,
                operation_type TEXT,
                FOREIGN KEY (flow_id) REFERENCES flow_output(flow_id) ON DELETE CASCADE
            );
            """

            create_global_feature_table = """
            CREATE TABLE IF NOT EXISTS global_feature (
                feature TEXT NOT NULL,
                feature_type TEXT NOT NULL,
                added_to_ui INTEGER,
                CONSTRAINT unique_feature_type UNIQUE (feature, feature_type)
            );
            """

            try:
                # Connect to the database
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        # Execute a query
                        cursor.execute(create_cluster_output_table)
                        cursor.execute(create_cluster_ticket_output_table)
                        cursor.execute(create_event_table)
                        cursor.execute(create_flow_output_table)
                        cursor.execute(create_operation_on_cluster_table)
                        cursor.execute(create_operation_on_flow_table)
                        cursor.execute(create_global_feature_table)
                    conn.commit()

                print(f"{func}: Database initialized")
            except Exception as ex:
                print(f"{func}: Failed to create tables in database.")
                raise ex


        def node_translation(event, aliases, all_keys):
            """
            Translate event's 'src' and 'dst' attributes to corresponding node IDs in the graph.
            If a event is missing a mapping for src or dst the raw value is left

            Parameters:
                event (dict): Event object with 'other_attributes_dict' containing attribute values.
                aliases (dict): Mapping of node names to their IDs.
                all_keys (dict): Dictionary containing keys for 'src' and 'dst' attributes.

            Returns:
                dict: The input event object with 'src' and 'dst' attributes updated to their corresponding node IDs.

            Raises:
                Exception: If more than one node is mapped for either 'src' or 'dst'.
            """
            aliases_keys = set(aliases.keys())

            # src mapping
            events_values = set([event['other_attributes_dict'][key] for key in all_keys['src'] if key in event['other_attributes_dict']])
            matched_keys = aliases_keys.intersection(events_values)
            matching_nodes = set([aliases[x] for x in matched_keys])

            if len(matching_nodes) == 1:
                event['src'] = matching_nodes.pop()
            elif len(matching_nodes) > 1:
                raise Exception("Error: more than one mapped SRC node")

            # dst mapping
            events_values = set([event['other_attributes_dict'][key] for key in all_keys['dst'] if key in event['other_attributes_dict']])
            matched_keys = aliases_keys.intersection(events_values)
            matching_nodes = set([aliases[x] for x in matched_keys])

            if len(matching_nodes) == 1:
                event['dst'] = matching_nodes.pop()
            elif len(matching_nodes) > 1:
                raise Exception("Error: more than one mapped DST node")

            return event


        def node_feature_encoding(event, feature_mapping, node_features):
            """
            Encodes node features from an event into a dictionary.

            This function checks if the source node of an event exists in the `node_features` dictionary.
            If not, it creates a new entry for the node with default values based on the provided `feature_mapping`.
            Then, it extracts relevant features from the event and updates the node's feature dictionary accordingly.

            Args:
                event (dict): An event containing a source node and other attributes.
                feature_mapping (dict): A dictionary mapping feature names to their possible values, along with default values.
                node_features (dict): A dictionary of node features, where each key is a node ID and each value is a dictionary of features.

            Returns:
                dict: The updated `node_features` dictionary with the encoded features from the event.

            """
            feature_keys = set(feature_mapping.keys())

            # node is not in node feature list then make the node
            if event['src'] not in node_features:
                tmp = {}
                # make all features at defautl value
                for key in feature_keys:
                    tmp[key] = feature_mapping[key][0] * feature_mapping[key][2]
                node_features[event['src']] = tmp

            # get node featuers from the event
            matched_keys = list(feature_keys.intersection(set(event['other_attributes_dict'].keys())))

            # map the node features
            for key in matched_keys:
                print(feature_mapping[key][1])
                if event['other_attributes_dict'][key] in feature_mapping[key][1]:
                    node_features[event['src']][key] = feature_mapping[key][1][event['other_attributes_dict'][key]] * feature_mapping[key][2]

            return node_features


        def process_node_features():
            '''
            Encode node features
            '''
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_USER_FEEDBACK_WEIGHTS
            global TEMP_NODE_FEATURE_LOOKUP_FILE
            func = "process_node_features"

            node_feature = json.load(open(TEMP_NODE_FEATURE_INPUT_FILENAME, "r"))
            user_feedback_weights = json.load(open(TEMP_USER_FEEDBACK_WEIGHTS, "r"))
            node_weights = user_feedback_weights["node"]

            node_feature_lookup = {}

            for key in node_feature.keys():
                if type(node_feature[key]) is dict:
                    for key_2 in node_feature[key]:
                        if key_2 not in node_feature_lookup:
                            node_feature_lookup[key_2] = len(node_feature_lookup)
                else:
                    print(f"{func}: Skip node feature: {key}")

            new_node_feature = {}
            encode_table = {}
            for key in node_feature.keys():
                if type(node_feature[key]) is dict:
                    new_node_feature_embedding = [0] * len(node_feature_lookup)
                    for key_2 in node_feature[key]:
                        if node_feature[key][key_2]:
                            encoded, encode_table = custom_encoder_node(node_feature[key][key_2], encode_table)
                        else:
                            encoded = 0
                        if key_2 in node_weights:
                            encoded = encoded * (node_weights[key_2] / 100.0)
                        new_node_feature_embedding[node_feature_lookup[key_2]] = encoded
                    new_node_feature[key] = new_node_feature_embedding

            json.dump(new_node_feature, open(TEMP_NODE_FEATURE_INPUT_FILENAME, "w"))
            json.dump(node_feature_lookup, open(TEMP_NODE_FEATURE_LOOKUP_FILE, "w"))


        def custom_encoder_node(data, encode_table):
            '''
            Simple encoder
            Args:
                data: data to encode
                encode_table: history
            Returns:
                (encoded data, updated history)
            '''
            if str(data) not in encode_table:
                encode_table[str(data)] = len(encode_table)
            return encode_table[str(data)], encode_table


        def custom_encoder(data, encode_table):
            '''
            Simple encoder
            Args:
                data: data to encode
                encode_table: history
            Returns:
                (encoded data, updated history)
            '''
            if str(data) not in encode_table:
                encode_table[str(data)] = len(encode_table)
            return [encode_table[str(data)]], encode_table


        def get_cef_to_internal_mappings():
            '''
            Create cef to internal mappings dataframe
            Returns:
                dataframe of cef to internal mappings
            '''
            cef_to_internal_mappings = [
                {
                    "source_field": "accessGroup",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "accountName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientLogonId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientMachineName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "clientUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddress",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddress",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceAddressIPv6",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceCustomString1",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceHostName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceNtHost",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "domain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "group",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "groupDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "groupTypeChange",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "hostname",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonAccount",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonAccount",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberDn",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "memberNtDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "primaryDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "primaryUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectAccountDomain",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectAccountName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectDomainName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectLogonId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectLogonId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectSecurityId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "subjectUserName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "suser",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "user",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userGroup",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userGroupId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userId",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "userType",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "workstation",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "workstationName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "workstationName",
                    "dest_field": "self_loop_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "acl",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "act",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "act",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachDisposition",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachFileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachSize",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachSizeDecoded",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachTransferEncoding",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "attachType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "authMethod",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "awsAccountId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "body",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bucket",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytesIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "bytesOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerComputerName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerDomain",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerLogonId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerMachineName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "callerUserName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cat",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cat",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "categoryString",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cd",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeClass",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeDescription",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "changeType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaMessageId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaUser",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ciscoAsaVendorAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn1",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn2",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn3",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn4",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn5",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cn6",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "command",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "commProto",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "content",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "count",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "creatorProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs1",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs1Label",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs2",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs3",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs4",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs5",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "cs6",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceDirection",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceProduct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceProduct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceReceiptTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVendor",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVendor",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVersion",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "deviceVersion",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "duration",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationHour",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationMinute",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "durationSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "errorCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventHour",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventMinute",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventMonth",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventSubtype",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventTypeColor",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventWeekDay",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventYear",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "eventZone",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "exitStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "filePath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "filePath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileSize",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fileType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "flowId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "fragmentCount",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "hashCodes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "icmpCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "icmpType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "id",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "idsType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "imageFileName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "index",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "indexTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "initialRtt",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "interfaceId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "keywords",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "kv",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "laction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "lineCount",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logLevel",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logonType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "logStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "messageId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "messageType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "name",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "name",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newAccountName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newDomain",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "newProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "object",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectAttrs",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "objectType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "opCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "outcome",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packets",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packetsIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "packetsOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcess",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "parentProcessPath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "preMsg",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "privilegeList",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "product",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "punct",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "query",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "query",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "queryType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "raw",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "recordNumber",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "region",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryPath",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryValueName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "registryValueType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "replyCode",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "replyCodeId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestClientApplication",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestMethod",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "requestURL",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "responseTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "rule",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ruleId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "securityId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serial",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "service",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serviceId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "serviceName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sessionId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "si",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "signatureId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "source",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceContent",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceNetworkAddress",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceSgInfo",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "splunkServer",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "splunkServerGroup",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "sslIsValid",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "status",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "subject",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "subSecond",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "suppliedRealmName",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tag",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagApp",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagEventType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tagObjectCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "taskCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "timeTaken",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tokenElevationType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tokenElevationType",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "tos",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalBytes",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalPacketsIn",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalPacketsOut",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "totalResponseTime",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "transactionId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "transport",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "ttl",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "type",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "type",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "valuesFlowId",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "version",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "vpcFlowAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winAction",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winSecurityCategory",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "winStatus",
                    "dest_field": "event_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationAddress",
                    "dest_field": "dst",
                    "priority": 6.0
                },
                {
                    "source_field": "destinationNtHost",
                    "dest_field": "dst",
                    "priority": 6.0
                },
                {
                    "source_field": "destinationAddress",
                    "dest_field": "dst",
                    "priority": 5.0
                },
                {
                    "source_field": "destinationAddressIPv6",
                    "dest_field": "dst",
                    "priority": 4.0
                },
                {
                    "source_field": "destinationHostName",
                    "dest_field": "dst",
                    "priority": 3.0
                },
                {
                    "source_field": "destinationTranslatedAddress",
                    "dest_field": "dst",
                    "priority": 1.0
                },
                {
                    "source_field": "destContent",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destHost",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationInterface",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationInterfaceId",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationMacAddress",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationNtDomain",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetAccountId",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationPriority",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetAccountName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destinationZone",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "destSgInfo",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "duser",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetDomain",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetDomainName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetProcessName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetServerName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetServerName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetUserName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "targetUserName",
                    "dest_field": "dst_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "description",
                    "dest_field": "name",
                    "priority": 5.0
                },
                {
                    "source_field": "name",
                    "dest_field": "name",
                    "priority": 4.0
                },
                {
                    "source_field": "ruleName",
                    "dest_field": "name",
                    "priority": 3.0
                },
                {
                    "source_field": "description",
                    "dest_field": "name",
                    "priority": 2.0
                },
                {
                    "source_field": "msg",
                    "dest_field": "name",
                    "priority": 2.0
                },
                {
                    "source_field": "message",
                    "dest_field": "name",
                    "priority": 1.0
                },
                {
                    "source_field": "destPublicPort",
                    "dest_field": "dst_port",
                    "priority": 3.0
                },
                {
                    "source_field": "destinationTranslatedPort",
                    "dest_field": "dst_port",
                    "priority": 2.0
                },
                {
                    "source_field": "destinationPort",
                    "dest_field": "dst_port",
                    "priority": 1.0
                },
                {
                    "source_field": "process",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processCommandLine",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processExec",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processId",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processId",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processName",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processName",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "processPath",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "proto",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoCode",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoFullName",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoId",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoStack",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "protoVersion",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "sourceType",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "sourceType",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendor",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorAccount",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorAction",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorClass",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorDefinition",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorPrivilege",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "vendorSeverity",
                    "dest_field": "event_ftr_IMPRTNT",
                    "priority": None
                },
                {
                    "source_field": "severity",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "severityId",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "severityLevel",
                    "dest_field": "event_ftr_IMPRTNT_priority",
                    "priority": None
                },
                {
                    "source_field": "computerName",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 6.0
                },
                {
                    "source_field": "hostAddr",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 5.0
                },
                {
                    "source_field": "assignedIp",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 4.0
                },
                {
                    "source_field": "clientAddress",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 3.0
                },
                {
                    "source_field": "ip",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 2.0
                },
                {
                    "source_field": "ipAddress",
                    "dest_field": "self_loop_src (when both dst & src r empty) ",
                    "priority": 1.0
                },
                {
                    "source_field": "sourceAddressIPv6",
                    "dest_field": "src",
                    "priority": 6.0
                },
                {
                    "source_field": "sourceTranslatedAddress",
                    "dest_field": "src",
                    "priority": 5.0
                },
                {
                    "source_field": "sourceAddress",
                    "dest_field": "src",
                    "priority": 4.0
                },
                {
                    "source_field": "sourceAddress",
                    "dest_field": "src",
                    "priority": 3.0
                },
                {
                    "source_field": "sourceNtHost",
                    "dest_field": "src",
                    "priority": 2.0
                },
                {
                    "source_field": "sourceHostName",
                    "dest_field": "src",
                    "priority": 1.0
                },
                {
                    "source_field": "sourceHost",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceInterface",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceInterfaceId",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceMacAddress",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceNtDomain",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourcePriority",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceUserName",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceWorkstation",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourceZone",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "suser",
                    "dest_field": "src_node_ftr",
                    "priority": None
                },
                {
                    "source_field": "sourcePublicPort",
                    "dest_field": "src_port",
                    "priority": 3.0
                },
                {
                    "source_field": "sourceTranslatedPort",
                    "dest_field": "src_port",
                    "priority": 2.0
                },
                {
                    "source_field": "sourcePort",
                    "dest_field": "src_port",
                    "priority": 1.0
                },
                {
                    "source_field": "endTime",
                    "dest_field": "time",
                    "priority": 5.0
                },
                {
                    "source_field": "endTime",
                    "dest_field": "time",
                    "priority": 4.0
                },
                {
                    "source_field": "startTime",
                    "dest_field": "time",
                    "priority": 3.0
                },
                {
                    "source_field": "timestamp",
                    "dest_field": "time",
                    "priority": 2.0
                },
                {
                    "source_field": "time",
                    "dest_field": "time",
                    "priority": 1.0
                }
            ]

            # cef_to_internal_mappings_df = pd.DataFrame(cef_to_internal_mappings)

            return cef_to_internal_mappings


        def convert_to_unix_timestamp(time_str):
            """
            Convert any date/time format to a UNIX timestamp.
            Args:
                time_str: The date/time string or UNIX timestamp to convert.
            Returns
                return: The UNIX timestamp.
            """
            try:
                # Check if the input is already a UNIX timestamp
                if isinstance(time_str, (int, float)):
                    return time_str

                # Attempt to convert string to a float or int
                try:
                    timestamp = float(time_str)
                    return timestamp
                except ValueError:
                    pass

                # Parse the date/time string into a datetime object
                dt = parser.parse(time_str)
                # Convert the datetime object to a UNIX timestamp
                unix_timestamp = int(dt.timestamp())
                return unix_timestamp
            except Exception as e:
                print(f"Error: {e}")
                return None


        def get_static_cluster_config():
            '''
            Get static cluster config for clustering model
            '''
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST

            CLUSTER_CONFIG_EXISTS_IN_REQUEST = True
            cluster_config = {
                "set_space": 256,
                "cc_8": 64,
                "cc_1": 45,
                "cc_2": 0.05,
                "cc_3": 0.4,
                "cc_5": 7,
                "cc_4": 25,
                "cc_6": 7,
                "cc_7": False,
                "contrastive_loss": "js",
                "triplet_margin": 0.1,
                "max_time": None,
                "time_feat_dim": 8,
                "channel_embedding_dim": 128,
                "direct_edge_weight": 1000,
                "technique": 1000000,
                "tactic": 100,
                "stage": 1,
                "count": 10,
                "priority": 10000,
                "port": 1,
                "url": 10,
                "user_agent": 1000000,
                "cert": 10000,
                "time": 1,
                "user_feedback": 100,
                "learning_rate": 3e-05,
                "num_epochs": 30,
                "batch_size": 500,
                "embedding_type": "avg",
                "dropout": 0.05,
                "patch_size": 1,
                "max_input_sequence_length": 64,
                "num_neighbors": 30,
                "seed": 1,
                "aggalerts_flag": False,
                "direct_emb": False,
            }

            json.dump(cluster_config, open(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, "w"))


        def delete_error_log_s3(bucket):
            '''
            Delete contents of output folder in root of S3 bucket
            '''
            global S3_CLIENT

            func = "delete_error_log_s3"

            error_log_key = "output/error_log.txt"

            # Delete the objects
            response = S3_CLIENT.delete_object(
                Bucket=bucket,
                Key=error_log_key
            )

            print(f"{func}: Deleted objects: {response}")


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          technique_lookup_object: scratch/lambda/data.csv
          chunk_size: !Ref ChunkSize
          encode_other_attrs: 'true'
          map_cef_to_internal: 'true'
          encode_node_feature: 'true'
          db_connection_string: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/postgres
  enrichWithTechniqueLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${enrichWithTechnique}
  enrichWithTechniqueLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref enrichWithTechnique
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  startTechTask:
    Type: AWS::Serverless::Function
    DependsOn:
    - ECSCluster
    - TechTaskDefinition
    Properties:
      FunctionName: !Sub ${AWS::StackName}_start_tech_task
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: startTechTask
      InlineCode: |
        '''
        Process response from technique classification model and enrich alerts with technique. Create input for the Temporal clustering model.

        Input: Response from Technique classification model
        '''

        import os
        import glob
        import boto3

        BUCKET = os.getenv("bucket")

        ECS_CLUSTER_ARN = os.getenv("ecs_cluster_arn")

        TECH_TASK_ARN = os.getenv("tech_task_arn")

        missing_variables = []

        if BUCKET is None:
            missing_variables.append("bucket")
        if ECS_CLUSTER_ARN is None:
            missing_variables.append("ecs_cluster_arn")
        if TECH_TASK_ARN is None:
            missing_variables.append("tech_task_arn")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global BUCKET
            global ECS_CLUSTER_ARN
            global TECH_TASK_ARN

            func = "lambda_handler"

            # bucket = event["Records"][0]["s3"]["bucket"]["name"]
            # input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            # bucket, input_filename = get_output_object_key(event)

            print(event)

            bucket = BUCKET
            input_filename = event["Key"]

            try:

                clear_temp_dir()

                print(f"{func}: Download file to process from S3 bucket: {bucket}/{input_filename}")

                first_unique_id = input_filename.split("/")[-2]

                unique_id = input_filename.split("/")[-1].split("_")[-1].split(".")[0]

                filename = input_filename.split("/")[-1]

                print(f"{func}: Processing chunk file and enriching with techniques. unique ID: {unique_id}")
                path_to_enriched_alerts_with_techniques = f"{SCRATCH_DIR}/response/classification_out/{first_unique_id}/"

                lambda_response = {
                    # for process tech
                    "bucket": bucket,
                    "prefix": path_to_enriched_alerts_with_techniques,

                    # for ecs task
                    # "Cluster": "arn:aws:ecs:us-west-2:582441423537:cluster/tech-ecs-1",
                    # "TaskDefinition": "arn:aws:ecs:us-west-2:582441423537:task-definition/tech-ecs-1-task:4",
                    "Cluster": ECS_CLUSTER_ARN,
                    "TaskDefinition": TECH_TASK_ARN,

                    "S3InputPath": f"s3://{bucket}/{input_filename}",
                    "S3OutputPath": f"s3://{bucket}/{path_to_enriched_alerts_with_techniques}{filename}.out"
                }

                print(f"{func}: Enriched with techniques: {unique_id}")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
          ecs_cluster_arn: !GetAtt ECSCluster.Arn
          tech_task_arn: !GetAtt TechTaskDefinition.TaskDefinitionArn
  startTechTaskLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${startTechTask}
  startTechTaskLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref startTechTask
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  processEnrichedWithTechnique:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_process_enriched_with_technique
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: processEnrichedWithTechnique
      InlineCode: |
        '''
        Process response from technique classification model and enrich alerts with technique. Create input for the Temporal clustering model.

        Input: Response from Technique classification model
        '''

        import os
        import urllib
        import json
        import glob
        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        BUCKET = os.getenv("bucket")

        missing_variables = []

        if BUCKET is None:
            missing_variables.append("bucket")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        TEMP_INPUT_DIR = "/tmp/input/"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)

        SCRATCH_DIR = "scratch"

        TEMP_INPUT_FILENAME = "/tmp/classification_response.json"
        TEMP_ALERTS_INPUT_FILENAME = f"{TEMP_INPUT_DIR}input.json"
        TEMP_QUEUE_FILENAME = f"{TEMP_INPUT_DIR}queue.json"
        TEMP_CLUSTER_CONFIG_INPUT_FILENAME = f"{TEMP_INPUT_DIR}cluster_config.json"
        TEMP_NODE_FEATURE_INPUT_FILENAME = f"{TEMP_INPUT_DIR}node_feature.json"
        TEMP_USER_FEEDBACK_WEIGHTS = f"{TEMP_INPUT_DIR}user_feedback_weights.json"

        CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
        NODE_FEATURE_EXISTS_IN_REQUEST = False


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_INPUT_FILENAME
            global SCRATCH_DIR
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global NODE_FEATURE_EXISTS_IN_REQUEST

            func = "lambda_handler"

            CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
            NODE_FEATURE_EXISTS_IN_REQUEST = False

            # bucket = event["Records"][0]["s3"]["bucket"]["name"]
            # input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            # bucket, input_filename = get_output_object_key(event)

            print(event)

            bucket = BUCKET
            input_filename = event["Key"]

            try:

                clear_temp_dir()

                print(f"{func}: Download file to process from S3 bucket: {bucket}/{input_filename}")

                try:
                    S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                    print(f"{func}: File to process download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file to process")
                    raise e

                first_unique_id = input_filename.split("/")[-2]

                unique_id = input_filename.split("/")[-1].split("_")[-1].split(".")[0]

                print(f"{func}: Download intermediate files from S3")
                try:
                    download_intermediate_input_files(bucket, unique_id)

                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                print(f"{func}: Processing chunk file and enriching with techniques. unique ID: {unique_id}")
                path_to_enriched_alerts_with_techniques = f"{SCRATCH_DIR}/output/classification/{first_unique_id}/input_{unique_id}.json"
                lambda_response = process_classification_output(bucket, unique_id, path_to_enriched_alerts_with_techniques)

                print(f"{func}: Enriched with techniques: {unique_id}")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def process_classification_output(bucket, unique_id, path_to_enriched_alerts_with_techniques):
            '''
            Enrich alerts with technique. Start next technique classification batch transform job from queue.
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
                path_to_enriched_alerts_with_techniques: S3 path to save enriched alerts
            Returns:
                Lambda response with next job to run
            '''
            global TEMP_INPUT_FILENAME
            global TEMP_ALERTS_INPUT_FILENAME
            global TEMP_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global TEMP_USER_FEEDBACK_WEIGHTS
            global S3_CLIENT

            func = "process_classification_output"

            print(f"{func}: Load classification response file")
            classification_response_lines = open(TEMP_INPUT_FILENAME, "r").readlines()

            print(f"{func}: Load chunk input file")
            chunk_input_json = json.load(open(TEMP_ALERTS_INPUT_FILENAME, "r"))

            print(f"{func}: Gather alerts and classified techniques")

            classification_response_list = []
            for response_lines in classification_response_lines:
                response = json.loads(response_lines)
                if "labels" in response and "techniques" in response["labels"]:
                    classification_response_dict = {"alerts": response["raw_alert"], "techniques": json.dumps(response["labels"]["techniques"])}
                else:
                    classification_response_dict = {"alerts": response["raw_alert"], "techniques": json.dumps([])}

                classification_response_list.append(classification_response_dict)

            classification_response_df = pd.DataFrame(classification_response_list, columns=["alerts", "techniques"])

            # Enrich input with recognized techniques from response from technique detection model
            if len(classification_response_df) > 0:
                print(f"{func}: Enrich alerts with techniques")

                for chunk_input_alert in chunk_input_json:

                    if len(chunk_input_alert["tech"]) > 0:
                        continue

                    alert_text = chunk_input_alert["name"]
                    alert_text_in_classification_response_df = classification_response_df["alerts"].eq(alert_text).any()

                    if alert_text_in_classification_response_df:
                        tech = classification_response_df[classification_response_df["alerts"] == alert_text].iloc[0]["techniques"]
                        tech = json.loads(tech)
                        chunk_input_alert["tech"] = tech

            update_lookup_table(bucket, unique_id, classification_response_df)

            chunk_filter_alerts = []
            for chunk_input_alert in chunk_input_json:
                if len(chunk_input_alert["tech"]) <= 0:
                    continue
                chunk_filter_alerts.append(chunk_input_alert)

            chunk_input_json = chunk_filter_alerts

            print(f"{func}: Check if current batch output should be skipped. Length of filtered alerts: {len(chunk_filter_alerts)}")
            if len(chunk_filter_alerts) > 0:
                temp_enriched_alerts_with_techniques_output_filename = "/tmp/input.json"

                user_feedback_weights_json = json.load(open(TEMP_USER_FEEDBACK_WEIGHTS, "r"))
                edge_weights = user_feedback_weights_json["event"]

                # Create request for temporal clustering model
                with open(temp_enriched_alerts_with_techniques_output_filename, "w") as of:
                    req = {"request_id": "1", "input": chunk_input_json, "user_feedback_weights": edge_weights}

                    if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                        req["cluster_config"] = json.load(open(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, "r"))

                    if NODE_FEATURE_EXISTS_IN_REQUEST:
                        req["node_feature"] = json.load(open(TEMP_NODE_FEATURE_INPUT_FILENAME, "r"))
                    json.dump(req, of)

                try:
                    print(f"{func}: Save enriched alerts with techniques as input to cluster detection: {bucket}/{path_to_enriched_alerts_with_techniques}")
                    S3_CLIENT.upload_file(temp_enriched_alerts_with_techniques_output_filename, bucket, path_to_enriched_alerts_with_techniques)
                except Exception as e:
                    print(f"Failed to save enriched alerts with technique to S3: {bucket}/{path_to_enriched_alerts_with_techniques}")
                    raise e

            current_queue_json = json.load(open(TEMP_QUEUE_FILENAME, "r"))

            lambda_response = {
                # for step function
                "TransformJobName": "transform-job-embedding",
                "bucket": bucket,
                "prefix": f"{SCRATCH_DIR}/output/classification/{current_queue_json['first']}/",
            }

            return lambda_response


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download chunk input file, queue, config if exists, and node features if exists
            Args:
                bucket: Bucket name
                path_to_intermediate_input_before_technique_classification: S3 path to chunk input file before technique enrichment
                path_to_intermediate_queue: S3 path to queue for chunk
                path_to_config_from_input: S3 path to config
                path_to_node_feature_from_input: S3 path to node_feature
            '''
            global TEMP_ALERTS_INPUT_FILENAME
            global TEMP_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_USER_FEEDBACK_WEIGHTS

            func = "download_intermediate_input_files"

            path_to_intermediate_input_before_technique_classification = f"{SCRATCH_DIR}/intermediate/{unique_id}/input.json"
            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            path_to_cluster_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_config.json"
            path_to_node_feature_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/node_feature.json"
            path_to_global_attribute_weights = f"{SCRATCH_DIR}/attribute_weights.json"

            S3_CLIENT.download_file(bucket, path_to_intermediate_input_before_technique_classification, TEMP_ALERTS_INPUT_FILENAME)
            S3_CLIENT.download_file(bucket, path_to_intermediate_queue, TEMP_QUEUE_FILENAME)
            S3_CLIENT.download_file(bucket, path_to_global_attribute_weights, TEMP_USER_FEEDBACK_WEIGHTS)

            try:
                print(f"{func}: Checking if cluster config file exists on S3 path {bucket}/{path_to_cluster_config_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_cluster_config_from_input)
                CLUSTER_CONFIG_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Cluster config file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_cluster_config_from_input, TEMP_CLUSTER_CONFIG_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_cluster_config_from_input}")
                    raise e

            try:
                print(f"{func}: Checking if node feaures file exists on S3 path {bucket}/{path_to_node_feature_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_node_feature_from_input)
                NODE_FEATURE_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Node feaures file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_node_feature_from_input, TEMP_NODE_FEATURE_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_node_feature_from_input}")
                    raise e


        def update_lookup_table(bucket, unique_id, classification_response_df):
            '''
            Create input to update technique classification lookup table. Trigger lookup table update.
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of input
                classification_response_df: dataframe to add to lookup table
            '''
            global SCRATCH_DIR
            global S3_CLIENT

            func = "update_lookup_table"

            classification_response_s3_key = f"{SCRATCH_DIR}/queue/{unique_id}/classification_response.csv"

            temp_message_payload_dir = "/tmp/message_payload/"
            os.makedirs(temp_message_payload_dir, exist_ok=True)
            temp_message_payload_filename = f"{temp_message_payload_dir}classification_response.csv"

            if len(classification_response_df) < 1:
                return
            classification_response_df.to_csv(temp_message_payload_filename, index=False)

            try:
                S3_CLIENT.upload_file(temp_message_payload_filename, bucket, classification_response_s3_key)
            except Exception as e:
                print(f"{func}: Failed to upload classification response message payload file to: {bucket}/{classification_response_s3_key}")
                raise e

            message_body = {
                "s3_bucket": bucket,
                "s3_key": classification_response_s3_key
            }

            print(json.dumps({"event": "update lookup from message", "message": message_body}))


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
  processEnrichedWithTechniqueLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${processEnrichedWithTechnique}
  processEnrichedWithTechniqueLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref processEnrichedWithTechnique
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  startEmbeddingTask:
    Type: AWS::Serverless::Function
    DependsOn:
    - ECSCluster
    - ClusterPart1TaskDefinition
    Properties:
      FunctionName: !Sub ${AWS::StackName}_start_embedding_task
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: startEmbeddingTask
      InlineCode: |
        '''
        Process response from technique classification model and enrich alerts with technique. Create input for the Temporal clustering model.

        Input: Response from Technique classification model
        '''

        import os
        import glob
        import boto3

        BUCKET = os.getenv("bucket")

        ECS_CLUSTER_ARN = os.getenv("ecs_cluster_arn")

        EMBEDDING_TASK_ARN = os.getenv("embedding_task_arn")

        missing_variables = []

        if BUCKET is None:
            missing_variables.append("bucket")
        if ECS_CLUSTER_ARN is None:
            missing_variables.append("ecs_cluster_arn")
        if EMBEDDING_TASK_ARN is None:
            missing_variables.append("embedding_task_arn")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global BUCKET
            global ECS_CLUSTER_ARN
            global EMBEDDING_TASK_ARN

            func = "lambda_handler"

            # bucket = event["Records"][0]["s3"]["bucket"]["name"]
            # input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            # bucket, input_filename = get_output_object_key(event)

            print(event)

            bucket = BUCKET
            input_filename = event["Key"]

            try:

                clear_temp_dir()

                print(f"{func}: Download file to process from S3 bucket: {bucket}/{input_filename}")

                first_unique_id = input_filename.split("/")[-2]

                unique_id = input_filename.split("/")[-1].split("_")[-1].split(".")[0]

                filename = input_filename.split("/")[-1]

                print(f"{func}: Processing chunk file and enriching with techniques. unique ID: {unique_id}")
                path_to_embedding_response = f"{SCRATCH_DIR}/response/embedding_out/{first_unique_id}/"

                lambda_response = {
                    # for process tech
                    "bucket": bucket,
                    "prefix": path_to_embedding_response,

                    # for ecs task
                    # "Cluster": "arn:aws:ecs:us-west-2:582441423537:cluster/tech-ecs-1",
                    # "TaskDefinition": "arn:aws:ecs:us-west-2:582441423537:task-definition/tech-ecs-1-task:6",
                    "Cluster": ECS_CLUSTER_ARN,
                    "TaskDefinition": EMBEDDING_TASK_ARN,

                    "S3InputPath": f"s3://{bucket}/{input_filename}",
                    "S3OutputPath": f"s3://{bucket}/{path_to_embedding_response}{filename}.out"
                }

                print(f"{func}: Enriched with techniques: {unique_id}")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
          ecs_cluster_arn: !GetAtt ECSCluster.Arn
          embedding_task_arn: !GetAtt ClusterPart1TaskDefinition.TaskDefinitionArn
  startEmbeddingTaskLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${startEmbeddingTask}
  startEmbeddingTaskLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref startEmbeddingTask
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  createEmbedding:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_create_embedding
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: createEmbedding
      InlineCode: |
        '''
        Unused
        Create batch transform job for Temporal Clustering model

        Input: input alerts enriched with techniques
        '''

        import json
        import os
        import urllib
        import zipfile
        import glob
        from datetime import datetime, timezone

        import boto3
        from botocore.exceptions import ClientError

        missing_variables = []

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        TEMP_INPUT_DIR = "/tmp/input"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)

        QUEUE_LOOKUP_FILE = f"{SCRATCH_DIR}/queue_lookup.json"
        TEMP_QUEUE_LOOKUP_FILE = "/tmp/queue_lookup.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global QUEUE_LOOKUP_FILE
            global TEMP_QUEUE_LOOKUP_FILE
            func = "lambda_handler"

            bucket = event["bucket"]
            try:

                clear_temp_dir()

                unique_id = event['prefix'].split("/")[-2]

                S3_CLIENT.download_file(bucket, QUEUE_LOOKUP_FILE, TEMP_QUEUE_LOOKUP_FILE)

                lambda_response = prepare_queue(bucket, unique_id)

                print(f"{func}: Prepare queue for clustering")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def prepare_queue(bucket, first_unique_id):
            '''
            Prepare queue for clustering
            Args:
                bucket: Bucket name
                first_unique_id: the first unique id of the input
            '''
            global S3_CLIENT
            global TEMP_INPUT_DIR

            func = "prepare_queue"

            original_input_batch_queue = []
            filtered_input_batch_queue = []

            unique_id = first_unique_id
            while True:
                print(f"{func}: Get intermediate queue for unique id: {unique_id}")

                path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
                temp_intermediate_queue = f"{TEMP_INPUT_DIR}/{unique_id}/queue.json"
                os.makedirs(os.path.dirname(temp_intermediate_queue), exist_ok=True)

                original_input_batch_queue.append(unique_id)

                S3_CLIENT.download_file(bucket, path_to_intermediate_queue, temp_intermediate_queue)

                path_to_enriched_alerts_with_techniques = f"{SCRATCH_DIR}/output/classification/{first_unique_id}/input_{unique_id}.json"
                temp_enriched_alerts_with_technique = f"{TEMP_INPUT_DIR}/{unique_id}/input_{unique_id}.json"
                print(f"{func}: Check if the current batch is to be processed")
                try:
                    S3_CLIENT.head_object(Bucket=bucket, Key=path_to_enriched_alerts_with_techniques)
                except ClientError as e:
                    if e.response['Error']['Code'] == '404':
                        print(f"{func}: File does not exist")
                    else:
                        raise e
                else:
                    S3_CLIENT.download_file(bucket, path_to_enriched_alerts_with_techniques, temp_enriched_alerts_with_technique)
                    # TODO: filter alerts
                    input_json = json.load(open(temp_enriched_alerts_with_technique, "r"))
                    chunk_alert = input_json["input"]
                    new_chunk_alert = []
                    for alert in chunk_alert:
                        if len(alert["tech"]) <= 0:
                            continue
                        new_chunk_alert.append(alert)
                    if len(new_chunk_alert) > 0:
                        input_json["input"] = new_chunk_alert
                        json.dump(input_json, open(temp_enriched_alerts_with_technique, "w"))
                        S3_CLIENT.upload_file(temp_enriched_alerts_with_technique, bucket, path_to_enriched_alerts_with_techniques)
                        filtered_input_batch_queue.append(unique_id)
                    else:
                        # if the object is not deleted, anyways the queue will not be including this unique id, so it will be skipped.
                        try:
                            S3_CLIENT.delete_object(Bucket=bucket, Key=path_to_enriched_alerts_with_techniques)
                        except:
                            print(f"{func}: Failed to delete object from S3. Skip deleting object")

                try:
                    os.remove(temp_enriched_alerts_with_technique)
                except Exception as e:
                    print(f"{func}: Failed to delete file: {e}")

                current_batch_queue_json = json.load(open(temp_intermediate_queue, "r"))

                if current_batch_queue_json["next"] is None:
                    print(f"{func}: Current batch is the last batch of current input")
                    break
                else:
                    print(f"{func}: Next batch unique id: {current_batch_queue_json['next']}")
                    unique_id = current_batch_queue_json["next"]

            # if there was only 1 batch and it was to be skipped, then the length of filtered list is 0. skip entire input
            if len(filtered_input_batch_queue) <= 0:
                print(f"{func}: All batches of current input is to be skipped")

                lambda_response = skip_input()

                return lambda_response

            if len(original_input_batch_queue) != len(filtered_input_batch_queue):

                # if there were more than 1 batch and the first batch was to be skipped, then update queue.
                if first_unique_id != filtered_input_batch_queue[0]:
                    print(f"{func}: First batch of current input is to be skipped")
                    temp_intermediate_queue = f"{TEMP_INPUT_DIR}/{filtered_input_batch_queue[0]}/queue.json"
                    queue_json = json.load(open(temp_intermediate_queue, "r"))

                    temp_first_intermediate_queue = f"{TEMP_INPUT_DIR}/{first_unique_id}/queue.json"
                    first_queue_json = json.load(open(temp_first_intermediate_queue, "r"))

                    next_unique_id = None
                    if len(filtered_input_batch_queue) > 1:
                        next_unique_id = filtered_input_batch_queue[1]

                    queue_json["next"] = next_unique_id

                    queue_json["previous"] = first_queue_json["previous"]
                    queue_json["first"] = filtered_input_batch_queue[0]

                    json.dump(queue_json, open(temp_intermediate_queue, "w"))

                    path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{filtered_input_batch_queue[0]}/queue.json"
                    S3_CLIENT.upload_file(temp_intermediate_queue, bucket, path_to_intermediate_queue)

                else:
                    print(f"{func}: First batch of current input is not to be skipped")
                    temp_intermediate_queue = f"{TEMP_INPUT_DIR}/{filtered_input_batch_queue[0]}/queue.json"
                    queue_json = json.load(open(temp_intermediate_queue, "r"))

                    next_unique_id = None
                    if len(filtered_input_batch_queue) > 1:
                        next_unique_id = filtered_input_batch_queue[1]

                    queue_json["next"] = next_unique_id

                    json.dump(queue_json, open(temp_intermediate_queue, "w"))

                    path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{filtered_input_batch_queue[0]}/queue.json"
                    S3_CLIENT.upload_file(temp_intermediate_queue, bucket, path_to_intermediate_queue)

                print(f"{func}: update all the queue json that was not to be skipped based on new queue")
                for i, filtered_id in enumerate(filtered_input_batch_queue):
                    if i == 0:
                        continue
                    temp_intermediate_queue = f"{TEMP_INPUT_DIR}/{filtered_id}/queue.json"

                    queue_json = json.load(open(temp_intermediate_queue, "r"))

                    next_unique_id = None
                    if (i+1) < len(filtered_input_batch_queue):
                        next_unique_id = filtered_input_batch_queue[i+1]

                    queue_json["next"] = next_unique_id
                    queue_json["previous"] = filtered_input_batch_queue[i-1]
                    queue_json["first"] = filtered_input_batch_queue[0]

                    json.dump(queue_json, open(temp_intermediate_queue, "w"))
                    path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{filtered_id}/queue.json"
                    S3_CLIENT.upload_file(temp_intermediate_queue, bucket, path_to_intermediate_queue)

            lambda_response = {
                # for step function
                "TransformJobName": "transform-job-embedding",
                "bucket": bucket,
                "prefix": f"{SCRATCH_DIR}/output/classification/{first_unique_id}/",
            }
            return lambda_response


        def skip_input():
            '''
            skip current input
            Returns: lambda response for skipped input for step function
            '''
            global TEMP_QUEUE_LOOKUP_FILE

            func = "skip_input"

            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            # need to send update queue log to remove current input from input queue.
            print(f"{func}: Input file does not have any tranformed events that could be processed. Skip input file.")
            payload_json = {"prev": queue_lookup_json["prev"]}
            log_json = {"message": "update queue lookup", "payload": payload_json}
            print(f"{json.dumps(log_json)}")

            if len(queue_lookup_json["input_queue"]) > 1:
                print(f"{func}: Next input file to process: {queue_lookup_json['input_queue'][1]}")
                try:
                    print(f"{func}: Upload next input file in S3")
                    next_input_queue_entry = queue_lookup_json["input_queue"][1]
                    next_input_bucket = next_input_queue_entry.split("/")[0]
                    next_input_key = "/".join(next_input_queue_entry.split("/")[1:])
                    copy_source = {
                        'Bucket': next_input_bucket,
                        'Key': next_input_key
                    }
                    S3_CLIENT.copy_object(CopySource=copy_source, Bucket=next_input_bucket, Key=next_input_key,
                                            Metadata={'UpdatedAt': str(datetime.now())}, MetadataDirective='REPLACE')
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {next_input_bucket}/{next_input_key}")
                    raise e

            lambda_response = {"TransformJobName": "skipped_input"}
            return lambda_response


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
  createEmbeddingLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${createEmbedding}
  createEmbeddingLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref createEmbedding
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  processEmbedding:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_process_embedding
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: processEmbedding
      InlineCode: |
        '''
        Process response from embedding model and create input for temporal clustering model.

        Input: Response from temporal clustering model part 1
        '''

        import os
        import urllib
        import json
        import glob
        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        BUCKET = os.getenv("bucket")

        missing_variables = []

        if BUCKET is None:
            missing_variables.append("bucket")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        TEMP_QUEUE_FILENAME = "/tmp/queue.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global BUCKET
            global SCRATCH_DIR

            func = "lambda_handler"

            # bucket = event["Records"][0]["s3"]["bucket"]["name"]
            # input_filename = urllib.parse.unquote_plus(event["Records"][0]["s3"]["object"]["key"], encoding="utf-8")

            # bucket, input_filename = get_output_object_key(event)

            print(event)

            bucket = BUCKET
            input_filename = event["Key"]

            try:

                clear_temp_dir()

                unique_id = input_filename.split("/")[-1].split("_")[-1].split(".")[0]

                try:
                    path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
                    print(f"{func}: Download file from S3: {bucket}/{path_to_intermediate_queue}")
                    S3_CLIENT.download_file(bucket, path_to_intermediate_queue, TEMP_QUEUE_FILENAME)
                except Exception as e:
                    print(f"{func}: Failed to download file from S3")
                    raise e

                print(f"{func}: Processing chunk file and creating output for embedding model. unique ID: {unique_id}")
                path_to_embedding_output = f"{SCRATCH_DIR}/output/embedding/{unique_id}/input.gz"
                process_embedding_output(bucket, path_to_embedding_output, input_filename)

                print(f"{func}: Processed chunk: {unique_id}")
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            queue_json = json.load(open(TEMP_QUEUE_FILENAME, "r"))

            lambda_response = {
                "TransformJobName": "transform-job-cluster",
                "bucket": bucket,
                "input_first_unique_id": queue_json["first"]
            }

            return lambda_response


        def process_embedding_output(bucket, path_to_embedding_output, input_filename):
            '''
            Copy response from embedding output to corresponding output path
            Args:
                bucket: Bucket name
                path_to_embedding_output: S3 path to save output for embedding model
                input_filename: S3 path to response from embedding model
            '''
            global S3_CLIENT

            func = "process_classification_output"

            copy_source = {
                'Bucket': bucket,
                'Key': input_filename
            }

            try:
                print(f"{func}: Upload embedding model output in S3")
                S3_CLIENT.copy(copy_source, bucket, path_to_embedding_output)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_embedding_output}")
                raise e


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
  processEmbeddingLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${processEmbedding}
  processEmbeddingLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref processEmbedding
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  createCluster:
    Type: AWS::Serverless::Function
    DependsOn:
    - ECSCluster
    - ClusterPart2TaskDefinition
    Properties:
      FunctionName: !Sub ${AWS::StackName}_create_cluster
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: createCluster
      InlineCode: |
        '''
        Create batch transform job for Temporal Clustering model

        Input: input alerts enriched with techniques
        '''

        import json
        import os
        import urllib
        import zipfile
        import glob

        import boto3
        from botocore.exceptions import ClientError

        ECS_CLUSTER_ARN = os.getenv("ecs_cluster_arn")

        CLUSTER_TASK_ARN = os.getenv("cluster_task_arn")

        missing_variables = []
        if ECS_CLUSTER_ARN is None:
            missing_variables.append("ecs_cluster_arn")
        if CLUSTER_TASK_ARN is None:
            missing_variables.append("cluster_task_arn")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        CLUSTER_CONFIG_EXISTS_IN_REQUEST = False

        TEMP_INPUT_FILENAME = "/tmp/input.gz"
        TEMP_INTERMEDIATE_QUEUE_FILENAME = "/tmp/queue.json"

        TEMP_CLUSTER_CONFIG_INPUT_FILENAME = "/tmp/cluster_config.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global TEMP_INPUT_FILENAME
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global ECS_CLUSTER_ARN
            global CLUSTER_TASK_ARN
            func = "lambda_handler"

            CLUSTER_CONFIG_EXISTS_IN_REQUEST = False

            bucket = event["bucket"]
            try:
                input_filename = f"{SCRATCH_DIR}/output/embedding/{event['input_first_unique_id']}/input.gz"

                clear_temp_dir()

                print(f"{func}: Download input file from S3 bucket: {bucket}/{input_filename}")

                try:
                    S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                    print(f"{func}: Input file download completed")
                except Exception as e:
                    print(f"{func}: Failed to download input file")
                    raise e

                unique_id = input_filename.split("/")[-2]

                print(f"{func}: Download intermediate files from S3")
                try:
                    download_intermediate_input_files(bucket, unique_id)

                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                if check_if_skip_creating_tranform_job(bucket, unique_id):
                    return

                print(f"{func}: Start batch transform job for aggregate and cluster and alerts")

                aggregate_and_cluster_alerts(bucket, unique_id)
            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            filename = "input.zip"
            path_to_embedding_output = f"{SCRATCH_DIR}/output/embedding/{unique_id}/{filename}"
            path_to_cluster_response = f"{SCRATCH_DIR}/response/cluster_out/{unique_id}/"
            lambda_response = {
                "TransformJobName": f'transform-job-cluster-{unique_id}',

                # for ecs task
                # "Cluster": "arn:aws:ecs:us-west-2:582441423537:cluster/tech-ecs-1",
                # "TaskDefinition": "arn:aws:ecs:us-west-2:582441423537:task-definition/tech-ecs-1-task:9",
                "Cluster": ECS_CLUSTER_ARN,
                "TaskDefinition": CLUSTER_TASK_ARN,

                "S3InputPath": f"s3://{bucket}/{path_to_embedding_output}",
                "S3OutputPath": f"s3://{bucket}/{path_to_cluster_response}{filename}.out"
            }
            return lambda_response


        def check_if_skip_creating_tranform_job(bucket, unique_id):
            '''
            Check if batch transform job should be created for the unique id based on queue
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of the input
            Returns:
                boolean: True, if the transform job creation should be skipped. False, otherwise
            '''

            global TEMP_INPUT_FILENAME
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_INTERMEDIATE_QUEUE_FILENAME

            queue_json = json.load(open(TEMP_INTERMEDIATE_QUEUE_FILENAME, "r"))
            print(f"Contents of the queue for creating transform job: {queue_json}")

            if queue_json["previous"] is None:
                print(f"No previous unique id found. Create batch transform job for {unique_id}")
                return False

            previous_unique_id = queue_json["previous"]
            print(f"Previous unique id found: {previous_unique_id}. Check if the cluster detection is completed.")

            try:
                path_to_cluster_output_of_previous = f"{SCRATCH_DIR}/response/cluster_out/{previous_unique_id}/input.zip.out"
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_cluster_output_of_previous)
                print(f"Will create batch transform job for {unique_id}")
                return False
            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e
                print(f"Clustering did not complete for {previous_unique_id}. Skip create batch transform job for {unique_id}")
                return True


        def aggregate_and_cluster_alerts(bucket, unique_id):
            '''
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk of the input
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_INPUT_FILENAME
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST

            func = "aggregate_and_cluster_alerts"

            temp_previous_cluster_output_zip = "/tmp/previous_output.zip"
            temp_enriched_alerts_with_techniques_output_filename = "input.gz"
            temp_cluster_config_filename = "cluster_config.json"

            queue_json = json.load(open(TEMP_INTERMEDIATE_QUEUE_FILENAME, "r"))

            try:
                if queue_json["previous"] is not None:
                    previous_unique_id = queue_json["previous"]
                    path_to_cluster_output_of_previous = f"{SCRATCH_DIR}/response/cluster_out/{previous_unique_id}/input.zip.out"
                    S3_CLIENT.download_file(bucket, path_to_cluster_output_of_previous, temp_previous_cluster_output_zip)
            except Exception as e:
                print(f"{func}: Unable to download file from S3: {bucket}/{path_to_cluster_output_of_previous}")
                raise e

            try:
                with zipfile.ZipFile(temp_previous_cluster_output_zip, "a") as z:
                    z.write(TEMP_INPUT_FILENAME, arcname=temp_enriched_alerts_with_techniques_output_filename)
                    if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                        z.write(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, arcname=temp_cluster_config_filename)
            except Exception as e:
                print(f"{func}: Unable to write input file to zip while creating the zip file")
                raise e

            path_to_embedding_output = f"{SCRATCH_DIR}/output/embedding/{unique_id}/input.zip"

            try:
                print(f"{func}: Save zip input to flow detection: {bucket}/{path_to_embedding_output}")
                S3_CLIENT.upload_file(temp_previous_cluster_output_zip, bucket, path_to_embedding_output)
            except Exception as e:
                print(f"{func}: Failed to upload zip file to S3: {bucket}/{path_to_embedding_output}")
                raise e


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download chunk input file, queue, config if exists, and node features if exists
            Args:
                bucket: Bucket name
                path_to_intermediate_input_before_technique_classification: S3 path to chunk input file before technique enrichment
                path_to_intermediate_queue: S3 path to queue for chunk
                path_to_config_from_input: S3 path to config
                path_to_node_feature_from_input: S3 path to node_feature
            '''
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME

            func = "download_intermediate_input_files"

            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            path_to_cluster_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_config.json"

            try:
                S3_CLIENT.download_file(bucket, path_to_intermediate_queue, TEMP_INTERMEDIATE_QUEUE_FILENAME)
                print(f"{func}: Queue file download completed")
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{path_to_intermediate_queue}")
                raise e

            try:
                print(f"{func}: Checking if cluster config file exists on S3 path {bucket}/{path_to_cluster_config_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_cluster_config_from_input)
                CLUSTER_CONFIG_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Cluster config file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_cluster_config_from_input, TEMP_CLUSTER_CONFIG_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_cluster_config_from_input}")
                    raise e


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          ecs_cluster_arn: !GetAtt ECSCluster.Arn
          cluster_task_arn: !GetAtt ClusterPart2TaskDefinition.TaskDefinitionArn
  createClusterLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${createCluster}
  createClusterLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref createCluster
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  processCluster:
    Type: AWS::Serverless::Function
    DependsOn:
    - DbLoadBalancer
    - ECSCluster
    - ClusterPart2TaskDefinition
    - FlowTaskDefinition
    Properties:
      FunctionName: !Sub ${AWS::StackName}_process_cluster
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: processCluster
      InlineCode: |
        '''
        Create input for next batch if in queue. Extract internal aggregated alerts, cluster json. Create input for flow model.
        Input: response from the clustering model
        '''

        import json
        import os
        import urllib
        import zipfile
        import glob
        import sqlite3
        from contextlib import closing

        import pandas as pd
        import boto3
        from botocore.exceptions import ClientError

        import sys
        import subprocess

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        subprocess.call('pip install psycopg2-binary==2.9.9 sqlalchemy==2.0.32 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import psycopg2
        from psycopg2 import sql
        from psycopg2.extras import DictCursor

        pd.options.mode.chained_assignment = None

        FLOW_INPUT_WINDOW_SIZE = os.getenv("flow_input_window_size")

        DB_CONNECTION_STRING = os.getenv("db_connection_string")

        ECS_CLUSTER_ARN = os.getenv("ecs_cluster_arn")

        CLUSTER_TASK_ARN = os.getenv("cluster_task_arn")

        FLOW_TASK_ARN = os.getenv("flow_task_arn")

        MAP_CEF_TO_INTERNAL = os.getenv("map_cef_to_internal")

        missing_variables = []

        if FLOW_INPUT_WINDOW_SIZE is None:
            missing_variables.append("flow_input_window_size")
        if ECS_CLUSTER_ARN is None:
            missing_variables.append("tech_cluster_arn")
        if CLUSTER_TASK_ARN is None:
            missing_variables.append("cluster_task_arn")
        if FLOW_TASK_ARN is None:
            missing_variables.append("flow_task_arn")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        FLOW_INPUT_WINDOW_SIZE = int(FLOW_INPUT_WINDOW_SIZE)

        if MAP_CEF_TO_INTERNAL is not None:
            MAP_CEF_TO_INTERNAL = (MAP_CEF_TO_INTERNAL == "true")
        else:
            MAP_CEF_TO_INTERNAL = True

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        TEMP_INPUT_DIR = "/tmp/input"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)

        TEMP_INTERMEDIATE_QUEUE_FILENAME = "/tmp/queue.json"

        TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME = "/tmp/aggregate_model_output.zip"

        TEMP_AGGREGATED_ALERTS_JSON_FILENAME = "/tmp/aggregated_alerts_output.json"
        TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME = "/tmp/cluster_output.json"

        TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME = "/tmp/input_cluster_id_list.json"

        TEMP_INPUT_TO_SEQUENCE_ALERTS_JSON_FILENAME = "/tmp/input_sequence.json"

        TEMP_CHUNK_INPUT_FILE = "/tmp/original_input.json"
        TEMP_CEF_INPUT = "/tmp/cef_input.json"

        TEMP_CONFIG_INPUT_FILENAME = "/tmp/config.json"
        CONFIG_EXISTS_IN_REQUEST = False

        TEMP_CLUSTER_CONFIG_INPUT_FILENAME = "/tmp/cluster_config.json"
        CLUSTER_CONFIG_EXISTS_IN_REQUEST = False

        INTERNAL_ID_TO_USER_ID_OBJECT_KEY = f"{SCRATCH_DIR}/internal_id_to_user_id.json"
        TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE = "/tmp/internal_id_to_user_id.json"

        TICKET_ID_OBJECT_KEY = f"{SCRATCH_DIR}/ticket_id.json"
        TEMP_TICKET_ID_LOOKUP_FILE = "/tmp/ticket_id.json"

        USER_OVERRIDE_CLUSTER = False


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME
            global CONFIG_EXISTS_IN_REQUEST
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global MAP_CEF_TO_INTERNAL
            global USER_OVERRIDE_CLUSTER

            func = "lambda_handler"

            CONFIG_EXISTS_IN_REQUEST = False
            CLUSTER_CONFIG_EXISTS_IN_REQUEST = False
            USER_OVERRIDE_CLUSTER = False

            bucket, aggregate_and_cluster_alerts_output = get_output_object_key(event)

            # bucket = event['Records'][0]['s3']['bucket']['name']
            # aggregate_and_cluster_alerts_output = event['Records'][0]['s3']['object']['key']

            try:

                clear_temp_dir()

                unique_id = aggregate_and_cluster_alerts_output.split("/")[-2]
                print(f"{func}: Unique ID: {unique_id}")

                if "TransformJobName" in event and event["TransformJobName"] == "user_override_cluster":
                    print(f"{func}: User override cluster")
                    USER_OVERRIDE_CLUSTER = True
                    MAP_CEF_TO_INTERNAL = False

                try:
                    print(f"{func}: input file: {bucket}/{aggregate_and_cluster_alerts_output}")
                    S3_CLIENT.download_file(bucket, aggregate_and_cluster_alerts_output, TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME)
                    print(f"{func}: Downloaded input file")
                except Exception as e:
                    print(f"{func}: Failed to download input file")
                    raise e

                input_to_sequence_alerts = f"{SCRATCH_DIR}/output/cluster/{unique_id}/input_flow.json"

                print(f"{func}: Download intermediate files from S3 bucket")
                try:
                    download_intermediate_input_files(bucket, unique_id)

                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                fetch_or_create_ticket_id_lookup_file(bucket)

                print(f"{func}: Starting processing cluster output")
                lambda_response = process_output_of_aggregate_and_cluster_alerts(bucket, unique_id, input_to_sequence_alerts)
                print(f"{func}: Completed processing")

            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            return lambda_response


        def process_output_of_aggregate_and_cluster_alerts(bucket, unique_id, input_to_sequence_alerts):
            '''
            Process output of temporal clustering model, extract aggregated alerts, and clusters internally.
            create cluster ticket output for batch, global cluster output
            Create next batch transform job if in queue. else save input file for flow model
            Args:
                bucket: Bucket name
                unique_id: Unique id of current chunk
                input_to_sequence_alerts: S3 path to save input for flow model
            '''

            global S3_CLIENT
            global TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME
            global TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_INPUT_TO_SEQUENCE_ALERTS_JSON_FILENAME
            global TEMP_CONFIG_INPUT_FILENAME
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global CONFIG_EXISTS_IN_REQUEST
            global ECS_CLUSTER_ARN
            global CLUSTER_TASK_ARN
            global FLOW_TASK_ARN

            func = "process_output_of_aggregate_and_cluster_alerts"

            print(f"{func}: Extract cluster and alert output from zip")

            with zipfile.ZipFile(TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME, "r") as z:
                with z.open("cluster_output.json", "r") as f:
                    clusters_of_aggregated_alerts = json.load(f)

                with z.open("alert_output.json", "r") as f:
                    aggregated_alerts = json.load(f)

            with open(TEMP_INTERMEDIATE_QUEUE_FILENAME, "r") as f:
                queue_json = json.load(f)

            print(f"{func}: Sort output")

            aggregated_alerts.sort(key=sort_by_alert_id)
            clusters_of_aggregated_alerts.sort(key=sort_by_cluster_id)

            print(f"{func}: Save raw output")

            json.dump(clusters_of_aggregated_alerts, open(TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME, "w"))
            json.dump(aggregated_alerts, open(TEMP_AGGREGATED_ALERTS_JSON_FILENAME, "w"))

            print(f"{func}: Update cluster id in input, get current batch cluster id list")

            # batch cluster id list is the list of all the cluster which is active for current batch
            # a cluster may have all alerts from previous batch
            batch_cluster_id_list = get_batch_cluster_ids_update_input_cluster_ids(bucket, unique_id)

            # save output from cluster model to S3
            print(f"{func}: save output from cluster model to S3")
            save_aggreate_cluster_json_output(bucket, unique_id)

            # update event sqlite table with current batch output
            print(f"{func}: update event table with current batch output")
            update_event_sql(bucket, unique_id, batch_cluster_id_list)

            # update cluster output sqlite table with operations and current batch output
            print(f"{func}: update cluster output table with current batch output. get partial metrics dict")
            metrics = update_cluster_output_sql(batch_cluster_id_list)

            # update cluster ticket output sqlite for current batch output
            print(f"{func}: update cluster ticket output table with current batch output. create per per cluster metrics")
            update_cluster_ticket_output_sql(bucket, batch_cluster_id_list, metrics)

            lambda_response = {}

            if queue_json["next"] is None:
                print(f"{func}: Start flow detection as this is last chunk of input")

                if USER_OVERRIDE_CLUSTER:
                    lambda_response = {
                        # for step function
                        "TransformJobName": "user_override_flow",
                        "S3InputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/flow_out/{unique_id}/flow_output.json.out",
                        "S3OutputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/flow_out/{unique_id}/flow_output.json.out",
                    }

                    return lambda_response

                print(f"{func}: Save input to sequence alerts to: {input_to_sequence_alerts}")

                print(f"{func}: Get last {FLOW_INPUT_WINDOW_SIZE} clusters as input to flow model")
                flow_input = get_last_n_clusters()
                flow_input = [dict(row) for row in flow_input]

                print(f"{func}: Process data for flow model input")
                for row in flow_input:
                    row["cluster_srcips"] = json.loads(row["cluster_srcips"])
                    row["cluster_dstips"] = json.loads(row["cluster_dstips"])
                    row["cluster_ips"] = list(set(row["cluster_srcips"] + row["cluster_dstips"]))
                    row["cluster_techs"] = json.loads(row["cluster_techs"])
                    row["cluster_tacs"] = json.loads(row["cluster_tacs"])
                    row["cluster_tacs"] = [int(t) for t in row["cluster_tacs"]]
                    row["cluster_stages"] = json.loads(row["cluster_stages"])
                    row["cluster_stages"] = [int(t) for t in row["cluster_stages"]]

                print(f"{func}: Save input for flow model")
                with open(TEMP_INPUT_TO_SEQUENCE_ALERTS_JSON_FILENAME, "w") as of:
                    req = {"request_id": "1", "input": flow_input}
                    if CONFIG_EXISTS_IN_REQUEST:
                        print(f"{func}: Flow model config added to request")
                        req["config"] = json.load(open(TEMP_CONFIG_INPUT_FILENAME, "r"))
                    json.dump(req, of)

                try:
                    S3_CLIENT.upload_file(TEMP_INPUT_TO_SEQUENCE_ALERTS_JSON_FILENAME, bucket, input_to_sequence_alerts)
                    print(f"{func}: Input to sequence alerts saved")
                except Exception as e:
                    print(f"{func}: Failed to upload file to S3: {bucket}/{input_to_sequence_alerts}")
                    raise e

                filename = input_to_sequence_alerts.split("/")[-1]

                path_to_flow_response = f"{SCRATCH_DIR}/response/flow_out/{unique_id}/"

                lambda_response = {
                    "TransformJobName": "transform-job-flow",

                    # for ecs task
                    # "Cluster": "arn:aws:ecs:us-west-2:582441423537:cluster/tech-ecs-1",
                    # "TaskDefinition": "arn:aws:ecs:us-west-2:582441423537:task-definition/tech-ecs-1-task:9",
                    "Cluster": ECS_CLUSTER_ARN,
                    "TaskDefinition": FLOW_TASK_ARN,

                    "S3InputPath": f"s3://{bucket}/{input_to_sequence_alerts}",
                    "S3OutputPath": f"s3://{bucket}/{path_to_flow_response}{filename}.out"
                }

            else:
                if USER_OVERRIDE_CLUSTER:
                    lambda_response = {
                        # for step function
                        "TransformJobName": "user_override_cluster",
                        "S3InputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/cluster_out/{queue_json['next']}/cluster_output.zip.out",
                        "S3OutputPath": f"s3://{bucket}/{SCRATCH_DIR}/response/cluster_out/{queue_json['next']}/cluster_output.zip.out",
                    }

                    return lambda_response

                aggregate_and_cluster_alerts(bucket, queue_json["next"])

                filename = "input.zip"
                path_to_embedding_output = f"{SCRATCH_DIR}/output/embedding/{queue_json['next']}/{filename}"
                path_to_cluster_response = f"{SCRATCH_DIR}/response/cluster_out/{queue_json['next']}/"
                lambda_response = {
                    "TransformJobName": f"transform-job-cluster-{queue_json['next']}",

                    # for ecs task
                    # "Cluster": "arn:aws:ecs:us-west-2:582441423537:cluster/tech-ecs-1",
                    # "TaskDefinition": "arn:aws:ecs:us-west-2:582441423537:task-definition/tech-ecs-1-task:9",
                    "Cluster": ECS_CLUSTER_ARN,
                    "TaskDefinition": CLUSTER_TASK_ARN,

                    "S3InputPath": f"s3://{bucket}/{path_to_embedding_output}",
                    "S3OutputPath": f"s3://{bucket}/{path_to_cluster_response}{filename}.out"
                }

            return lambda_response


        def aggregate_and_cluster_alerts(bucket, unique_id):
            '''
            Create input and start next batch transform job from the queue
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk used for next batch transform job
            '''

            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME

            func = "aggregate_and_cluster_alerts"

            next_input_filename = f"{SCRATCH_DIR}/output/embedding/{unique_id}/input.gz"

            temp_input_dir = "/tmp/input/"
            os.makedirs(temp_input_dir, exist_ok=True)

            temp_next_input_filename = f"{temp_input_dir}input.gz"

            print(f"{func}: Download next input file: {bucket}/{next_input_filename}")

            try:
                S3_CLIENT.download_file(bucket, next_input_filename, temp_next_input_filename)
                print(f"{func}: Next input file download completed")
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{next_input_filename}")
                raise e

            print(f"{func}: Create next zip for clustering")

            temp_enriched_alerts_with_techniques_output_filename = "input.gz"
            temp_cluster_config_filename = "cluster_config.json"

            with zipfile.ZipFile(TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME, "a") as z:
                z.write(temp_next_input_filename, arcname=temp_enriched_alerts_with_techniques_output_filename)
                if CLUSTER_CONFIG_EXISTS_IN_REQUEST:
                    z.write(TEMP_CLUSTER_CONFIG_INPUT_FILENAME, arcname=temp_cluster_config_filename)
                print(f"{func}: Next zip input contents: {z.namelist()}")

            path_to_embedding_output = f"{SCRATCH_DIR}/output/embedding/{unique_id}/input.zip"

            try:
                print(f"{func}: Save next zip input to clustering: {bucket}/{path_to_embedding_output}")
                S3_CLIENT.upload_file(TEMP_AGGREGATE_AND_CLUSTER_ALERTS_OUTPUT_FILENAME, bucket, path_to_embedding_output)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_embedding_output}")
                raise e


        def get_batch_cluster_ids_update_input_cluster_ids(bucket, unique_id):
            '''
            Get cluster ids in current batch and update cluster ids in input
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk
            Returns:
                list of cluster ids in current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME

            func = "get_batch_cluster_ids_update_input_cluster_ids"

            queue_json = json.load(open(TEMP_INTERMEDIATE_QUEUE_FILENAME, "r"))

            # if its first batch of current input. initialize cluster id list. else download from previous batch
            if queue_json["first"] == unique_id:
                print(f"{func}: This is first batch of current input. Initialize cluster id list for input")
                json.dump([], open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "w"))
            else:
                path_to_prev_cluster_id_list = f"{SCRATCH_DIR}/intermediate/{queue_json['previous']}/input_cluster_id_list.json"
                try:
                    S3_CLIENT.download_file(bucket, path_to_prev_cluster_id_list, TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME)
                    print(f"{func}: Download file from S3: {bucket}/{path_to_prev_cluster_id_list}")
                except Exception as e:
                    print(f"{func}: Failed to donwnload file from S3: {bucket}/{path_to_prev_cluster_id_list}")
                    raise e

            print(f"{func}: Update cluster id list for input")
            cluster_id_list_json = json.load(open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "r"))
            cluster_id_list_set = set(cluster_id_list_json)

            batch_cluster_df = pd.read_json(TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME)
            batch_cluster_id_list = batch_cluster_df["cluster_id"].to_list()
            batch_cluster_id_list.sort()

            cluster_id_list_list = list(cluster_id_list_set.union(batch_cluster_id_list))

            json.dump(cluster_id_list_list, open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "w"))

            path_to_cluster_id_list = f"{SCRATCH_DIR}/intermediate/{unique_id}/input_cluster_id_list.json"
            print(f"{func}: Save file to S3: {bucket}/{path_to_cluster_id_list}")

            try:
                S3_CLIENT.upload_file(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, bucket, path_to_cluster_id_list)
                print(f"{func}: Uploaded file to S3: {bucket}/{path_to_cluster_id_list}")
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_cluster_id_list}")
                raise e

            return batch_cluster_id_list


        def save_aggreate_cluster_json_output(bucket, unique_id):
            '''
            Save aggregated alerts and current batch cluster output to S3
            Args:
                bucket: Bucket name
                unique_id: Unique id of the chunk
            '''

            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_AGGREGATED_ALERTS_JSON_FILENAME

            func = "save_aggreate_cluster_json_output"
            aggregate_and_cluster_alerts_json_output_prefix = f"{SCRATCH_DIR}/intermediate/{unique_id}/"

            cluster_of_aggregated_alerts_json_filename = f"{aggregate_and_cluster_alerts_json_output_prefix}cluster_output.json"
            aggregated_alerts_json_filename = f"{aggregate_and_cluster_alerts_json_output_prefix}aggregated_alerts_output.json"

            print(f"{func}: Save aggregated alerts and clusters json output: {aggregate_and_cluster_alerts_json_output_prefix}")
            try:
                S3_CLIENT.upload_file(TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME, bucket,
                                      cluster_of_aggregated_alerts_json_filename)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{cluster_of_aggregated_alerts_json_filename}")
                raise e

            try:
                S3_CLIENT.upload_file(TEMP_AGGREGATED_ALERTS_JSON_FILENAME, bucket, aggregated_alerts_json_filename)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{aggregated_alerts_json_filename}")
                raise e


        def update_event_sql(bucket, unique_id, batch_cluster_id_list):
            '''
            Update event table in sqlite from current batch cluster output.
            Save current batch cluster output with current batch user given alert ids associated to it
            Args:
                bucket: Bucket name
                unique_id: Unique id of the current batch
                batch_cluster_id_list: current batch active cluster id list
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_CLUSTER_OF_AGGREGATED_ALERTS_JSON_FILENAME
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE
            global TEMP_CHUNK_INPUT_FILE
            global TEMP_CEF_INPUT

            func = "update_event_sql"

            print(f"{func}: Read aggregate alert output, cluster output from current batch")
            agg_df = pd.read_json(TEMP_AGGREGATED_ALERTS_JSON_FILENAME)
            # reduce the amount of agg data we read in memory
            agg_df = agg_df[["tech", "tac", "stage", "ids", "cluster_id"]]
            agg_df = agg_df[agg_df["cluster_id"].isin(batch_cluster_id_list)]

            internal_id_to_user_id_json = json.load(open(TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE, "r"))

            # get all user alert ids for cluster
            print(f"{func}: Get user alert ids in aggregate alert output")
            internal_id_exploded_agg_df = agg_df.explode("ids")
            internal_id_exploded_agg_df = internal_id_exploded_agg_df.apply(lambda row: get_user_ids(row,
                                                                                                     internal_id_to_user_id_json),
                                                                            axis=1)

            # get all original alerts
            print(f"{func}: Get original input alerts for current batch")
            chunk_input_file = f"{SCRATCH_DIR}/intermediate/{unique_id}/original_input.json"

            try:
                S3_CLIENT.download_file(bucket, chunk_input_file, TEMP_CHUNK_INPUT_FILE)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{chunk_input_file}")
                raise e

            original_alerts_df = pd.read_json(TEMP_CHUNK_INPUT_FILE)
            if "other_attributes_dict" not in original_alerts_df:
                original_alerts_df["other_attributes_dict"] = [{}] * len(original_alerts_df)
            else:
                original_alerts_df["other_attributes_dict"] = original_alerts_df["other_attributes_dict"].apply(lambda x: {} if pd.isna(x) else x)
            if "tech" in original_alerts_df:
                original_alerts_df.drop(columns=["tech"], inplace=True)

            original_alert_keep_cols = original_alerts_df.columns.to_list()
            original_alert_keep_cols = list(set(original_alert_keep_cols) - set(["tech", "tac", "stage", "ids"]))
            original_alerts_df = original_alerts_df[original_alert_keep_cols]
            original_alert_keep_cols.extend(["tech", "tac", "stage", "ids", "cluster_id"])

            print(f"{func}: Original alert keep columns: {original_alert_keep_cols}")

            # filter aggregate alert df with alerts for current batch only
            print(f"{func}: Filter aggregated alert with alerts from current batch")
            internal_id_exploded_agg_df = internal_id_exploded_agg_df[internal_id_exploded_agg_df["user_id"].isin(original_alerts_df["id"])]

            to_merge_internal_id_exploded_agg_df = internal_id_exploded_agg_df[["tech", "tac", "stage", "user_id", "ids", "cluster_id"]]

            print(f"{func}: Length of original alerts before merge: {len(original_alerts_df)}")
            # enrich original alerts with tech, tac, stage
            print(f"{func}: Enrich original alerts with tech, tac, stage")
            original_alerts_df = original_alerts_df.merge(to_merge_internal_id_exploded_agg_df,
                                                          how="left",
                                                          left_on="id",
                                                          right_on="user_id")
            print(f"{func}: Original alert columns after enriching: {original_alerts_df.columns.to_list()}")
            print(f"{func}: Length of original alerts after merge: {len(original_alerts_df)}")
            original_alerts_df = original_alerts_df[original_alert_keep_cols]
            original_alerts_df = original_alerts_df[~original_alerts_df["tech"].isna()]

            current_batch_cluster_ids = original_alerts_df["cluster_id"].unique()

            path_to_cef_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cef_input.json"

            S3_CLIENT.download_file(bucket, path_to_cef_input, TEMP_CEF_INPUT)

            cef_input_df = pd.read_json(TEMP_CEF_INPUT)

            print(f"{func}: Get event id list for cef input")
            event_id_list = original_alerts_df["id"].to_list()
            if MAP_CEF_TO_INTERNAL:
                event_id_list = [event_id[-40:].split(":")[-1] for event_id in event_id_list]

                print(f"{func}: Filter and sort cef input, original input on event id")
                cef_input_df = cef_input_df[cef_input_df["_event_id"].isin(event_id_list)]
                cef_input_df.sort_values("_event_id")
            else:
                cef_input_df = cef_input_df[cef_input_df["id"].isin(event_id_list)]
                cef_input_df.sort_values("id")

            original_alerts_df["event_id"] = event_id_list
            original_alerts_df.sort_values("event_id")

            insert_event = """
            INSERT INTO event(
                alert_id,
                event_id,
                internal_id,
                unique_id,
                src,
                dst,
                time,
                name,
                tech,
                tac,
                stage,
                other_attributes,
                cluster_id,
                raw_data
            ) VALUES(
                %(alert_id)s,
                %(event_id)s,
                %(internal_id)s,
                %(unique_id)s,
                %(src)s,
                %(dst)s,
                %(time)s,
                %(name)s,
                %(tech)s,
                %(tac)s,
                %(stage)s,
                %(other_attributes)s,
                %(cluster_id)s,
                %(raw_data)s
            );
            """

            insert_global_feature = """
            INSERT INTO global_feature(
                feature,
                feature_type,
                added_to_ui
            ) VALUES(
                %(feature)s,
                %(feature_type)s,
                %(added_to_ui)s
            )
            ON CONFLICT (feature, feature_type) DO NOTHING
            ;
            """

            insert_or_ignore_cluster = """
            INSERT INTO cluster_output(
                cluster_id
            )
            VALUES(
                %(cluster_id)s
            )
            ON CONFLICT (cluster_id) DO NOTHING
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        insert_or_ignore_cluster_params = []
                        for current_batch_cluster_id in current_batch_cluster_ids:
                            params = {
                                "cluster_id": int(current_batch_cluster_id)
                            }
                            insert_or_ignore_cluster_params.append(params)

                        # insert or ignore cluster output
                        print(f"{func}: Add or ignore current batch cluster to cluster output table")
                        cursor.executemany(insert_or_ignore_cluster, insert_or_ignore_cluster_params)

                        insert_event_params = []
                        unique_other_attributes_key_set = set()
                        for (_, row_original_alerts), (_, row_cef_input_alert) in zip(original_alerts_df.iterrows(), cef_input_df.iterrows()):
                            # Execute a query
                            params = {
                                "alert_id": row_original_alerts['id'],
                                "event_id": row_original_alerts['event_id'],
                                "internal_id": row_original_alerts['ids'],
                                "unique_id": unique_id,
                                "src": row_original_alerts['src'],
                                "dst": row_original_alerts['dst'],
                                "time": row_original_alerts['time'],
                                "name": row_original_alerts['name'],
                                "tech": json.dumps(row_original_alerts['tech']),
                                "tac": json.dumps(list(map(str, row_original_alerts['tac']))),
                                "stage": json.dumps(list(map(str, row_original_alerts['stage']))),
                                "other_attributes": json.dumps(row_original_alerts['other_attributes_dict']),
                                "cluster_id": row_original_alerts['cluster_id'],
                                "raw_data": row_cef_input_alert.to_json(),
                            }
                            insert_event_params.append(params)

                            unique_other_attributes_key_set = unique_other_attributes_key_set.union(list(row_original_alerts['other_attributes_dict'].keys()))

                        insert_global_feature_params = []
                        for other_attrs_key in unique_other_attributes_key_set:

                            global_feature_params = {
                                "feature": other_attrs_key,
                                "feature_type": "event",
                                "added_to_ui": 0
                            }
                            insert_global_feature_params.append(global_feature_params)

                        print(f"{func}: Add current batch alert to event table")
                        cursor.executemany(insert_event, insert_event_params)

                        print(f"{func}: Add event features to table")
                        cursor.executemany(insert_global_feature, insert_global_feature_params)

                    conn.commit()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_cluster_output_sql(batch_cluster_id_list):
            '''
            Update cluster output table in sqlite
            Args:
                batch_cluster_id_list: cluster id in current batch
            Returns:
                metrics dictionary for cluster id (contains count of unique tech, tac, stage per cluster)
            '''

            func = "update_cluster_output_sql"

            update_cluster_query = """
            UPDATE cluster_output
            SET cluster_starttime = %(cluster_starttime)s,
                cluster_endtime = %(cluster_endtime)s,
                cluster_srcips = %(cluster_srcips)s,
                cluster_dstips = %(cluster_dstips)s,
                cluster_techs = %(cluster_techs)s,
                cluster_tacs = %(cluster_tacs)s,
                cluster_stages = %(cluster_stages)s
            WHERE cluster_id = %(cluster_id)s
            ;
            """

            metrics = {}

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        print(f"{func}: Update cluster details in cluster output table")
                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"

                        select_events_filter_cluster_query = f"""
                        SELECT
                            cluster_id,
                            MIN(time) AS min_time,
                            MAX(time) AS max_time,
                            STRING_AGG(DISTINCT src, ',') AS unique_src,
                            STRING_AGG(DISTINCT dst, ',') AS unique_dst,
                            STRING_AGG(tech,';') AS combined_tech,
                            STRING_AGG(tac,';') AS combined_tac,
                            STRING_AGG(stage,';') AS combined_stage
                        FROM
                            event
                        WHERE cluster_id IN {cluster_id_list}
                        GROUP BY cluster_id
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_events_filter_cluster_query)
                        result = cursor.fetchall()

                        update_cluster_output_params = []
                        for row in result:
                            row = dict(row)
                            src_ips = row['unique_src']
                            src_ips = src_ips.split(",")

                            dst_ips = row['unique_dst']
                            dst_ips = dst_ips.split(",")

                            combined_tech = row['combined_tech']
                            combined_tech = combined_tech.split(";")
                            tech_list = []
                            for techs in combined_tech:
                                tech_list += json.loads(techs)
                            tech_list = list(set(tech_list))

                            combined_tac = row['combined_tac']
                            combined_tac = combined_tac.split(";")
                            tac_list = []
                            for tacs in combined_tac:
                                tac_list += json.loads(tacs)
                            tac_list = list(set(tac_list))

                            combined_stage = row['combined_stage']
                            combined_stage = combined_stage.split(";")
                            stage_list = []
                            for stage in combined_stage:
                                stage_list += json.loads(stage)
                            stage_list = list(set(stage_list))

                            metrics[row['cluster_id']] = {
                                "tech": {"count": len(tech_list)},
                                "tac": {"count": len(tac_list)},
                                "stage": {"count": len(stage_list)}
                            }

                            # if cluster id is present in table. Update fields for existing row.
                            params = {
                                "cluster_starttime": row['min_time'],
                                "cluster_endtime": row['max_time'],
                                "cluster_srcips": json.dumps(src_ips),
                                "cluster_dstips": json.dumps(dst_ips),
                                "cluster_techs": json.dumps(tech_list),
                                "cluster_tacs": json.dumps(tac_list),
                                "cluster_stages": json.dumps(stage_list),
                                "cluster_id": row['cluster_id']
                            }

                            update_cluster_output_params.append(params)

                        cursor.executemany(update_cluster_query, update_cluster_output_params)

                    conn.commit()
                return metrics
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_cluster_ticket_output_sql(bucket, batch_cluster_id_list, metrics):
            '''
            Update cluster ticket output table and add per cluster metrics
            Args:
                bucket: Bucket name
                batch_cluster_id_list: cluster id in current batch
                metrics: dict of metric for cluster id (contains count of unique tech, tac, stage per cluster)
            '''
            global TEMP_TICKET_ID_LOOKUP_FILE

            func = "update_cluster_ticket_output_sql"

            metrics_cols = ["tech", "tac", "stage"]

            ticket_id_json = json.load(open(TEMP_TICKET_ID_LOOKUP_FILE, "r"))
            ticket_id_start = ticket_id_json["last_ticket_id"]

            insert_cluster_ticket_output_query = """
            INSERT INTO cluster_ticket_output (
                cluster_id,
                ticket_id,
                metrics
            )
            VALUES (
                %(cluster_id)s,
                %(ticket_id)s,
                %(metrics)s
            )
            ;
            """

            update_cluster_ticket_output_query = """
            UPDATE cluster_ticket_output
            SET
                metrics = %(metrics)s
            WHERE cluster_id = %(cluster_id)s
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        print(f"{func}: Get events for clusters in current batch")
                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"
                        select_event_query = f"""
                        SELECT *
                        FROM event
                        WHERE cluster_id IN {cluster_id_list}
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_event_query)
                        results = cursor.fetchall()
                        results = [dict(row) for row in results]
                        events_df = pd.DataFrame(results)

                        print(f"{func}: Get existing cluster ticket output for clusters in current batch")
                        select_cluster_ticket_output_query = f"""
                        SELECT *
                        FROM cluster_ticket_output
                        WHERE cluster_id IN {cluster_id_list}
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_cluster_ticket_output_query)
                        results = cursor.fetchall()
                        results = [dict(row) for row in results]
                        cluster_ticket_output_df = pd.DataFrame(results)

                        cluster_ticket_dict = {}
                        if len(cluster_ticket_output_df):
                            cluster_ticket_dict = cluster_ticket_output_df.set_index('cluster_id')['ticket_id'].to_dict()

                        print(f"{func}: Get per cluster metrics and update cluster ticket output")
                        insert_cluster_ticket_output_params = []
                        update_cluster_ticket_output_params = []
                        for cluster_id in batch_cluster_id_list:
                            filtered_events = events_df[events_df["cluster_id"] == cluster_id]
                            cluster_metrics = per_cluster_metrics_events(filtered_events)

                            for metric_col in metrics_cols:
                                cluster_metrics[metric_col]["count"] = float(metrics[cluster_id][metric_col]["count"])

                            if cluster_id not in cluster_ticket_dict:
                                ticket_id_start += 1
                                params = {
                                    "cluster_id": cluster_id,
                                    "ticket_id": ticket_id_start,
                                    "metrics": json.dumps(cluster_metrics)
                                }
                                insert_cluster_ticket_output_params.append(params)
                            else:
                                params = {
                                    "cluster_id": cluster_id,
                                    "metrics": json.dumps(cluster_metrics)
                                }
                                update_cluster_ticket_output_params.append(params)
                        if insert_cluster_ticket_output_params:
                            cursor.executemany(insert_cluster_ticket_output_query, insert_cluster_ticket_output_params)
                        if update_cluster_ticket_output_params:
                            cursor.executemany(update_cluster_ticket_output_query, update_cluster_ticket_output_params)

                    conn.commit()

                print(f"{func}: Save ticket id lookup file")
                ticket_id_json["last_ticket_id"] = ticket_id_start
                json.dump(ticket_id_json, open(TEMP_TICKET_ID_LOOKUP_FILE, "w"))
                save_ticket_id_lookup_file(bucket)
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def per_cluster_metrics_events(involved_events_df):
            '''
            Get metrics per cluster
            Args:
                involved_events_df: involved events of one cluster
            Returns:
                metrics per cluster
            '''

            metrics_cols = ["tech", "tac", "stage"]

            metrics = {}

            for metric_col in metrics_cols:
                involved_events_df[f'{metric_col}'] = involved_events_df[f'{metric_col}'].apply(json.loads)
                involved_events_df[f'{metric_col}_count'] = involved_events_df[f'{metric_col}'].apply(len)

                metrics[metric_col] = {
                    "avg": float(involved_events_df[f"{metric_col}_count"].mean()),
                    "min": float(involved_events_df[f"{metric_col}_count"].min()),
                    "max": float(involved_events_df[f"{metric_col}_count"].max()),
                    "median": float(involved_events_df[f"{metric_col}_count"].median())
                }

            return metrics


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download intermediate files required
            Args:
                bucket: Bucket name
                unique_id: unique id of current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_CONFIG_INPUT_FILENAME
            global CONFIG_EXISTS_IN_REQUEST
            global TEMP_INTERMEDIATE_QUEUE_FILENAME
            global INTERNAL_ID_TO_USER_ID_OBJECT_KEY
            global TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE
            global CLUSTER_CONFIG_EXISTS_IN_REQUEST
            global TEMP_CLUSTER_CONFIG_INPUT_FILENAME

            func = "download_intermediate_input_files"

            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            path_to_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/config.json"
            path_to_cluster_config_from_input = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_config.json"

            try:
                print(f"{func}: Download queue json file from S3")
                S3_CLIENT.download_file(bucket, path_to_intermediate_queue, TEMP_INTERMEDIATE_QUEUE_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{path_to_intermediate_queue}")
                raise e

            try:
                S3_CLIENT.download_file(bucket, INTERNAL_ID_TO_USER_ID_OBJECT_KEY, TEMP_INTERNAL_ID_TO_USER_ID_LOOKUP_FILE)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{INTERNAL_ID_TO_USER_ID_OBJECT_KEY}")
                raise e

            try:
                print(f"{func}: Checking if cluster config file exists on S3 path {bucket}/{path_to_cluster_config_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_cluster_config_from_input)
                CLUSTER_CONFIG_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Cluster config file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_cluster_config_from_input, TEMP_CLUSTER_CONFIG_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_cluster_config_from_input}")
                    raise e

            try:
                print(f"{func}: Checking if config file exists on S3 path {bucket}/{path_to_config_from_input}")
                S3_CLIENT.head_object(Bucket=bucket, Key=path_to_config_from_input)
                CONFIG_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e
                print(f"{func}: Config file does not exist")
            else:
                try:
                    S3_CLIENT.download_file(bucket, path_to_config_from_input, TEMP_CONFIG_INPUT_FILENAME)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{path_to_config_from_input}")
                    raise e


        def save_ticket_id_lookup_file(bucket):
            '''
            Save ticket id lookup file to S3
            Args:
                bucket: The S3 bucket name
            '''
            global TICKET_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_TICKET_ID_LOOKUP_FILE
            func = "save_ticket_id_lookup_file"
            try:
                print(f"{func}: Upload ticket id lookup file to S3")
                S3_CLIENT.upload_file(TEMP_TICKET_ID_LOOKUP_FILE, bucket, TICKET_ID_OBJECT_KEY)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{TICKET_ID_OBJECT_KEY}")
                raise e


        def fetch_or_create_ticket_id_lookup_file(bucket):
            '''
            Check if the ticket_id lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global TICKET_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_TICKET_ID_LOOKUP_FILE

            func = "fetch_or_create_ticket_id_lookup_file"

            try:
                print(
                    f"{func}: Checking if ticket id lookup file exists on S3 path {bucket}/{TICKET_ID_OBJECT_KEY}")

                S3_CLIENT.head_object(Bucket=bucket, Key=TICKET_ID_OBJECT_KEY)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Ticket id lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = {"last_ticket_id": 0}
                json.dump(internal_id_to_unique_id_json, open(TEMP_TICKET_ID_LOOKUP_FILE, "w"))

                print(f"{func}: Initiated ticket id")
            else:
                try:
                    print(f"{func}: Download available internal id to unique id lookup file from S3")

                    S3_CLIENT.download_file(bucket, TICKET_ID_OBJECT_KEY, TEMP_TICKET_ID_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{TICKET_ID_OBJECT_KEY}")
                    raise e


        def get_user_ids(row, internal_id_to_user_id_json):
            '''
            Get user alert id for a row
            Args:
                row: A row containing internal id
                internal_id_to_user_id_json: A dictionary mapping user ids to internal ids
            Returns:
                The row with the added unique_id field
            '''

            row["user_id"] = internal_id_to_user_id_json[row["ids"]]
            return row


        def sort_by_alert_id(obj):
            '''
            sort by alert id
            Args:
                obj: aggregate alert dictionary object
            Returns:
                aggregate alert id
            '''
            return obj["aggalert_id"]


        def sort_by_cluster_id(obj):
            '''
            sort by cluster id
            Args:
                obj: cluster dictionary object
            Returns:
                cluster id
            '''
            return obj["cluster_id"]


        def get_last_n_clusters():
            '''
            Get last n clusters from cluster output

            Returns:
                list of cluster output rows
            '''
            global FLOW_INPUT_WINDOW_SIZE

            func = "get_last_n_clusters"

            select_query = """
            SELECT *
            FROM cluster_output
            ORDER BY cluster_id DESC
            LIMIT %(limit)s
            ;
            """
            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        params = {"limit": FLOW_INPUT_WINDOW_SIZE}
                        cursor.execute(select_query, params)

                        results = cursor.fetchall()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            return results


        def get_output_object_key(event):
            '''
            Get output object key from input and output s3 path
            Args:
                event: Input to lambda
            Returns:
                (bucket_name, object_key)
            '''
            func = "get_output_object_key"

            input_bucket, input_key = parse_s3_path(event["inputPath"])

            print(f"{func}: Input bucket: {input_bucket}")
            print(f"{func}: Input key: {input_key}")

            output_bucket, output_key = parse_s3_path(event["outputPath"])

            print(f"{func}: Output bucket: {output_bucket}")
            print(f"{func}: Output key: {output_key}")

            if input_bucket is None or input_key is None or output_bucket is None or output_key is None:
                raise ValueError("Failed to get output path of the batch transform job")

            return output_bucket, output_key


        def parse_s3_path(s3_path):
            '''
            Parse an S3 path and extract the bucket name and object key.
            Args:
                s3_path: The S3 path to parse. Should be in the format: s3://bucket-name/path/to/object
            Returns:
                (bucket_name, object_key).
                (None, None) if the path is invalid.
            '''
            if not s3_path.startswith("s3://"):
                return None, None

            # Remove the "s3://" prefix
            path_without_prefix = s3_path[5:]

            # Split the remaining path into parts
            parts = path_without_prefix.split('/', 1)

            # If there's only one part, it means there's no object key
            if len(parts) == 1:
                return parts[0], None

            # If there are two parts, the first is the bucket name, the second is the object key
            return parts[0], parts[1]


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          flow_input_window_size: !Ref FlowInputMaxClusters
          db_connection_string: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/postgres
          ecs_cluster_arn: !GetAtt ECSCluster.Arn
          cluster_task_arn: !GetAtt ClusterPart2TaskDefinition.TaskDefinitionArn
          flow_task_arn: !GetAtt FlowTaskDefinition.TaskDefinitionArn
          map_cef_to_internal: 'true'
  processClusterLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${processCluster}
  processClusterLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref processCluster
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  processFlow:
    Type: AWS::Serverless::Function
    DependsOn:
    - DbLoadBalancer
    Properties:
      FunctionName: !Sub ${AWS::StackName}_process_flow
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: processFlow
      InlineCode: |
        '''
        Process response from flow detection model. Extract cluster, flow. Create enriched_alerts_input

        Input: Response from Flow detection model
        '''

        import os
        import json
        import urllib
        import glob
        from datetime import datetime, timezone
        import sqlite3
        from contextlib import closing
        import copy
        import gc

        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        import sys
        import subprocess

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        subprocess.call('pip install psycopg2-binary==2.9.9 sqlalchemy==2.0.32 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import psycopg2
        from psycopg2 import sql
        from psycopg2.extras import DictCursor

        FLOW_INPUT_WINDOW_SIZE = os.getenv("flow_input_window_size")

        CAMPAIGN_MAP = os.getenv("campaign_map")

        SKIP_SINGLE_ALERT = os.getenv("skip_single_alert")

        MAP_CEF_TO_INTERNAL = os.getenv("map_cef_to_internal")

        DB_CONNECTION_STRING = os.getenv("db_connection_string")

        missing_variables = []
        if FLOW_INPUT_WINDOW_SIZE is None:
            missing_variables.append("flow_input_window_size")
        if CAMPAIGN_MAP is None:
            missing_variables.append("campaign_map")
        if SKIP_SINGLE_ALERT is None:
            SKIP_SINGLE_ALERT = "true"

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")
        if CAMPAIGN_MAP not in ["flow", "cluster"]:
            raise ValueError(f"Valid values for 'campaign_map' environemnt variable are 'flow', 'cluster'. Entered value is: '{CAMPAIGN_MAP}'")
        if SKIP_SINGLE_ALERT not in ["true", "false"]:
            raise ValueError(f"Valid values for 'skip_single_alert' environemnt variable are 'true', 'false'. Entered value is: '{SKIP_SINGLE_ALERT}'")

        FLOW_INPUT_WINDOW_SIZE = int(FLOW_INPUT_WINDOW_SIZE)

        if MAP_CEF_TO_INTERNAL is not None:
            MAP_CEF_TO_INTERNAL = (MAP_CEF_TO_INTERNAL == "true")
        else:
            MAP_CEF_TO_INTERNAL = True

        S3_CLIENT = boto3.client("s3")

        SCRATCH_DIR = "scratch"

        NODE_FEATURE_EXISTS_IN_REQUEST = False
        USER_OVERRIDE_CLUSTER = False

        TEMP_INPUT_DIR = "/tmp/input"
        os.makedirs(TEMP_INPUT_DIR, exist_ok=True)

        TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME = "/tmp/input_cluster_id_list.json"

        TEMP_USER_CLUSTER_OUTPUT_FILENAME = "/tmp/user_cluster_output.json"

        TEMP_USER_EVENT_OUTPUT_FILENAME = "/tmp/user_event_output.json"

        TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME = "/tmp/aggregate_model_output.json"

        TEMP_FLOW_OUTPUT_JSON_FILENAME = "/tmp/flow_output.json"
        TEMP_USER_FLOW_OUTPUT_JSON_FILENAME = "/tmp/user_flow_output.json"
        TEMP_FLOW_TICKET_OUTPUT_FILENAME = "/tmp/flow_ticket_output.json"

        SPLUNK_USER_CLUSTER_JSON_LINES = f"/tmp/splunk_user_cluster_output.json"
        SPLUNK_USER_FLOW_JSON_LINES = f"/tmp/splunk_user_flow_output.json"

        TICKET_ID_OBJECT_KEY = f"{SCRATCH_DIR}/ticket_id.json"
        TEMP_TICKET_ID_LOOKUP_FILE = "/tmp/ticket_id.json"

        QUEUE_LOOKUP_FILE = f"{SCRATCH_DIR}/queue_lookup.json"
        TEMP_QUEUE_LOOKUP_FILE = "/tmp/queue_lookup.json"

        TEMP_QUEUE_FILENAME = "/tmp/queue.json"

        TEMP_NODE_FEATURE_INPUT_FILENAME = "/tmp/node_feature.json"
        NODE_FEATURE_LOOKUP_FILE = f"{SCRATCH_DIR}/node_feature_lookup.json"
        TEMP_NODE_FEATURE_LOOKUP_FILE = "/tmp/node_feature_lookup.json"

        TEMP_FLOW_GLOBAL_METRICS_FILENAME = "/tmp/flow_global_metrics.json"
        TEMP_GLOBAL_FEATURE_FILENAME = "/tmp/global_feature.json"

        TEMP_INPUT_CLUSTER_TICKET_OUTPUT_FILENAME = "/tmp/input_cluster_ticket_output.json"
        TEMP_GLOBAL_METRICS_FILENAME = "/tmp/global_metrics.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME
            global SCRATCH_DIR
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global USER_OVERRIDE_CLUSTER
            global MAP_CEF_TO_INTERNAL
            func = "lambda_handler"

            NODE_FEATURE_EXISTS_IN_REQUEST = False
            USER_OVERRIDE_CLUSTER = False

            bucket, sequence_alerts_output = get_output_object_key(event)

            # bucket = event['Records'][0]['s3']['bucket']['name']
            # sequence_alerts_output = event['Records'][0]['s3']['object']['key']

            try:

                clear_temp_dir()

                unique_id = sequence_alerts_output.split("/")[-2]

                print(f"{func}: Unique ID: {unique_id}")

                if "TransformJobName" in event and event["TransformJobName"] == "user_override_flow":
                    print(f"{func}: User override cluster")
                    USER_OVERRIDE_CLUSTER = True
                    MAP_CEF_TO_INTERNAL = False

                print(f"{func}: Download flow output from S3 bucket: {bucket}/{sequence_alerts_output}")
                try:
                    S3_CLIENT.download_file(bucket, sequence_alerts_output, TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME)

                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                try:
                    print(f"{func}: Download intermediate files from S3 bucket")
                    download_intermediate_input_files(bucket, unique_id)
                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                fetch_or_create_ticket_id_lookup_file(bucket)

                print(f"{func}: Starting processing flow output")
                output_filename = process_output_of_sequence_alerts(bucket, unique_id)
                print(f"{func}: Completed processing flow output")

            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            lambda_response = {"bucket": bucket, "input_path": output_filename}
            return lambda_response


        def process_output_of_sequence_alerts(bucket, unique_id):
            '''
            Extract cluster, flow. Create flow ticket output.
            Upload user flow, cluster output, flow ticket output
            Args:
                bucket: Bucket name
                unique_id: Unique id of chunk of input
            '''
            global TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME
            global TEMP_FLOW_OUTPUT_JSON_FILENAME
            global SCRATCH_DIR
            global CAMPAIGN_MAP
            global S3_CLIENT
            global TEMP_QUEUE_LOOKUP_FILE
            global NODE_FEATURE_EXISTS_IN_REQUEST

            func = "process_output_of_sequence_alerts"

            sequence_alerts_output_json = json.load(open(TEMP_SEQUENCE_ALERTS_OUTPUT_FILENAME))
            sequence_alerts_output_json = sequence_alerts_output_json[0]["output"]
            flow_output = sequence_alerts_output_json["flow_output"]

            json.dump(flow_output, open(TEMP_FLOW_OUTPUT_JSON_FILENAME, "w"))

            print(f"{func}: Save flow output to scratch intermediate path")

            flow_output_json_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow.json"
            try:
                print(f"{func}: Save aggregated flows json output: {flow_output_json_filename}")
                S3_CLIENT.upload_file(TEMP_FLOW_OUTPUT_JSON_FILENAME, bucket, flow_output_json_filename)
            except Exception as e:
                print(f"{func}: Failed to save sequence output to S3: {bucket}/{flow_output_json_filename}")
                raise e

            if NODE_FEATURE_EXISTS_IN_REQUEST:
                print(f"{func}: Update global feature for nodes in sql for current input")
                update_global_feature_node_sql()

            create_global_feature_for_ui()

            print(f"{func}: Save global feature for UI to S3")
            global_feature_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/global_feature.json"
            try:
                S3_CLIENT.upload_file(TEMP_GLOBAL_FEATURE_FILENAME, bucket, global_feature_filename)
            except Exception as e:
                print(f"{func}: Failed to upload to S3: {bucket}/{global_feature_filename}")
                raise e

            print(f"{func}: Create cluster output for current input")
            create_input_cluster_output()

            if len(flow_output) < 1:
                print(f"{func}: No flows were detected for the input. Skip creating flow ticket output.")
            else:

                print(f"{func}: Update flow in sql. Create flow ticket output")
                update_flow_sql(bucket, unique_id)

            print(f"{func}: Create event output for current input")
            create_input_event_output(bucket)

            print(f"{func}: Get global metrics for all clusters currently in sql")
            get_global_metrics_from_cluster_ticket_output_sql(bucket, unique_id)

            # keep this code after create_input_event_output. as the cef events are required to be downloaded
            print(f"{func}: create cluster ticket output for input")
            create_input_cluster_ticket_output_sql(bucket, unique_id)

            print(f"{func}: Process cluster output to user readable structure. Save cluster, flow output")
            upload_cluster_and_flow_output(bucket, unique_id)
            print(f"{func}: Saved cluster, flow output")

            if len(flow_output) > 0:
                output_filename = f"{SCRATCH_DIR}/output/flow/{unique_id}/flow_ticket_output.json"
                source_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_ticket_output.json"
                copy_source = {
                    'Bucket': bucket,
                    'Key': source_filename
                }

                try:
                    print(f"{func}: Upload flow ticket output file to output for flow in S3")
                    S3_CLIENT.copy(copy_source, bucket, output_filename)
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{output_filename}")
                    raise e

            output_filename = f"{SCRATCH_DIR}/output/flow/{unique_id}/cluster_ticket_output.json"
            source_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_ticket_output.json"
            copy_source = {
                'Bucket': bucket,
                'Key': source_filename
            }

            try:
                print(f"{func}: Upload cluster ticket output file to output for flow in S3")
                S3_CLIENT.copy(copy_source, bucket, output_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{output_filename}")
                raise e

            print(f"{func}: Saved ticket output")

            print(f"{func}: Check if next input is present to start processing")
            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            queue_json = json.load(open(TEMP_QUEUE_FILENAME, "r"))
            update_queue_lookup([queue_json["first"], unique_id])

            if len(queue_lookup_json["input_queue"]) > 1:
                print(f"{func}: Next input file to process: {queue_lookup_json['input_queue'][1]}")
                try:
                    print(f"{func}: Process next input file")
                    next_input_queue_entry = queue_lookup_json["input_queue"][1]
                    next_input_bucket = next_input_queue_entry.split("/")[0]
                    next_input_key = "/".join(next_input_queue_entry.split("/")[1:])
                    copy_source = {
                        'Bucket': next_input_bucket,
                        'Key': next_input_key
                    }
                    S3_CLIENT.copy_object(CopySource=copy_source, Bucket=next_input_bucket, Key=next_input_key,
                                          Metadata={'UpdatedAt': str(datetime.now())}, MetadataDirective='REPLACE')
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {next_input_bucket}/{next_input_key}")
                    raise e

            return output_filename


        def update_global_feature_node_sql():
            '''
            Update global feature table for current input
            '''
            global TEMP_NODE_FEATURE_LOOKUP_FILE

            func = "update_global_feature_node_sql"

            node_feature_lookup_json = json.load(open(TEMP_NODE_FEATURE_LOOKUP_FILE, "r"))

            insert_global_feature = """
            INSERT INTO global_feature(
                feature,
                feature_type,
                added_to_ui
            ) VALUES(
                %(feature)s,
                %(feature_type)s,
                %(added_to_ui)s
            )
            ON CONFLICT (feature, feature_type) DO NOTHING
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        insert_global_feature_params = []

                        for node_key in node_feature_lookup_json.keys():
                            global_feature_params = {
                                "feature": node_key,
                                "feature_type": "node",
                                "added_to_ui": 0
                            }
                            insert_global_feature_params.append(global_feature_params)

                        print(f"{func}: Add node features to table")
                        cursor.executemany(insert_global_feature, insert_global_feature_params)

                    conn.commit()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def create_global_feature_for_ui():
            '''
            Create global feature file to update on UI
            '''
            global TEMP_GLOBAL_FEATURE_FILENAME

            func = "update_global_feature_node_sql"

            select_global_feature_event = """
            SELECT * FROM global_feature
            WHERE added_to_ui = 0 AND feature_type = 'event'
            ;
            """

            select_global_feature_node = """
            SELECT * FROM global_feature
            WHERE added_to_ui = 0 AND feature_type = 'node'
            ;
            """

            update_global_feature_added_to_ui = """
            UPDATE global_feature
            SET added_to_ui = 1
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        global_feature = {
                            "event_feature": [],
                            "node_feature": []
                        }

                        print(f"{func}: Select global feature event")
                        cursor.execute(select_global_feature_event)

                        result = cursor.fetchall()
                        result = [dict(row) for row in result]
                        global_feature["event_feature"] = [feat["feature"] for feat in result]

                        print(f"{func}: Select global feature node")
                        cursor.execute(select_global_feature_node)

                        result = cursor.fetchall()
                        result = [dict(row) for row in result]
                        global_feature["node_feature"] = [feat["feature"] for feat in result]

                        print(f"{func}: Save global feature")
                        json.dump(global_feature, open(TEMP_GLOBAL_FEATURE_FILENAME, "w"))

                        print(f"{func}: Update sql for features that are used")
                        cursor.execute(update_global_feature_added_to_ui)

                    conn.commit()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def create_input_event_output(bucket):
            '''
            Create user event output for input
            Args:
                bucket: Bucket name
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_INPUT_DIR
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global MAP_CEF_TO_INTERNAL
            global TEMP_USER_EVENT_OUTPUT_FILENAME

            func = "create_input_event_output"

            input_cluster_id_list = json.load(open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "r"))

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        with open(TEMP_USER_EVENT_OUTPUT_FILENAME, "w") as f:

                            cluster_id_list = ",".join(map(str, input_cluster_id_list))
                            cluster_id_list = f"({cluster_id_list})"

                            select_event_query = f"""
                            WITH matched_flows AS (
                                SELECT
                                    fo.flow_id,
                                    unnest(ARRAY(
                                        SELECT json_array_elements_text(fo.alert_ids::json)
                                    )) AS alert_id
                                FROM
                                    flow_output fo
                            )
                            SELECT
                                e.alert_id,
                                e.event_id,
                                e.cluster_id,
                                e.tech,
                                e.tac,
                                e.stage,
                                e.raw_data,
                                ARRAY_AGG(mf.flow_id) AS flow_ids
                            FROM
                                event e
                            LEFT JOIN
                                matched_flows mf ON e.alert_id = mf.alert_id
                            WHERE
                                e.cluster_id IN {cluster_id_list}
                            GROUP BY
                                e.alert_id, e.event_id, e.cluster_id, e.tech, e.tac, e.stage, e.raw_data
                            ;
                            """

                            cursor.execute(select_event_query)
                            event_row = cursor.fetchone()

                            while event_row:
                                event_row = dict(event_row)
                                if event_row["flow_ids"] == [None,]:
                                    event_row["flow_ids"] = []

                                alert = json.loads(event_row["raw_data"])
                                alert["tech"] = json.loads(event_row["tech"])
                                alert["tac"] = json.loads(event_row["tac"])
                                alert["stage"] = json.loads(event_row["stage"])
                                alert["cluster_id"] = event_row["cluster_id"]
                                alert["flow_ids"] = event_row["flow_ids"]

                                f.write(json.dumps(alert) + "\n")

                                event_row = cursor.fetchone()

                    conn.commit()

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def create_input_cluster_output():
            '''
            Create user cluster output for input
            '''
            global TEMP_USER_CLUSTER_OUTPUT_FILENAME
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global MAP_CEF_TO_INTERNAL

            func = "create_input_cluster_output"

            input_cluster_id_list = json.load(open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "r"))

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        cluster_id_list = ",".join(map(str, input_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"
                        select_cluster_output_query = f"""
                        SELECT
                            co.cluster_id,
                            co.cluster_starttime,
                            co.cluster_endtime,
                            co.cluster_srcips,
                            co.cluster_dstips,
                            co.cluster_techs,
                            co.cluster_tacs,
                            co.cluster_stages,
                            STRING_AGG(e.alert_id, ',') AS alert_ids
                        FROM
                            cluster_output co
                        INNER JOIN
                            event e ON co.cluster_id = e.cluster_id
                        WHERE co.cluster_id IN {cluster_id_list}
                        GROUP BY
                            co.cluster_id
                        ;
                        """
                        cursor.execute(select_cluster_output_query)
                        rows = cursor.fetchall()

                        user_cluster_output = []

                        for row in rows:
                            row = dict(row)
                            alert_ids_list = row["alert_ids"].split(",")
                            if SKIP_SINGLE_ALERT and len(alert_ids_list) <= 1:
                                continue

                            if MAP_CEF_TO_INTERNAL:
                                alert_ids_list = [alert_id[:-40] for alert_id in alert_ids_list]
                            splunk_query = ") OR (".join(alert_ids_list)
                            splunk_query = f"({splunk_query})"

                            cluster = {
                                "cluster_id": row["cluster_id"],
                                "ids": alert_ids_list,
                                "splunk_query": splunk_query,
                                "start_time": row["cluster_starttime"],
                                "end_time": row["cluster_endtime"],
                                "src": json.loads(row["cluster_srcips"]),
                                "dst": json.loads(row["cluster_dstips"]),
                                "cluster_techs": json.loads(row["cluster_techs"]),
                                "cluster_tacs": json.loads(row["cluster_tacs"]),
                                "cluster_stages": json.loads(row["cluster_stages"])
                            }

                            user_cluster_output.append(cluster)
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            with open(TEMP_USER_CLUSTER_OUTPUT_FILENAME, "w") as f:
                json.dump(user_cluster_output, f)


        def create_input_cluster_ticket_output_sql(bucket, unique_id):
            '''
            Create cluster ticket output for current input and save to S3
            Args:
                bucket: Bucket name
                unique_id: Unique id of chunk of input
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global TEMP_INPUT_CLUSTER_TICKET_OUTPUT_FILENAME

            func = "create_input_cluster_ticket_output"

            input_cluster_id_list = json.load(open(TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME, "r"))

            try:
                with open(TEMP_INPUT_CLUSTER_TICKET_OUTPUT_FILENAME, "w") as f:
                    with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                        with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                            cluster_id_list = ",".join(map(str, input_cluster_id_list))
                            cluster_id_list = f"({cluster_id_list})"
                            select_query = f"""
                            SELECT
                                co.cluster_id,
                                co.cluster_starttime,
                                co.cluster_endtime,
                                co.cluster_srcips,
                                co.cluster_dstips,
                                co.cluster_techs,
                                co.cluster_tacs,
                                co.cluster_stages,
                                cto.ticket_id,
                                cto.metrics,
                                e.alert_id,
                                e.tech AS event_tech,
                                e.tac AS event_tac,
                                e.stage AS event_stage,
                                e.raw_data AS event_raw_data
                            FROM cluster_ticket_output cto
                            INNER JOIN cluster_output co ON co.cluster_id = cto.cluster_id
                            INNER JOIN event e ON e.cluster_id = cto.cluster_id
                            WHERE cto.cluster_id IN {cluster_id_list}
                            ORDER BY cto.cluster_id
                            ;
                            """

                            cursor.execute(select_query)

                            row = cursor.fetchone()
                            prev_cluster = None

                            involved_events = []

                            while row:
                                row = dict(row)

                                curr_cluster = row
                                if prev_cluster is None:
                                    prev_cluster = curr_cluster

                                if curr_cluster["cluster_id"] != prev_cluster["cluster_id"]:
                                    row_cluster_ticket_output = {
                                        "ticket_id": prev_cluster["ticket_id"],
                                        "cluster_id": prev_cluster["cluster_id"],
                                        "involved_events": involved_events,
                                        "start_time": prev_cluster["cluster_starttime"],
                                        "end_time": prev_cluster["cluster_endtime"],
                                        "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                        "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                        "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                        "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                        "metrics": json.loads(prev_cluster["metrics"]),
                                    }
                                    f.write(json.dumps(row_cluster_ticket_output) + "\n")
                                    involved_events = []

                                events = json.loads(row["event_raw_data"])
                                events["id"] = row["alert_id"]
                                events["tech"] = json.loads(row["event_tech"])
                                events["tac"] = json.loads(row["event_tac"])
                                events["stage"] = json.loads(row["event_stage"])
                                involved_events.append(events)

                                prev_cluster = curr_cluster
                                row = cursor.fetchone()

                            row_cluster_ticket_output = {
                                "ticket_id": prev_cluster["ticket_id"],
                                "cluster_id": prev_cluster["cluster_id"],
                                "involved_events": involved_events,
                                "start_time": prev_cluster["cluster_starttime"],
                                "end_time": prev_cluster["cluster_endtime"],
                                "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                "metrics": json.loads(prev_cluster["metrics"]),
                            }
                            f.write(json.dumps(row_cluster_ticket_output) + "\n")

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            try:
                path_to_batch_input_cluster_ticket_output = f"{SCRATCH_DIR}/intermediate/{unique_id}/cluster_ticket_output.json"
                S3_CLIENT.upload_file(TEMP_INPUT_CLUSTER_TICKET_OUTPUT_FILENAME, bucket, path_to_batch_input_cluster_ticket_output)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_batch_input_cluster_ticket_output}")
                raise e


        def get_global_metrics_from_cluster_ticket_output_sql(bucket, unique_id):
            '''
            Get global metrics from an SQLite database.
            Args:
                bucket: Bucket name
                unique_id: Unique id of chunk of input
            Returns:
                metrics dictionary
            '''
            global TEMP_GLOBAL_METRICS_FILENAME
            global SCRATCH_DIR
            global S3_CLIENT

            func = "get_global_metrics_from_cluster_ticket_output_sql"

            metrics = {}
            metrics_cols = ["tech", "tac", "stage", "event", "entity"]
            mapped_metric_cols = ["cluster_techs", "cluster_tacs", "cluster_stages", "cluster_events", "cluster_entities"]

            # Initialize counters for metrics
            metrics_data = {col: [] for col in metrics_cols}
            time_ranges = []

            event_count = 0

            select_query = """
            SELECT
                co.cluster_id,
                co.cluster_starttime,
                co.cluster_endtime,
                co.cluster_srcips,
                co.cluster_dstips,
                co.cluster_techs,
                co.cluster_tacs,
                co.cluster_stages,
                COUNT(e.cluster_id) AS cluster_events
            FROM
                cluster_output co
            INNER JOIN
                event e ON co.cluster_id = e.cluster_id
            GROUP BY
                co.cluster_id
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        cursor.execute(select_query)

                        row = cursor.fetchone()
                        while row:
                            row = dict(row)
                            record = {
                                "cluster_id": row["cluster_id"],
                                "cluster_starttime": row["cluster_starttime"],
                                "cluster_endtime": row["cluster_endtime"],
                                "cluster_techs": json.loads(row["cluster_techs"]),
                                "cluster_tacs": json.loads(row["cluster_tacs"]),
                                "cluster_stages": json.loads(row["cluster_stages"]),
                                "cluster_events": row["cluster_events"],
                                "cluster_entities": list(set(json.loads(row["cluster_dstips"]) + json.loads(row["cluster_srcips"])))
                            }

                            for metric_col, mapped_metric_col in zip(metrics_cols, mapped_metric_cols):

                                if metric_col == "event":
                                    count = record[mapped_metric_col]
                                    event_count += count
                                    metrics_data[metric_col].append(count)
                                else:
                                    count = float(len(record[mapped_metric_col]))
                                    metrics_data[metric_col].append(count)

                            start_time = pd.to_datetime(record['cluster_starttime'], unit='s')
                            end_time = pd.to_datetime(record['cluster_endtime'], unit='s')
                            time_range = (end_time - start_time).total_seconds()
                            time_ranges.append(time_range)

                            row = cursor.fetchone()
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            # Calculate statistics for each metric
            for metric_col in metrics_cols:
                if metric_col not in metrics:
                    metrics[metric_col] = {}

                if metrics_data[metric_col]:
                    metrics[metric_col]["avg"] = float(pd.Series(metrics_data[metric_col]).mean())
                    metrics[metric_col]["min"] = float(pd.Series(metrics_data[metric_col]).min())
                    metrics[metric_col]["max"] = float(pd.Series(metrics_data[metric_col]).max())
                    metrics[metric_col]["median"] = float(pd.Series(metrics_data[metric_col]).median())
                    metrics[metric_col]["q1"] = float(pd.Series(metrics_data[metric_col]).quantile(0.25))
                    metrics[metric_col]["q3"] = float(pd.Series(metrics_data[metric_col]).quantile(0.75))
                    if metric_col == "event":
                        metrics[metric_col]["count"] = event_count

            # Calculate statistics for time range
            if time_ranges:
                metrics["time_range"] = {
                    "avg": float(pd.Series(time_ranges).mean()),
                    "min": float(pd.Series(time_ranges).min()),
                    "max": float(pd.Series(time_ranges).max()),
                    "median": float(pd.Series(time_ranges).median()),
                    "q1": float(pd.Series(time_ranges).quantile(0.25)),
                    "q3": float(pd.Series(time_ranges).quantile(0.75)),
                }

            json.dump(metrics, open(TEMP_GLOBAL_METRICS_FILENAME, "w"))

            try:
                print(f"{func}: Upload global metric output file to S3")
                global_metrics_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/global_metrics.json"
                S3_CLIENT.upload_file(TEMP_GLOBAL_METRICS_FILENAME, bucket, global_metrics_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{global_metrics_filename}")
                raise e

            return metrics


        def upload_cluster_and_flow_output(bucket, unique_id):
            '''
            Upload user cluster output, flow output to output folder, splunk
            Args:
                bucket: Bucket name
                unique_id: Unique id of current batch
            '''
            global S3_CLIENT
            global TEMP_USER_FLOW_OUTPUT_JSON_FILENAME
            global TEMP_USER_CLUSTER_OUTPUT_FILENAME
            global TEMP_USER_EVENT_OUTPUT_FILENAME
            global SPLUNK_USER_FLOW_JSON_LINES
            global SPLUNK_USER_CLUSTER_JSON_LINES
            func = "upload_cluster_and_flow_output"

            current_timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %z")
            path_to_user_event_output = f"output/{current_timestamp}/event.json"
            path_to_user_cluster_output = f"output/{current_timestamp}/cluster.json"
            path_to_user_flow_output = f"output/{current_timestamp}/flow.json"

            flow_exists = os.path.exists(TEMP_USER_FLOW_OUTPUT_JSON_FILENAME)

            print(f"{func}: Save user cluster and flow output to S3: {path_to_user_event_output}, {path_to_user_cluster_output}, {path_to_user_flow_output}")
            try:
                S3_CLIENT.upload_file(TEMP_USER_EVENT_OUTPUT_FILENAME, bucket, path_to_user_event_output)
                print(f"{func}: Saved user event output to S3")
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_user_event_output}")
                raise e

            print(f"{func}: Save user cluster and flow output to S3: {path_to_user_cluster_output}, {path_to_user_flow_output}")
            try:
                S3_CLIENT.upload_file(TEMP_USER_CLUSTER_OUTPUT_FILENAME, bucket, path_to_user_cluster_output)
                print(f"{func}: Saved user cluster output to S3")
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_user_cluster_output}")
                raise e

            if flow_exists:
                try:
                    S3_CLIENT.upload_file(TEMP_USER_FLOW_OUTPUT_JSON_FILENAME, bucket, path_to_user_flow_output)
                    print(f"{func}: Saved user flow output to S3")
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_user_flow_output}")
                    raise e

            # upload to splunk
            path_to_splunk_flow_output = f"splunk/flow_{unique_id}.json"
            path_to_splunk_cluster_output = f"splunk/cluster_{unique_id}.json"

            # convert to json lines format

            if flow_exists:
                with open(TEMP_USER_FLOW_OUTPUT_JSON_FILENAME, "r") as f_read, open(SPLUNK_USER_FLOW_JSON_LINES, "w") as f_write:
                    user_cluster_output = json.load(f_read)
                    for cluster in user_cluster_output:
                        f_write.write(json.dumps(cluster) + "\n")

            with open(TEMP_USER_CLUSTER_OUTPUT_FILENAME, "r") as f_read, open(SPLUNK_USER_CLUSTER_JSON_LINES, "w") as f_write:
                user_cluster_output = json.load(f_read)
                for cluster in user_cluster_output:
                    f_write.write(json.dumps(cluster) + "\n")

            print(f"{func}: Save flow output to S3 for splunk: {path_to_splunk_flow_output}")

            if flow_exists:
                try:
                    S3_CLIENT.upload_file(SPLUNK_USER_FLOW_JSON_LINES, bucket, path_to_splunk_flow_output)
                    print(f"{func}: Saved flow output to S3")
                except Exception as e:
                    print(f"{func}: Failed to save file to S3: {bucket}/{path_to_splunk_flow_output}")
                    raise e

            try:
                S3_CLIENT.upload_file(SPLUNK_USER_CLUSTER_JSON_LINES, bucket, path_to_splunk_cluster_output)
                print(f"{func}: Saved cluster output to S3")
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{path_to_splunk_cluster_output}")
                raise e


        def update_flow_sql(bucket, unique_id):
            '''
            Update flow output sqlite and create flow ticket output
            Args:
                bucket: Bucket name
                unique_id: Unique id of chunk of input
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_FLOW_OUTPUT_JSON_FILENAME
            global TEMP_USER_FLOW_OUTPUT_JSON_FILENAME
            global TEMP_FLOW_TICKET_OUTPUT_FILENAME
            global TEMP_TICKET_ID_LOOKUP_FILE
            global TEMP_FLOW_GLOBAL_METRICS_FILENAME

            func = "update_flow_sql"

            flow_output_json = json.load(open(TEMP_FLOW_OUTPUT_JSON_FILENAME, "r"))

            print(f"{func}: Number of flows: {len(flow_output_json)}")

            # create ticket id
            ticket_id_json = json.load(open(TEMP_TICKET_ID_LOOKUP_FILE, "r"))
            ticket_id_start = ticket_id_json["last_ticket_id"]

            print(ticket_id_start)

            metrics_cols = ["tech", "tac", "stage", "event", "entity"]
            mapped_metric_cols = ["involved_techs", "involved_tacs", "involved_stages", "ids", "involved_entities"]

            global_metric_time_ranges = []
            event_count = 0
            metrics_data = {col: [] for col in metrics_cols}
            global_metrics = {}

            insert_flow_output_query = """
            INSERT INTO flow_output (
                flow_id,
                cluster_prob,
                alert_ids
            )
            VALUES (
                %(flow_id)s,
                %(cluster_prob)s,
                %(alert_ids)s
            )
            ON CONFLICT (flow_id) DO NOTHING
            ;
            """

            try:
                with open(TEMP_FLOW_TICKET_OUTPUT_FILENAME, "w") as f:
                    with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                        with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                            insert_flow_output_params = []
                            for flow in flow_output_json:
                                flow["flow_id"] = flow["Flow_id"]
                                flow.pop("Flow_id")
                                flow["ids"] = []

                                ticket_id_start += 1
                                flow_ticket = copy.deepcopy(flow)
                                flow_ticket.pop("cluster_ids")
                                flow_ticket["ticket_id"] = ticket_id_start

                                print(ticket_id_start)

                                cluster_id_list = ",".join(map(str, flow["cluster_ids"]))
                                cluster_id_list = f"({cluster_id_list})"
                                select_event_query = f"""
                                SELECT
                                    cluster_id,
                                    MIN(time) AS start_time,
                                    MAX(time) AS end_time,
                                    STRING_AGG(DISTINCT src, ',') AS unique_src,
                                    STRING_AGG(DISTINCT dst, ',') AS unique_dst,
                                    STRING_AGG(tech,';') AS combined_tech,
                                    STRING_AGG(tac,';') AS combined_tac,
                                    STRING_AGG(stage,';') AS combined_stage,
                                    STRING_AGG(alert_id, ',') AS alert_ids
                                FROM
                                    event
                                WHERE cluster_id IN {cluster_id_list}
                                GROUP BY cluster_id
                                ORDER BY cluster_id
                                ;
                                """
                                cursor.execute(select_event_query)

                                print("select event query executed")

                                clusters = cursor.fetchall()
                                clusters = [dict(cluster) for cluster in clusters]

                                print("create cluster dataframe")

                                cluster_df = pd.DataFrame(clusters)

                                print("created cluster dataframe")

                                flow_ticket["start_time"] = cluster_df["start_time"].min()
                                flow_ticket["end_time"] = cluster_df["end_time"].max()

                                start_time = pd.to_datetime(cluster_df["start_time"].min(), unit='s')
                                end_time = pd.to_datetime(cluster_df["end_time"].max(), unit='s')

                                print("get start and end time")

                                global_metric_time_range = (end_time - start_time).total_seconds()
                                global_metric_time_ranges.append(global_metric_time_range)

                                print("get time range")

                                flow_ticket["involved_entities"] = []
                                flow_ticket["involved_techs"] = []
                                flow_ticket["involved_tacs"] = []
                                flow_ticket["involved_stages"] = []

                                print("create flow ticket")

                                cluster_df = cluster_df.apply(process_row, axis=1)
                                for index, row_cluster in cluster_df.iterrows():

                                    flow_ticket["involved_entities"] = list(set(flow_ticket["involved_entities"] + row_cluster["involved_entities"]))
                                    flow_ticket["involved_techs"] = list(set(flow_ticket["involved_techs"] + row_cluster["involved_techs"]))
                                    flow_ticket["involved_tacs"] = list(set(flow_ticket["involved_tacs"] + row_cluster["involved_tacs"]))
                                    flow_ticket["involved_stages"] = list(set(flow_ticket["involved_stages"] + row_cluster["involved_stages"]))
                                    flow_ticket["ids"] = list(set(flow_ticket["ids"] + row_cluster["involved_events"]))

                                print("get global metrics from cluster")
                                flow_ticket["metrics"] = get_global_metrics_from_cluster_sql(cluster_df)

                                for metric_col, mapped_metric_col in zip(metrics_cols, mapped_metric_cols):

                                    if metric_col == "event":
                                        count = float(len(flow_ticket[mapped_metric_col]))
                                        event_count += count
                                        metrics_data[metric_col].append(count)
                                    else:
                                        count = float(len(flow_ticket[mapped_metric_col]))
                                        metrics_data[metric_col].append(count)

                                f.write(json.dumps(flow_ticket) + "\n")

                                flow["ids"] = flow_ticket["ids"]

                                flow.pop("cluster_ids")

                                params = {
                                    "flow_id": flow["flow_id"],
                                    "cluster_prob": flow["cluster_prob"],
                                    "alert_ids": json.dumps(flow["ids"])
                                }
                                insert_flow_output_params.append(params)

                                if MAP_CEF_TO_INTERNAL:
                                    flow["event_ids"] = [id[-36:] for id in flow_ticket["ids"]]

                            print(f"{func}: Insert flow output to table")
                            cursor.executemany(insert_flow_output_query, insert_flow_output_params)

                            print(f"{func}: Save ticket id lookup file")
                            ticket_id_json["last_ticket_id"] = ticket_id_start
                            json.dump(ticket_id_json, open(TEMP_TICKET_ID_LOOKUP_FILE, "w"))
                            save_ticket_id_lookup_file(bucket)
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            # Calculate statistics for each metric
            for metric_col in metrics_cols:
                if metric_col not in global_metrics:
                    global_metrics[metric_col] = {}

                if metrics_data[metric_col]:
                    global_metrics[metric_col]["avg"] = float(pd.Series(metrics_data[metric_col]).mean())
                    global_metrics[metric_col]["min"] = float(pd.Series(metrics_data[metric_col]).min())
                    global_metrics[metric_col]["max"] = float(pd.Series(metrics_data[metric_col]).max())
                    global_metrics[metric_col]["median"] = float(pd.Series(metrics_data[metric_col]).median())
                    global_metrics[metric_col]["q1"] = float(pd.Series(metrics_data[metric_col]).quantile(0.25))
                    global_metrics[metric_col]["q3"] = float(pd.Series(metrics_data[metric_col]).quantile(0.75))
                    if metric_col == "event":
                        global_metrics[metric_col]["count"] = event_count

            # Calculate statistics for time range
            if global_metric_time_ranges:
                global_metrics["time_range"] = {
                    "avg": float(pd.Series(global_metric_time_ranges).mean()),
                    "min": float(pd.Series(global_metric_time_ranges).min()),
                    "max": float(pd.Series(global_metric_time_ranges).max()),
                    "median": float(pd.Series(global_metric_time_ranges).median()),
                    "q1": float(pd.Series(global_metric_time_ranges).quantile(0.25)),
                    "q3": float(pd.Series(global_metric_time_ranges).quantile(0.75)),
                }

            json.dump(global_metrics, open(TEMP_FLOW_GLOBAL_METRICS_FILENAME, "w"))

            print(f"{func}: Save flow output for user")
            json.dump(flow_output_json, open(TEMP_USER_FLOW_OUTPUT_JSON_FILENAME, "w"))

            try:
                print(f"{func}: Upload global metric output file to S3")
                flow_global_metrics_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_global_metrics.json"
                S3_CLIENT.upload_file(TEMP_FLOW_GLOBAL_METRICS_FILENAME, bucket, flow_global_metrics_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{flow_global_metrics_filename}")
                raise e

            try:
                print(f"{func}: Upload flow ticket output file to S3")
                flow_ticket_output_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_ticket_output.json"
                S3_CLIENT.upload_file(TEMP_FLOW_TICKET_OUTPUT_FILENAME, bucket, flow_ticket_output_filename)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{flow_ticket_output_filename}")
                raise e


        def process_row(row):
            combined_tech = row['combined_tech']
            combined_tech = combined_tech.split(";")
            tech_list = []
            for techs in combined_tech:
                tech_list += json.loads(techs)
            tech_list = list(set(tech_list))

            combined_tac = row['combined_tac']
            combined_tac = combined_tac.split(";")
            tac_list = []
            for tacs in combined_tac:
                tac_list += json.loads(tacs)
            tac_list = list(set(tac_list))

            combined_stage = row['combined_stage']
            combined_stage = combined_stage.split(";")
            stage_list = []
            for stage in combined_stage:
                stage_list += json.loads(stage)
            stage_list = list(set(stage_list))

            row["involved_entities"] = list(set(row["unique_src"].split(",") + row["unique_dst"].split(",")))
            row["involved_techs"] = tech_list
            row["involved_tacs"] = tac_list
            row["involved_stages"] = stage_list
            row["involved_events"] = row["alert_ids"].split(",")
            return row


        def get_global_metrics_from_cluster_sql(cluster_df: pd.DataFrame):
            '''
            Get global metrics from cluster
            Args:
                cluster_df: dataframe of cluster output for which metrics is to be calculated
            Returns:
                metrics dictionary
            '''
            func = "get_global_metrics_from_cluster_sql"

            metrics = {}
            metrics_cols = ["tech", "tac", "stage", "event", "entity"]
            mapped_metric_cols = ["involved_techs", "involved_tacs", "involved_stages", "involved_events", "involved_entities"]

            for metric_col, mapped_metric_col in zip(metrics_cols, mapped_metric_cols):
                cluster_df[f'{metric_col}_count'] = cluster_df[mapped_metric_col].apply(lambda x: float(len(x)))

                metrics[metric_col] = {
                    "avg": cluster_df[f"{metric_col}_count"].mean(),
                    "min": cluster_df[f"{metric_col}_count"].min(),
                    "max": cluster_df[f"{metric_col}_count"].max(),
                    "median": cluster_df[f"{metric_col}_count"].median(),
                    "q1": cluster_df[f"{metric_col}_count"].quantile(0.25),
                    "q3": cluster_df[f"{metric_col}_count"].quantile(0.75)
                }

                if metric_col == "event":
                    metrics[metric_col]["count"] = float(cluster_df[f"{metric_col}_count"].sum())

            # time range metric
            cluster_df["start_time"] = pd.to_datetime(cluster_df['start_time'], unit="s")
            cluster_df["end_time"] = pd.to_datetime(cluster_df['end_time'], unit="s")
            cluster_df["time_range"] = (cluster_df["end_time"] - cluster_df["start_time"]).dt.total_seconds()
            metrics["time_range"] = {
                "avg": cluster_df["time_range"].mean(),
                "min": cluster_df["time_range"].min(),
                "max": cluster_df["time_range"].max(),
                "median": cluster_df["time_range"].median(),
                "q1": cluster_df["time_range"].quantile(0.25),
                "q3": cluster_df["time_range"].quantile(0.75)
            }

            # for _, row in cluster_ticket_output_for_flow.iterrows():
            #     unique_other_attributes = row["metrics"].keys()

            return metrics


        def update_queue_lookup(unique_ids):
            '''
            Update queue lookup file
            Args:
                unique_ids: Unique ids generated for the current input file
            '''
            global TEMP_QUEUE_LOOKUP_FILE
            func = "update_queue_lookup"

            queue_lookup_json = json.load(open(TEMP_QUEUE_LOOKUP_FILE, "r"))

            payload_json = {}

            if queue_lookup_json["first"] is None:
                payload_json["first"] = unique_ids[0]

            payload_json["prev"] = unique_ids[-1]

            log_json = {"message": "update queue lookup", "payload": payload_json}
            print(f"{json.dumps(log_json)}")


        def save_ticket_id_lookup_file(bucket):
            '''
            Save ticket id lookup file to S3
            Args:
                bucket: The S3 bucket name
            '''
            global TICKET_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_TICKET_ID_LOOKUP_FILE
            func = "save_ticket_id_lookup_file"
            try:
                print(f"{func}: Upload ticket id lookup file to S3")
                S3_CLIENT.upload_file(TEMP_TICKET_ID_LOOKUP_FILE, bucket, TICKET_ID_OBJECT_KEY)
            except Exception as e:
                print(f"{func}: Failed to save file to S3: {bucket}/{TICKET_ID_OBJECT_KEY}")
                raise e


        def fetch_or_create_ticket_id_lookup_file(bucket):
            '''
            Check if the ticket_id lookup file exists on S3. Create if it does not
            Args:
                bucket: The S3 bucket name
            '''
            global TICKET_ID_OBJECT_KEY
            global S3_CLIENT
            global TEMP_TICKET_ID_LOOKUP_FILE

            func = "fetch_or_create_ticket_id_lookup_file"

            try:
                print(
                    f"{func}: Checking if ticket id lookup file exists on S3 path {bucket}/{TICKET_ID_OBJECT_KEY}")

                S3_CLIENT.head_object(Bucket=bucket, Key=TICKET_ID_OBJECT_KEY)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Ticket id lookup file does not exist on S3. Create empty table")

                internal_id_to_unique_id_json = {"last_ticket_id": 0}
                json.dump(internal_id_to_unique_id_json, open(TEMP_TICKET_ID_LOOKUP_FILE, "w"))

                print(f"{func}: Initiated ticket id")
            else:
                try:
                    print(f"{func}: Download available internal id to unique id lookup file from S3")

                    S3_CLIENT.download_file(bucket, TICKET_ID_OBJECT_KEY, TEMP_TICKET_ID_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {bucket}/{TICKET_ID_OBJECT_KEY}")
                    raise e


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download intermediate files for processing.
            Args:
                bucket: Bucket name
                unique_id: unique id for current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME
            global QUEUE_LOOKUP_FILE
            global TEMP_QUEUE_LOOKUP_FILE
            global NODE_FEATURE_EXISTS_IN_REQUEST
            global TEMP_NODE_FEATURE_INPUT_FILENAME
            global NODE_FEATURE_LOOKUP_FILE
            global TEMP_NODE_FEATURE_LOOKUP_FILE
            global TEMP_QUEUE_FILENAME

            func = "download_intermediate_input_files"

            path_to_intermediate_queue = f"{SCRATCH_DIR}/intermediate/{unique_id}/queue.json"
            S3_CLIENT.download_file(bucket, path_to_intermediate_queue, TEMP_QUEUE_FILENAME)

            try:
                print(f"{func}: Download queue lookup file from S3")
                S3_CLIENT.download_file(bucket, QUEUE_LOOKUP_FILE, TEMP_QUEUE_LOOKUP_FILE)
            except Exception as e:
                print(f"{func}: Failed to download file: {bucket}/{QUEUE_LOOKUP_FILE}")
                raise e

            path_to_cluster_id_list = f"{SCRATCH_DIR}/intermediate/{unique_id}/input_cluster_id_list.json"
            print(f"{func}: Save file to S3: {bucket}/{path_to_cluster_id_list}")

            try:
                S3_CLIENT.download_file(bucket, path_to_cluster_id_list, TEMP_INPUT_CLUSTER_ID_LIST_JSON_FILENAME)
                print(f"{func}: Uploaded file to S3: {bucket}/{path_to_cluster_id_list}")
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {bucket}/{path_to_cluster_id_list}")
                raise e

            try:
                print(f"{func}: Checking if node feautres lookup file exists on S3 path {bucket}/{NODE_FEATURE_LOOKUP_FILE}")
                S3_CLIENT.head_object(Bucket=bucket, Key=NODE_FEATURE_LOOKUP_FILE)
                NODE_FEATURE_EXISTS_IN_REQUEST = True

            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    print(f"{func}: Node feaures file does not exist")
                else:
                    raise e
            else:
                try:
                    S3_CLIENT.download_file(bucket, NODE_FEATURE_LOOKUP_FILE, TEMP_NODE_FEATURE_LOOKUP_FILE)
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download file: {bucket}/{NODE_FEATURE_LOOKUP_FILE}")
                    raise e


        def get_output_object_key(event):
            '''
            Get output object key from input and output s3 path
            Args:
                event: Input to lambda
            Returns:
                (bucket_name, object_key)
            '''
            func = "get_output_object_key"

            input_bucket, input_key = parse_s3_path(event["inputPath"])

            print(f"{func}: Input bucket: {input_bucket}")
            print(f"{func}: Input key: {input_key}")

            output_bucket, output_key = parse_s3_path(event["outputPath"])

            print(f"{func}: Output bucket: {output_bucket}")
            print(f"{func}: Output key: {output_key}")

            if input_bucket is None or input_key is None or output_bucket is None or output_key is None:
                raise ValueError("Failed to get output path of the batch transform job")

            return output_bucket, output_key


        def parse_s3_path(s3_path):
            '''
            Parse an S3 path and extract the bucket name and object key.
            Args:
                s3_path: The S3 path to parse. Should be in the format: s3://bucket-name/path/to/object
            Returns:
                (bucket_name, object_key).
                (None, None) if the path is invalid.
            '''
            if not s3_path.startswith("s3://"):
                return None, None

            # Remove the "s3://" prefix
            path_without_prefix = s3_path[5:]

            # Split the remaining path into parts
            parts = path_without_prefix.split('/', 1)

            # If there's only one part, it means there's no object key
            if len(parts) == 1:
                return parts[0], None

            # If there are two parts, the first is the bucket name, the second is the object key
            return parts[0], parts[1]


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          campaign_map: !Ref ClusterOrFlowMapToCampaign
          flow_input_window_size: !Ref FlowInputMaxClusters
          map_cef_to_internal: 'true'
          skip_single_alert: 'true'
          db_connection_string: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/postgres
  processFlowLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${processFlow}
  processFlowLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref processFlow
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  updateLookupTable:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}_update_lookup_table
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: updateLookupTable
      InlineCode: |
        '''
        Update technique lookup table
        Input: Row to be added to lookup table
        '''

        import os
        import json
        import glob
        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        TECHNIQUE_LOOKUP_OBJECT = os.getenv("technique_lookup_object")

        TECHNIQUE_LOOKUP_LIMIT = os.getenv("technique_lookup_limit")

        missing_variables = []

        if TECHNIQUE_LOOKUP_OBJECT is None:
            missing_variables.append("technique_lookup_object")
        if TECHNIQUE_LOOKUP_LIMIT is None:
            missing_variables.append("technique_lookup_limit")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        TECHNIQUE_LOOKUP_LIMIT = int(TECHNIQUE_LOOKUP_LIMIT)

        S3_CLIENT = boto3.client("s3")

        TEMP_INPUT_FILENAME = "/tmp/classification_response.csv"

        TEMP_NEW_LOOKUP_TABLE_FILENAME = "/tmp/new_lookup_table.csv"

        TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE = "/tmp/technique_lookup.csv"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global S3_CLIENT
            global TEMP_INPUT_FILENAME
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE

            func = "lambda_handler"

            print(f"{func}: Received event: {json.dumps(event)}")

            bucket = event['Records'][0]['s3']['bucket']['name']
            input_filename = event['Records'][0]['s3']['object']['key']

            clear_temp_dir()

            print(f"{func}: Download file to process from S3 bucket: {bucket}/{input_filename}")

            try:
                S3_CLIENT.download_file(bucket, input_filename, TEMP_INPUT_FILENAME)
                print(f"{func}: File to process download completed")
            except Exception as e:
                print(f"{func}: Failed to download file to process")
                raise e

            fetch_or_create_technique_classification_lookup_file(bucket)

            print(f"{func}: Update lookup table")

            update_lookup_table(bucket)
            clean_up(bucket, input_filename)

            print(f"{func}: Lookup table updated.")


        def update_lookup_table(bucket):
            '''
            Update lookup table
            Args:
                bucket: Bucket name
            '''

            global TEMP_INPUT_FILENAME
            global TEMP_NEW_LOOKUP_TABLE_FILENAME
            global S3_CLIENT
            global TECHNIQUE_LOOKUP_OBJECT
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE
            global TECHNIQUE_LOOKUP_LIMIT

            func = "update_lookup_table"

            technique_classification_lookup_df = pd.read_csv(TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE)

            classification_response_df = pd.read_csv(TEMP_INPUT_FILENAME)

            if len(classification_response_df) >= TECHNIQUE_LOOKUP_LIMIT:
                new_technique_classification_lookup_df = classification_response_df.tail(TECHNIQUE_LOOKUP_LIMIT)
                new_technique_classification_lookup_df.to_csv(TEMP_NEW_LOOKUP_TABLE_FILENAME, index=False)

            else:
                technique_classification_lookup_df = technique_classification_lookup_df.tail(TECHNIQUE_LOOKUP_LIMIT - len(classification_response_df))
                new_technique_classification_lookup_df = pd.concat([technique_classification_lookup_df, classification_response_df])
                new_technique_classification_lookup_df_no_dupe = new_technique_classification_lookup_df.drop_duplicates(subset=["alerts"], keep="last")

                new_technique_classification_lookup_df_no_dupe.to_csv(TEMP_NEW_LOOKUP_TABLE_FILENAME, index=False)

            try:
                S3_CLIENT.upload_file(TEMP_NEW_LOOKUP_TABLE_FILENAME, bucket, TECHNIQUE_LOOKUP_OBJECT)
            except Exception as e:
                print(f"{func}: Failed to save lookup table to S3: {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")
                raise e


        def fetch_or_create_technique_classification_lookup_file(bucket):
            '''
            Fetch or create lookup file
            Args:
                bucket: Bucket name
            '''

            global TECHNIQUE_LOOKUP_OBJECT
            global S3_CLIENT
            global TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE

            func = "fetch_or_create_technique_classification_lookup_file"
            try:
                print(
                    f"{func}: Checking if technique classification lookup file for alerts exists on S3 path {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")

                S3_CLIENT.head_object(Bucket=bucket, Key=TECHNIQUE_LOOKUP_OBJECT)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: Technique classification lookup file for alerts does not exist on S3. Create empty table")

                technique_classification_lookup_df = pd.DataFrame(columns=["alerts", "techniques"])
                technique_classification_lookup_df.to_csv(TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE, index=False)

                print(f"{func}: Empty table created")
            else:
                try:
                    print(f"{func}: Download available technique classification lookup file for alerts from S3")

                    S3_CLIENT.download_file(bucket, TECHNIQUE_LOOKUP_OBJECT, TEMP_TECHNIQUE_CLASSIFICATION_LOOKUP_FILE)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup table from S3: {bucket}/{TECHNIQUE_LOOKUP_OBJECT}")
                    raise e


        def clean_up(bucket, input_filename):
            '''
            Delete input file
            Args:
                bucket: Bucket name
                input_filename: S3 object to delete
            '''
            global S3_CLIENT

            func = "clean_up"

            try:
                print(f"{func}: Delete message payload from S3")
                S3_CLIENT.delete_object(Bucket=bucket, Key=input_filename)
                print(f"{func}: Message payload deleted")
            except Exception as e:
                print(f"{func}: Failed to delete message payload from S3. You can manually delete the object: {bucket}/{input_filename}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      ReservedConcurrentExecutions: 1
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          technique_lookup_object: scratch/lambda/data.csv
          technique_lookup_limit: !Ref TechLookupLimit
      Events:
        S3ObjectCreated:
          Type: S3
          Properties:
            Bucket: !Ref Bucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                - Name: prefix
                  Value: scratch/queue/
                - Name: suffix
                  Value: .csv
  updateLookupTableLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${updateLookupTable}
  updateLookupTableLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref updateLookupTable
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  createCampaign:
    Type: AWS::Serverless::Function
    DependsOn:
    - UILoadBalancer
    - DbLoadBalancer
    Properties:
      FunctionName: !Sub ${AWS::StackName}_create_campaign
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: createCampaign
      InlineCode: |
        '''
        This lambda function will create campaigns, events on the UI.

        Input: cluster_ticket_output.json
        '''

        import gzip
        import json
        import os
        import time
        import urllib
        import glob
        import sqlite3
        from contextlib import closing

        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd
        import requests

        import sys
        import subprocess

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        subprocess.call('pip install psycopg2-binary==2.9.9 sqlalchemy==2.0.32 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import psycopg2
        from psycopg2 import sql
        from psycopg2.extras import DictCursor

        UI_LB_URL = os.getenv("ui_lb_url")

        EVENT_THRESHOLD = os.getenv("event_threshold")

        TAC_THRESHOLD = os.getenv("tac_threshold")

        UI_USERNAME = os.getenv("ui_username")

        UI_PASSWORD = os.getenv("ui_password")

        CAMPAIGN_MAP = os.getenv("campaign_map")

        MAX_CLUSTER_COUNT = os.getenv("max_cluster_count")

        MAX_FLOW_COUNT = os.getenv("max_flow_count")

        DB_CONNECTION_STRING = os.getenv("db_connection_string")

        missing_variables = []
        if UI_LB_URL is None:
            missing_variables.append("ui_lb_url")
        if EVENT_THRESHOLD is None:
            missing_variables.append("event_threshold")
        if TAC_THRESHOLD is None:
            missing_variables.append("tac_threshold")
        if UI_USERNAME is None:
            missing_variables.append("ui_username")
        if UI_PASSWORD is None:
            missing_variables.append("ui_password")
        if CAMPAIGN_MAP is None:
            missing_variables.append("campaign_map")
        if MAX_CLUSTER_COUNT is None:
            missing_variables.append("max_cluster_count")
        if MAX_FLOW_COUNT is None:
            missing_variables.append("max_flow_count")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        CAMPAIGN_MAP = CAMPAIGN_MAP.lower()
        if CAMPAIGN_MAP not in ["flow", "cluster"]:
            raise ValueError(f"Valid values for 'campaign_map' environemnt variable are 'flow', 'cluster'. Entered value is: '{CAMPAIGN_MAP}'")

        EVENT_THRESHOLD = int(EVENT_THRESHOLD)

        TAC_THRESHOLD = int(TAC_THRESHOLD)

        MAX_CLUSTER_COUNT = int(MAX_CLUSTER_COUNT)

        MAX_FLOW_COUNT = int(MAX_FLOW_COUNT)

        HOST = f"http://{UI_LB_URL}:8000/"

        S3_CLIENT = boto3.client("s3")

        CONTENT_TYPE = "application/json"

        SCRATCH_DIR = "scratch"

        TEMP_CLUSTER_TICKET_OUTPUT = "/tmp/cluster_ticket_output.json"
        TEMP_FLOW_TICKET_OUTPUT = "/tmp/flow_ticket_output.json"

        FLOW_TICKET_OUTPUT_EXIST = False

        TEMP_FLOW_GLOBAL_METRICS_FILENAME = "/tmp/flow_global_metrics.json"
        TEMP_GLOBAL_METRICS_FILENAME = "/tmp/global_metrics.json"
        TEMP_GLOBAL_FEATURE_FILENAME = "/tmp/global_feature.json"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global TEMP_CLUSTER_TICKET_OUTPUT
            global TEMP_FLOW_TICKET_OUTPUT
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_GLOBAL_METRICS_FILENAME
            global TEMP_GLOBAL_FEATURE_FILENAME
            global FLOW_TICKET_OUTPUT_EXIST

            func = "lambda_handler"

            FLOW_TICKET_OUTPUT_EXIST = False

            # bucket = event['Records'][0]['s3']['bucket']['name']
            # input_file = event['Records'][0]['s3']['object']['key']

            bucket = event["bucket"]
            try:
                input_file = event["input_path"]

                clear_temp_dir()
                unique_id = input_file.split("/")[-2]

                print(f"{func}: input file: {bucket}/{input_file}")

                temp_input_file = TEMP_CLUSTER_TICKET_OUTPUT

                try:
                    print(f"{func}: Download the input file")
                    S3_CLIENT.download_file(bucket, input_file, temp_input_file)
                except Exception as e:
                    print(f"{func}: Failed to download file from S3: {bucket}/{input_file}")
                    raise e

                try:
                    print(f"{func}: Download intermediate files from S3 bucket")
                    download_intermediate_input_files(bucket, unique_id)
                except Exception as e:
                    print(f"{func}: Failed to download files to process")
                    raise e

                print("Starting campaign creation")
                create_campaigns()
                print(f"{func}: Complete campaign creation")

            except Exception as ex:
                print(f"{func}: Exception occurred while running lambda function. Uploading error file to S3.")
                error_message = f"Error occurred in lambda function: {context.function_name}. More details can be found in CloudWatch Logs for this lambda function. The exception message is: {ex}"
                upload_error_to_s3(bucket, error_message)
                raise ex

            lambda_response = {"response": "completed campaign"}
            return lambda_response


        def create_campaigns():
            '''
            Create events, campaigns. Where campaigns could be a flow or a cluster.
            '''
            global TEMP_CLUSTER_TICKET_OUTPUT
            global CAMPAIGN_MAP
            global EVENT_THRESHOLD
            global HOST
            global FLOW_TICKET_OUTPUT_EXIST
            func = "create_campaign"

            retry_limit = 3
            retry_delay = 5

            print(f"{func}: Get UI session")
            for attempt in range(retry_limit):
                try:
                    ui_cookies, ui_headers, ui_session = get_ui_session()
                    break
                except Exception as e:
                    print(f"Attempt {attempt + 1} failed: {e}")
                    if attempt < retry_limit - 1:
                        print(f"Retrying in {retry_delay} seconds")
                        time.sleep(retry_delay)
                    else:
                        print("Max retries reached. Failed to get session.")
                        raise

            create_global_metrics(ui_cookies, ui_headers, ui_session)

            create_global_features(ui_cookies, ui_headers, ui_session)

            create_campaign_for_cluster(ui_cookies, ui_headers, ui_session)

            if FLOW_TICKET_OUTPUT_EXIST:
                create_campaign_for_flow(ui_cookies, ui_headers, ui_session)


        def create_global_metrics(ui_cookies, ui_headers, ui_session):
            '''
            Update or upload global metrics to UI
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_GLOBAL_METRICS_FILENAME
            global TEMP_FLOW_GLOBAL_METRICS_FILENAME
            global FLOW_TICKET_OUTPUT_EXIST
            func = "create_global_metrics"

            global_metrics_json = json.load(open(TEMP_GLOBAL_METRICS_FILENAME))
            payload = {"metric": global_metrics_json}

            try:
                response_from_ui = ui_session.post(f"{HOST}api/v2/globalmetric/", json=payload, cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)
                response_from_ui.raise_for_status()
                print(f"{func}: Global metrics added successful")
            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e

            if FLOW_TICKET_OUTPUT_EXIST:
                global_metrics_json = json.load(open(TEMP_FLOW_GLOBAL_METRICS_FILENAME))
                payload = {"metric": global_metrics_json}

                try:
                    response_from_ui = ui_session.post(f"{HOST}api/v2/flow_globalmetric/", json=payload, cookies=ui_cookies, headers=ui_headers)
                    print(f"{func}: Got response", response_from_ui)
                    response_from_ui.raise_for_status()
                    print(f"{func}: Flow Global metrics added successful")
                except requests.exceptions.RequestException as req_err:
                    print("Request error occurred.")
                    raise req_err
                except Exception as e:
                    print(f"{func}: An unexpected error occurred")
                    raise e


        def create_global_features(ui_cookies, ui_headers, ui_session):
            '''
            Update or upload global metrics to UI
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_GLOBAL_FEATURE_FILENAME
            func = "create_global_features"

            global_feature_json = json.load(open(TEMP_GLOBAL_FEATURE_FILENAME))

            if len(global_feature_json["event_feature"]) < 1 and len(global_feature_json["node_feature"]) < 1:
                print(f"{func}: No new global features to add. Skip adding global features")
                return

            try:
                response_from_ui = ui_session.post(f"{HOST}api/v2/globalfeature/bulk_create/", json=global_feature_json, cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)
                response_from_ui.raise_for_status()
                print(f"{func}: Global features added successful")
            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e


        def create_campaign_for_cluster(ui_cookies, ui_headers, ui_session):
            '''
            Create campaign for cluster
            Args:
                ui_cookies: UI cookies
                ui_headers: UI header
                ui_session: UI session
            '''
            global TEMP_CLUSTER_TICKET_OUTPUT
            global CAMPAIGN_MAP
            global EVENT_THRESHOLD
            global HOST
            func = "create_campaign_for_cluster"

            CAMPAIGN_MAP = "cluster"
            api_to_call = "campaign"
            limit = 1000

            print(f"{func}: Get campaign count")
            campaign_count = get_campaign_count(ui_cookies, ui_headers, ui_session, api_to_call)

            to_remove_campaign_id_list = get_campaign_to_remove(ui_cookies, ui_headers, ui_session, campaign_count, api_to_call, limit)

            update_cluster_output_campaign_id = """
            UPDATE cluster_output
            SET campaign_id = %(campaign_id)s
            WHERE cluster_id = %(cluster_id)s
            ;
            """

            update_event_added_to_ui = """
            UPDATE event
            SET cluster_added_to_ui = 1
            WHERE cluster_id = %(cluster_id)s
            ;
            """

            delete_cluster_output = """
            DELETE FROM cluster_output
            WHERE campaign_id = %(campaign_id)s
            ;
            """

            new_campaign_count = 0
            try:

                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        with open(TEMP_CLUSTER_TICKET_OUTPUT, "r") as f:
                            for line in f:
                                row_cluster = json.loads(line)

                                print(f"{func}: Evaluating for {CAMPAIGN_MAP}: {row_cluster['cluster_id']}")

                                select_event_output = f"""
                                SELECT
                                    co.cluster_id,
                                    co.campaign_id,
                                    e.alert_id,
                                    e.src,
                                    e.dst,
                                    e.time,
                                    e.name,
                                    e.tech,
                                    e.tac,
                                    e.stage,
                                    e.cluster_added_to_ui,
                                    e.raw_data
                                FROM cluster_output co, event e
                                WHERE co.cluster_id = {row_cluster['cluster_id']}
                                AND e.cluster_added_to_ui = 0
                                ORDER BY e.alert_id
                                ;
                                """
                                cursor.execute(select_event_output)

                                result = cursor.fetchone()
                                if result is None:
                                    continue

                                row = dict(result)

                                involved_events = []
                                original_input_events = []
                                while result is not None:
                                    event_row = dict(result)

                                    involved_event_dict = {
                                        "id": event_row["alert_id"],
                                        "src": event_row["src"],
                                        "dst": event_row["dst"],
                                        "time": event_row["time"],
                                        "name": event_row["name"],
                                        "tech": json.loads(event_row["tech"]),
                                        "tac": json.loads(event_row["tac"]),
                                        "stage": json.loads(event_row["stage"]),
                                    }
                                    involved_events.append(involved_event_dict)
                                    original_input_events.append(json.loads(row["raw_data"]))

                                    result = cursor.fetchone()

                                involved_events_df = pd.DataFrame(involved_events)
                                involved_events_df['time_df'] = pd.to_datetime(involved_events_df['time'], unit="s")
                                involved_events_df['time_df'] = involved_events_df['time_df'].dt.strftime('%Y-%m-%d %H:%M:%S')

                                events = create_events_list_for_ui(involved_events_df, original_input_events)

                                if len(events) < EVENT_THRESHOLD:
                                    print(
                                        f"{func}: Length of list of events is less than threshold set. Skip creation of campaign. length: {len(events)}")
                                    continue

                                files, payload = create_payload_to_create_campaign_and_events_in_ui(events, row_cluster['cluster_id'])

                                if row["campaign_id"] is None:
                                    if (campaign_count + new_campaign_count) >= MAX_CLUSTER_COUNT:
                                        while (campaign_count + new_campaign_count) >= MAX_CLUSTER_COUNT:
                                            print(f"{func}: Current cluster count on UI {(campaign_count + new_campaign_count)} is greater than max cluster count {MAX_CLUSTER_COUNT}.")

                                            if len(to_remove_campaign_id_list) < 1:
                                                print(f"{func}: The campaign list to remove is empty, fetch new list")
                                                campaign_count = get_campaign_count(ui_cookies, ui_headers, ui_session, api_to_call)
                                                new_campaign_count = 0
                                                to_remove_campaign_id_list = get_campaign_to_remove(ui_cookies, ui_headers, ui_session, campaign_count, api_to_call, limit)

                                            to_remove_campaign_id = to_remove_campaign_id_list[-1]

                                            print(f"{func}: Delete campaign from UI")
                                            delete_campaign(ui_cookies, ui_headers, ui_session, api_to_call, to_remove_campaign_id)

                                            print(f"{func}: Delete cluster from sqlite")
                                            delete_cluster_output_params = {"campaign_id": to_remove_campaign_id}
                                            cursor.execute(delete_cluster_output, delete_cluster_output_params)

                                            campaign_count -= 1
                                            to_remove_campaign_id_list.pop()

                                    print(f"{func}: Create campaign")
                                    try:
                                        response_from_ui = ui_session.post(f"{HOST}api/v2/campaign/", data=payload, files=files, cookies=ui_cookies, headers=ui_headers)

                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                        print(f"{func}: Campaign creation successful")
                                        response_from_ui = json.loads(response_from_ui.content)
                                        campaign_id = response_from_ui["id"]
                                        print("Campaign id: ", campaign_id)

                                        update_cluster_output_campaign_id_param = {
                                            "cluster_id": row_cluster['cluster_id'],
                                            "campaign_id": campaign_id
                                        }
                                        cursor.execute(update_cluster_output_campaign_id, update_cluster_output_campaign_id_param)

                                    except Exception as e:
                                        print(f"{func}: Failed to create campaign")
                                        raise e
                                    new_campaign_count += 1
                                else:
                                    print(f"{func}: Update campaign: {row['campaign_id']}")
                                    try:
                                        response_from_ui = ui_session.put(f"{HOST}api/v2/campaign/{row['campaign_id']}/", data=payload, files=files, cookies=ui_cookies, headers=ui_headers)
                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                    except Exception as e:
                                        print(f"{func}: Failed to update campaign")
                                        raise e

                                # update only those events that were added per cluster. in case the cluster has 1 event its not added. but it could get more events in next batch.
                                update_event_added_to_ui_param = {"cluster_id": row_cluster['cluster_id']}
                                cursor.execute(update_event_added_to_ui, update_event_added_to_ui_param)

                        conn.commit()

            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e


        def create_campaign_for_flow(ui_cookies, ui_headers, ui_session):
            '''
            Create campaign for flow
            Args:
                ui_cookies: UI cookies
                ui_headers: UI header
                ui_session: UI session
            '''
            global TEMP_FLOW_TICKET_OUTPUT
            global CAMPAIGN_MAP
            global EVENT_THRESHOLD
            global HOST
            func = "create_campaign_for_flow"

            CAMPAIGN_MAP = "flow"
            api_to_call = "flow"
            limit = 1000

            print(f"{func}: Get campaign count")
            campaign_count = get_campaign_count(ui_cookies, ui_headers, ui_session, api_to_call)

            to_remove_campaign_id_list = get_campaign_to_remove(ui_cookies, ui_headers, ui_session, campaign_count, api_to_call, limit)

            update_flow_output_campaign_id = """
            UPDATE flow_output
            SET campaign_id = %(campaign_id)s
            WHERE flow_id = %(flow_id)s
            ;
            """

            delete_flow_output = """
            DELETE FROM flow_output
            WHERE campaign_id = %(campaign_id)s
            ;
            """

            new_campaign_count = 0

            try:

                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        with open(TEMP_FLOW_TICKET_OUTPUT, "r") as f:
                            for line in f:
                                row_flow = json.loads(line)

                                print(f"{func}: Evaluating for flow: {row_flow['flow_id']}")

                                flow_alert_ids = "','".join(row_flow["ids"])
                                flow_alert_ids = f"('{flow_alert_ids}')"

                                # filter out involved events that are already added to UI

                                select_flow_output = f"""
                                SELECT
                                    fo.flow_id,
                                    fo.campaign_id,
                                    e.alert_id,
                                    e.src,
                                    e.dst,
                                    e.time,
                                    e.name,
                                    e.tech,
                                    e.tac,
                                    e.stage,
                                    e.flow_added_to_ui,
                                    e.raw_data
                                FROM flow_output fo, event e
                                WHERE fo.flow_id = {row_flow['flow_id']}
                                AND e.alert_id IN {flow_alert_ids}
                                ORDER BY e.alert_id
                                ;
                                """
                                cursor.execute(select_flow_output)

                                result = cursor.fetchone()
                                if result is None:
                                    continue

                                row = dict(result)
                                campaign_id_sql = row["campaign_id"]

                                involved_events = []
                                original_input_events = []
                                update_events = []
                                while result is not None:
                                    row = dict(result)

                                    if row["flow_added_to_ui"] == 1:
                                        update_events.append(row["alert_id"])

                                    else:
                                        involved_event_dict = {
                                            "id": row["alert_id"],
                                            "src": row["src"],
                                            "dst": row["dst"],
                                            "time": row["time"],
                                            "name": row["name"],
                                            "tech": json.loads(row["tech"]),
                                            "tac": json.loads(row["tac"]),
                                            "stage": json.loads(row["stage"]),
                                        }
                                        involved_events.append(involved_event_dict)
                                        original_input_events.append(json.loads(row["raw_data"]))

                                    result = cursor.fetchone()

                                involved_events_df = pd.DataFrame(involved_events)

                                involved_events_df['time_df'] = pd.to_datetime(involved_events_df['time'], unit="s")
                                involved_events_df['time_df'] = involved_events_df['time_df'].dt.strftime('%Y-%m-%d %H:%M:%S')

                                events = create_events_list_for_ui(involved_events_df, original_input_events)

                                if len(events) < EVENT_THRESHOLD:
                                    print(
                                        f"{func}: Length of list of events is less than threshold set. Skip creation of campaign. length: {len(events)}")
                                    continue

                                files, payload = create_payload_to_create_campaign_and_events_in_ui(events, row_flow['flow_id'])

                                add_events_to_campaign_id = campaign_id_sql

                                if campaign_id_sql is None:
                                    if (campaign_count + new_campaign_count) >= MAX_FLOW_COUNT:
                                        while (campaign_count + new_campaign_count) >= MAX_FLOW_COUNT:
                                            print(f"{func}: Current flow count on UI {(campaign_count + new_campaign_count)} is greater than max flow count {MAX_FLOW_COUNT}.")

                                            if len(to_remove_campaign_id_list) < 1:
                                                print(f"{func}: The campaign list to remove is empty, fetch new list")
                                                campaign_count = get_campaign_count(ui_cookies, ui_headers, ui_session, api_to_call)
                                                new_campaign_count = 0
                                                to_remove_campaign_id_list = get_campaign_to_remove(ui_cookies, ui_headers, ui_session, campaign_count, api_to_call, limit)

                                            to_remove_campaign_id = to_remove_campaign_id_list[-1]

                                            print(f"{func}: Delete campaign from UI: {to_remove_campaign_id}")
                                            delete_campaign(ui_cookies, ui_headers, ui_session, api_to_call, to_remove_campaign_id)

                                            print(f"{func}: Delete flow from sqlite")
                                            delete_cluster_output_params = {"campaign_id": to_remove_campaign_id}
                                            cursor.execute(delete_flow_output, delete_cluster_output_params)

                                            campaign_count -= 1
                                            to_remove_campaign_id_list.pop()
                                    print(f"{func}: Create campaign")
                                    try:
                                        response_from_ui = ui_session.post(f"{HOST}api/v2/flow/", data=payload, files=files, cookies=ui_cookies, headers=ui_headers)

                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                        print(f"{func}: Campaign creation successful")
                                        response_from_ui = json.loads(response_from_ui.content)
                                        campaign_id = response_from_ui["id"]
                                        print("Campaign id: ", campaign_id)

                                        add_events_to_campaign_id = campaign_id

                                        update_cluster_output_campaign_id_param = {
                                            "flow_id": row_flow['flow_id'],
                                            "campaign_id": campaign_id
                                        }
                                        cursor.execute(update_flow_output_campaign_id, update_cluster_output_campaign_id_param)

                                    except Exception as e:
                                        print(f"{func}: Failed to create campaign")
                                        raise e
                                    new_campaign_count += 1
                                else:
                                    print(f"{func}: Update campaign: {campaign_id_sql}")
                                    try:
                                        response_from_ui = ui_session.put(f"{HOST}api/v2/flow/{campaign_id_sql}/", data=payload, files=files, cookies=ui_cookies, headers=ui_headers)
                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                    except Exception as e:
                                        print(f"{func}: Failed to update campaign")
                                        raise e

                                if len(update_events) > 0:
                                    payload = {"alert_ids": update_events}

                                    print(f"{func}: Update campaign and add events: {add_events_to_campaign_id}")
                                    try:
                                        response_from_ui = ui_session.put(f"{HOST}api/v2/flow/{add_events_to_campaign_id}/event/bulk_update_event_flow/",
                                                                          json=payload, cookies=ui_cookies,
                                                                          headers={**ui_headers, "Content-Type": "application/json"})
                                        print(f"{func}: Got response", response_from_ui)
                                        response_from_ui.raise_for_status()

                                    except Exception as e:
                                        print(f"{func}: Failed to update campaign")
                                        raise e

                                # update only those events that were added per flow. in case the flow has 1 event its not added. but it could get more events in next batch.
                                update_event_added_to_ui = f"""
                                UPDATE event
                                SET flow_added_to_ui = 1
                                WHERE alert_id IN {flow_alert_ids}
                                ;
                                """
                                cursor.execute(update_event_added_to_ui)

                        conn.commit()

            except requests.exceptions.RequestException as req_err:
                print("Request error occurred.")
                raise req_err
            except Exception as e:
                print(f"{func}: An unexpected error occurred")
                raise e


        def create_payload_to_create_campaign_and_events_in_ui(events, campaign):
            '''
            Create payload for creating campaign and event in UI
            Args:
                events: list of events
                campaign: campaign name
            Return:
                files: The zip file that contains events data
                payload: JSON object to create campaign
            '''
            global CAMPAIGN_MAP

            req_gzip_filename = f"/tmp/req.gzip"

            req = json.dumps(events)
            req = gzip.compress(bytes(req, "utf-8"))
            open(req_gzip_filename, "wb").write(req)
            payload = {"name": f"{CAMPAIGN_MAP}-{campaign}", "description": "automatically created alerts", "auto_extract_iocs": "true"}
            files = [('details', ('req.gzip', open(req_gzip_filename, 'rb'), 'application/octet-stream'))]

            return files, payload


        def create_events_list_for_ui(involved_events_df: pd.DataFrame, original_input_events):
            '''
            Create list of events for a campaign
            Args:
                involved_events_df: pandas dataframe of involved events
                original_input_events: list of original input events
            Return:
                events: List of events for UI
            '''
            global TAC_THRESHOLD
            events = []

            for (_, row), raw_data in zip(involved_events_df.iterrows(), original_input_events):
                row_dict = row.to_dict()
                tac = row_dict["tac"]
                if len(tac) < TAC_THRESHOLD:
                    continue
                tech = row_dict["tech"]
                tech = [int(t[1:]) for t in tech]

                mapped_data = {
                    "TTP": {"TACTICS": tac, "TECHNIQUES": tech},
                    "suggested_ttp": tac,
                    "UID": row_dict["id"],
                    "SOURCE_IP": row_dict["src"],
                    "DESTINATION_IP": row_dict["dst"],
                    "HOSTNAME": "",
                    "MESSAGE": row_dict["name"],
                    "TIMESTAMP": row_dict["time_df"],
                }

                for key, value in raw_data.items():
                    raw_data[key] = str(value)
                events.append({"raw_data": raw_data, "mapped_data": mapped_data})

            return events


        def get_campaign_to_remove(ui_cookies, ui_headers, ui_session: requests.Session, campaign_count, cluster_or_flow, limit):
            '''
            Get campaigns that can be removed if it reaches max clusters or flows limit
            Args:
                ui_cookies: UI cookies
                ui_headers: UI header
                ui_session: UI session
                campaign_count: number of campaigns in UI
                cluster_or_flow: cluster or flow API to call
                limit: get n campaigns
            '''
            func = "get_campaign_to_remove"

            print(f"{func}: API to get count: {cluster_or_flow}")

            if campaign_count < 1:
                print(f"{func}: Campaign count: {campaign_count} is less than 1. Return empty list.")
                return []
            if limit < 1:
                print(f"{func}: Limit cannot be less than 1")
                raise ValueError(f"Limit set to {limit}. It needs to be greater than 0")
            if campaign_count < limit:
                print(f"{func}: Count: {campaign_count} is less than limit: {limit}. Reducing limit equal to count")
                limit = campaign_count

            print(f"{func}: Get first {limit} campaign list.")
            campaign_list = get_first_n_campaign_list(ui_cookies, ui_headers, ui_session, cluster_or_flow, campaign_count, limit)

            campaign_id_list = []
            for campaign in campaign_list:
                campaign_id_list.append(campaign["id"])

            return campaign_id_list


        def download_intermediate_input_files(bucket, unique_id):
            '''
            Download intermediate files for processing.
            Args:
                bucket: Bucket name
                unique_id: unique id for current batch
            '''
            global S3_CLIENT
            global SCRATCH_DIR
            global TEMP_GLOBAL_METRICS_FILENAME
            global TEMP_GLOBAL_FEATURE_FILENAME
            global TEMP_FLOW_GLOBAL_METRICS_FILENAME

            func = "download_intermediate_input_files"

            fetch_flow_ticket_output_if_present(bucket, unique_id)

            global_metrics_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/global_metrics.json"
            try:
                print(f"{func}: Download the global metrics file")
                S3_CLIENT.download_file(bucket, global_metrics_filename, TEMP_GLOBAL_METRICS_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{global_metrics_filename}")
                raise e

            if FLOW_TICKET_OUTPUT_EXIST:
                flow_global_metrics_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_global_metrics.json"
                try:
                    print(f"{func}: Download the flow global metrics file")
                    S3_CLIENT.download_file(bucket, flow_global_metrics_filename, TEMP_FLOW_GLOBAL_METRICS_FILENAME)
                except Exception as e:
                    print(f"{func}: Failed to download file from S3: {bucket}/{flow_global_metrics_filename}")
                    raise e

            print(f"{func}: Download global feature file")
            global_feature_filename = f"{SCRATCH_DIR}/intermediate/{unique_id}/global_feature.json"
            try:
                print(f"{func}: Download the global feature file")
                S3_CLIENT.download_file(bucket, global_feature_filename, TEMP_GLOBAL_FEATURE_FILENAME)
            except Exception as e:
                print(f"{func}: Failed to download file from S3: {bucket}/{global_feature_filename}")
                raise e


        def fetch_flow_ticket_output_if_present(bucket, unique_id):
            '''
            Fetch flow ticket output if present
            Args:
                bucket: Bucket name
            '''
            global S3_CLIENT
            global FLOW_TICKET_OUTPUT_EXIST
            global TEMP_FLOW_TICKET_OUTPUT

            func = "fetch_flow_ticket_output_if_present"

            flow_ticket_output_file = f"{SCRATCH_DIR}/intermediate/{unique_id}/flow_ticket_output.json"

            try:
                print(
                    f"{func}: Checking if flow ticket output exists on S3 path {bucket}/{flow_ticket_output_file}")

                S3_CLIENT.head_object(Bucket=bucket, Key=flow_ticket_output_file)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e
                FLOW_TICKET_OUTPUT_EXIST = False
                print(f"{func}: Flow ticket output does not exist on S3")
            else:
                try:
                    print(f"{func}: Download available flow ticket output from S3")

                    S3_CLIENT.download_file(bucket, flow_ticket_output_file, TEMP_FLOW_TICKET_OUTPUT)
                    FLOW_TICKET_OUTPUT_EXIST = True
                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download flow ticket output from S3: {bucket}/{flow_ticket_output_file}")
                    raise e


        def get_campaign_count(ui_cookies, ui_headers, ui_session: requests.Session, cluster_or_flow):
            '''
            Get campaign count for cluster or flow
            Args:
                ui_cookies: UI cookies
                ui_headers: UI header
                ui_session: UI session
                cluster_or_flow: cluster or flow API to call
            Returns:
                campaign count of cluster of flow
            '''
            global HOST
            func = "get_campaign_count"

            print(f"{func}: API to get count: {cluster_or_flow}")

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/{cluster_or_flow}/", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)
                response_from_ui.raise_for_status()

                response_from_ui = json.loads(response_from_ui.content)

                campaign_count = response_from_ui["count"]
                print(f"{func}: Campaign count: {campaign_count}")

                return campaign_count
            except Exception as e:
                print(f"{func}: Failed to get campaign count")
                raise e


        def get_first_n_campaign_list(ui_cookies, ui_headers, ui_session: requests.Session, cluster_or_flow, count, limit):
            '''
            Get campaign count for cluster or flow
            Args:
                ui_cookies: UI cookies
                ui_headers: UI header
                ui_session: UI session
                cluster_or_flow: cluster or flow API to call
                count: count of campaigns
                limit: number of first n clusters to get
            Returns:
                campaign count of cluster of flow
            '''
            global HOST
            func = "get_first_n_campaign_list"

            offset = count - limit

            params = {'limit': limit, 'offset': offset}

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/{cluster_or_flow}/", cookies=ui_cookies, headers=ui_headers, params=params)
                print(f"{func}: Got response", response_from_ui)
                response_from_ui.raise_for_status()

                response_from_ui = json.loads(response_from_ui.content)

                campaign_list = response_from_ui["results"]
                print(f"{func}: Campaign list length: {len(campaign_list)}")

                return campaign_list
            except Exception as e:
                print(f"{func}: Failed to get campaign count")
                raise e


        def delete_campaign(ui_cookies, ui_headers, ui_session: requests.Session, cluster_or_flow, campaign_id):
            '''
            Get campaign count for cluster or flow
            Args:
                ui_cookies: UI cookies
                ui_headers: UI header
                ui_session: UI session
                cluster_or_flow: cluster or flow API to call
                campaign_id: Campaign id to delete
            Returns:
                campaign count of cluster of flow
            '''
            global HOST
            func = "delete_campaign"

            print(f"{func}: API to get count: {cluster_or_flow}")

            try:
                response_from_ui = ui_session.delete(f"{HOST}api/v2/{cluster_or_flow}/{campaign_id}/", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)
                response_from_ui.raise_for_status()

                print(f"{func}: Campaign deleted")
            except Exception as e:
                print(f"{func}: Failed to delte campaign")
                raise e


        def get_ui_session():
            '''
            Get UI session and cookies
            Returns:
                cookies: Cookies for the connection to UI
                headers: Headers for the connection to UI
                s: session for the connection to UI
            '''
            global HOST
            global UI_PASSWORD
            global UI_USERNAME

            try:
                s = requests.Session()
                response = s.get(HOST + "login")
                response.raise_for_status()

                cookies = s.cookies.get_dict()
                csrf = cookies.get("csrftoken")
                if not csrf:
                    raise ValueError("CSRF token not found in cookies")

                headers = {"X-CSRFToken": csrf, "Accept": "application/json"}

                response = s.post(f"{HOST}login/?next",
                                  data={"username": UI_USERNAME, "password": UI_PASSWORD, "csrfmiddlewaretoken": csrf})

                if response.status_code != 404 and not response.url.endswith("/accounts/profile/"):
                    response.raise_for_status()

                print("login successful")
                return cookies, headers, s

            except requests.exceptions.RequestException as e:
                print("Request error occurred.")
                raise e
            except Exception as e:
                print("An unexpected error occurred.")
                raise e


        def upload_error_to_s3(bucket, error_message):
            '''
            Upload error message to S3
            Args:
                error_message: The error message to upload
                unique_id: Unique ID for identifying the error log
            '''
            global S3_CLIENT
            error_log_key = "output/error_log.txt"
            try:
                S3_CLIENT.put_object(Bucket=bucket, Key=error_log_key, Body=error_message)
                print(f"Uploaded error log to s3://{bucket}/{error_log_key}")
            except Exception as e:
                print(f"Failed to upload error log to S3. Error: {str(e)}")


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          campaign_map: !Ref ClusterOrFlowMapToCampaign
          event_threshold: !Ref EventThreshold
          tac_threshold: !Ref TacticThreshold
          ui_lb_url: !GetAtt UILoadBalancer.DNSName
          ui_password: !Ref SuperuserPassword
          ui_username: !Ref SuperuserUsername
          max_cluster_count: !Ref MaxClusterCount
          max_flow_count: !Ref MaxFlowCount
          db_connection_string: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/postgres
  createCampaignLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${createCampaign}
  createCampaignLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref createCampaign
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  deleteEvent:
    Type: AWS::Serverless::Function
    DependsOn:
    - UILoadBalancer
    - DbLoadBalancer
    Properties:
      FunctionName: !Sub ${AWS::StackName}_delete_event
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: deleteEvent
      InlineCode: |
        '''
        Delete flows and events that are not referenced
        Input: Row to be added to lookup table
        '''

        import os
        import subprocess
        import sys
        import json
        import glob
        import sqlite3
        from contextlib import closing
        import boto3
        from botocore.exceptions import ClientError
        import pandas as pd

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        subprocess.call('pip install psycopg2-binary==2.9.9 sqlalchemy==2.0.32 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import psycopg2
        from psycopg2 import sql
        from psycopg2.extras import DictCursor


        DB_CONNECTION_STRING = os.getenv("db_connection_string")

        missing_variables = []

        if DB_CONNECTION_STRING is None:
            missing_variables.append("db_connection_string")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''

            func = "lambda_handler"

            clear_temp_dir()

            try:

                print(f"{func}: Delete dangling events")
                delete_events()

            except Exception as e:
                print(f"{func}: Failed to delete event")

            return event


        def delete_events():
            '''
            Delete dangling events from SQL
            '''
            global DB_CONNECTION_STRING

            func = "delete_events"

            delete_event = """
            DELETE FROM event
            WHERE
                cluster_id IS NULL
                AND alert_id NOT IN (
                    SELECT DISTINCT e.alert_id
                    FROM event e
                    JOIN flow_output fo ON fo.alert_ids LIKE '%"' || e.alert_id || '"%'
                )
            ;
            """

            try:

                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        cursor.execute(delete_event)
                    conn.commit()

            except Exception as e:
                print(f"{func}: Failed to delete event")
                raise e


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          db_connection_string: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/postgres
  deleteEventLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${deleteEvent}
  deleteEventLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref deleteEvent
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  saveFeedback:
    Type: AWS::Serverless::Function
    DependsOn:
    - UILoadBalancer
    - DbLoadBalancer
    - ECSClusterLogGroup
    Properties:
      FunctionName: !Sub ${AWS::StackName}_save_feedback
      Description: !Sub
      - Stack ${AWS::StackName} Function ${ResourceName}
      - ResourceName: saveFeedback
      InlineCode: |
        '''
        Save copy/cut action feedback to S3

        Input: log message with action id and campaign ids involved
        '''

        import ast
        import base64
        import gzip
        import json
        import os
        import time
        import uuid
        import glob
        from datetime import datetime, timezone
        import sqlite3
        from contextlib import closing

        import pandas as pd
        import boto3
        from botocore.exceptions import ClientError
        import requests
        from requests.exceptions import RequestException

        import sys
        import subprocess

        # Install ijson pip library to handle large json input files
        os.makedirs("/tmp/pylib", exist_ok=True)
        subprocess.call('pip install psycopg2-binary==2.9.9 sqlalchemy==2.0.32 -t /tmp/pylib/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        sys.path.insert(1, '/tmp/pylib/')

        import psycopg2
        from psycopg2 import sql
        from psycopg2.extras import DictCursor

        pd.options.mode.chained_assignment = None

        UI_LB_URL = os.getenv("ui_lb_url")

        BUCKET = os.getenv("bucket")

        UI_USERNAME = os.getenv("ui_username")

        UI_PASSWORD = os.getenv("ui_password")

        DB_CONNECTION_STRING = os.getenv("db_connection_string")

        missing_variables = []

        if UI_LB_URL is None:
            missing_variables.append("ui_lb_url")
        if BUCKET is None:
            missing_variables.append("bucket")
        if UI_USERNAME is None:
            missing_variables.append("ui_username")
        if UI_PASSWORD is None:
            missing_variables.append("ui_password")

        if missing_variables:
            raise ValueError(f"Please enter environment variable(s): {missing_variables}")

        SCRATCH_DIR = "scratch"

        S3_CLIENT = boto3.client("s3")

        TEMP_CLUSTER_OF_ALERTS_JSON_FILENAME = "/tmp/feedback_alerts_cluster.json"

        TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME = "/tmp/attribute_weights.json"
        TEMP_GLOBAL_CLUSTER_OPERATIONS_FILENAME = "/tmp/global_cluster_operations.json"

        TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME = "/tmp/current_cluster_ticket_output.json"

        PATH_TO_FINAL_CLUSTER_OUTPUT = f"{SCRATCH_DIR}/cluster.json"
        TEMP_FINAL_CLUSTER_OUTPUT_FILENAME = "/tmp/final_cluster_output.json"

        PATH_TO_FINAL_CLUSTER_TICKET_OUTPUT = f"{SCRATCH_DIR}/cluster_ticket_output.json"
        TEMP_FINAL_CLUSTER_TICKET_OUTPUT_FILENAME = "/tmp/final_cluster_ticket_output.json"

        INITIAL_WEIGHTS_VALUE = 100
        MIN_WEIGHTS_VALUE = 0
        MAX_WEIGHTS_VALUE = 200

        SOURCE_EMPTY = False

        HOST = f"http://{UI_LB_URL}:8000/"


        def lambda_handler(event, context):
            '''
            Entrypoint from the trigger setup from lambda
            Args:
                event: Event triggered
                context: Context of the lambda function
            '''
            global SCRATCH_DIR
            global SOURCE_EMPTY

            func = "lambda_handler"

            SOURCE_EMPTY = False

            print(f"{func}: Received event: {json.dumps(event)}")

            aws_log = event['awslogs']['data']
            compressed_payload = base64.b64decode(aws_log)
            uncompressed_payload = gzip.decompress(compressed_payload)
            decoded_payload = json.loads(uncompressed_payload.decode('utf-8'))

            print("Decoded CloudWatch data: ", decoded_payload)

            message = decoded_payload["logEvents"][0]["message"]

            clear_temp_dir()

            try:
                message_json = json.loads(message)

                campaign_id_list = message_json["message"]["campaign_id"]
                action_id = message_json["message"]["action_id"]
            except json.JSONDecodeError as json_error:
                print(f"Error parsing JSON from message: {message}")
                raise json_error
            except KeyError as key_error:
                print("Missing key in JSON message")
                raise key_error

            print(f"{func}: Save feedback for campaign id(s): {campaign_id_list}")
            print(f"{func}: Save feedback for action id: {action_id}")

            save_feedback(action_id)


        def save_feedback(action_id):
            '''
            Fetch action performed on campaigns and create operations list, update final cluster json
            create new cluster ticket output
            Args:
                action_id: action id in UI that has details for the action performed
            '''
            global BUCKET
            global S3_CLIENT

            func = "save_feedback"

            try:
                ui_cookies, ui_headers, ui_session = get_ui_session()
            except Exception as e:
                print(f"{func}: Failed to get session.")
                raise e

            try:
                action_attributes = get_action_attributes(ui_cookies, ui_headers, ui_session, action_id)
            except Exception as e:
                print(f"{func}: Failed to get action attributes.")
                raise e

            print(action_attributes)

            # get weights from the action and save to global weights
            update_global_attributes_weights(action_attributes)

            # create current action operations
            source_cluster_id, destination_cluster_id = create_current_operations(action_attributes, ui_cookies, ui_headers, ui_session)

            metrics = update_cluster_output_sql(source_cluster_id, destination_cluster_id)

            update_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id, metrics)

            create_current_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id)

            # upload feedback cluster ticket output to S3
            current_timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %z")
            path_to_feedback_cluster_ticket_output = f"{SCRATCH_DIR}/feedback/cluster_ticket_output_{current_timestamp}.json"
            try:
                S3_CLIENT.upload_file(TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME, BUCKET, path_to_feedback_cluster_ticket_output)
                print(f"{func}: Uploaded file to S3: {BUCKET}/{path_to_feedback_cluster_ticket_output}")
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {BUCKET}/{path_to_feedback_cluster_ticket_output}")
                raise e


        def create_current_operations(action_attributes, ui_cookies, ui_headers, ui_session):
            '''
            Create operations of create and delete to perform on clusters
            Args:
                action_attributes: Record of current action performed on which the function is triggered
            '''

            func = "create_current_operations"

            print(f"{func}: Get involved events list")
            events_involved = action_attributes["events_involved"][str(action_attributes['dst_campaign'][0])]
            print(f"{func}: Involved events: {events_involved}")

            # get user alert ids
            print(f"{func}: Get involved user alert ids")
            involved_user_alert_ids = []

            campaign_event_list = get_events(ui_cookies, ui_headers, ui_session, action_attributes['dst_campaign'][0])

            for campaign_event in campaign_event_list:
                if campaign_event["id"] in events_involved:
                    involved_user_alert_ids.append(campaign_event["mapped_data"]["UID"])

            print(f"{func}: Involved user alert ids: {involved_user_alert_ids}")

            insert_operation_query = """
            INSERT INTO operation_on_cluster (
                cluster_id,
                alert_ids,
                operation_type
            )
            VALUES (
                %(cluster_id)s,
                %(alert_ids)s,
                %(action_type)s
            )
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        campaign_id_list = ",".join(map(str, [action_attributes['src_campaign'], action_attributes['dst_campaign'][0]]))
                        campaign_id_list = f"({campaign_id_list})"
                        select_cluster_output = f"""
                        SELECT cluster_id, campaign_id
                        FROM cluster_output
                        WHERE campaign_id IN {campaign_id_list}
                        ;
                        """

                        cursor.execute(select_cluster_output)
                        result = cursor.fetchall()

                        source_cluster_id = None
                        destination_cluster_id = None

                        for row in result:
                            row = dict(row)
                            if row["campaign_id"] == action_attributes['src_campaign']:
                                source_cluster_id = row["cluster_id"]
                            if row["campaign_id"] == action_attributes['dst_campaign'][0]:
                                destination_cluster_id = row["cluster_id"]

                        if source_cluster_id is None:
                            raise ValueError(f"Unable to find cluster id for campaign id: {action_attributes['src_campaign']}")
                        if destination_cluster_id is None:
                            raise ValueError(f"Unable to find cluster id for campaign id: {action_attributes['dst_campaign'][0]}")

                        print(f"{func}: Source cluster id: {source_cluster_id}")
                        print(f"{func}: Destination cluster id: {destination_cluster_id}")

                        insert_operation_params = [
                            {
                                "action_type": "DELETE",
                                "cluster_id": source_cluster_id,
                                "alert_ids": json.dumps([involved_user_alert_ids])
                            },
                            {
                                "action_type": "INSERT",
                                "cluster_id": destination_cluster_id,
                                "alert_ids": json.dumps([involved_user_alert_ids])
                            }
                        ]

                        cursor.executemany(insert_operation_query, insert_operation_params)

                        update_event_params = {
                            "cluster_id": destination_cluster_id
                        }
                        alert_id_list = "','".join(map(str, involved_user_alert_ids))
                        alert_id_list = f"('{alert_id_list}')"
                        update_event_query = f"""
                        UPDATE event
                        SET cluster_id = %(cluster_id)s
                        WHERE alert_id IN {alert_id_list}
                        ;
                        """
                        print(update_event_query)
                        cursor.execute(update_event_query, update_event_params)
                    conn.commit()

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e

            return source_cluster_id, destination_cluster_id


        def update_cluster_output_sql(source_cluster_id, destination_cluster_id):
            '''
            Update cluster output table in sqlite
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
            Returns:
                metrics dictionary for cluster id (contains count of unique tech, tac, stage per cluster)
            '''
            global SOURCE_EMPTY

            func = "update_cluster_output_sql"

            batch_cluster_id_list = [source_cluster_id, destination_cluster_id]

            update_cluster_query = """
            UPDATE cluster_output
            SET cluster_starttime = %(cluster_starttime)s,
                cluster_endtime = %(cluster_endtime)s,
                cluster_srcips = %(cluster_srcips)s,
                cluster_dstips = %(cluster_dstips)s,
                cluster_techs = %(cluster_techs)s,
                cluster_tacs = %(cluster_tacs)s,
                cluster_stages = %(cluster_stages)s
            WHERE cluster_id = %(cluster_id)s
            ;
            """

            metrics = {}

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:

                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"

                        select_events_filter_cluster_query = f"""
                        SELECT
                            cluster_id,
                            MIN(time) AS min_time,
                            MAX(time) AS max_time,
                            STRING_AGG(DISTINCT src, ',') AS unique_src,
                            STRING_AGG(DISTINCT dst, ',') AS unique_dst,
                            STRING_AGG(tech,';') AS combined_tech,
                            STRING_AGG(tac,';') AS combined_tac,
                            STRING_AGG(stage,';') AS combined_stage
                        FROM
                            event
                        WHERE cluster_id IN {cluster_id_list}
                        GROUP BY cluster_id
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_events_filter_cluster_query)
                        result = cursor.fetchall()

                        update_cluster_output_params = []
                        for row in result:
                            row = dict(row)
                            src_ips = row['unique_src']
                            src_ips = src_ips.split(",")

                            dst_ips = row['unique_dst']
                            dst_ips = dst_ips.split(",")

                            combined_tech = row['combined_tech']
                            combined_tech = combined_tech.split(";")
                            tech_list = []
                            for techs in combined_tech:
                                tech_list += json.loads(techs)
                            tech_list = list(set(tech_list))

                            combined_tac = row['combined_tac']
                            combined_tac = combined_tac.split(";")
                            tac_list = []
                            for tacs in combined_tac:
                                tac_list += json.loads(tacs)
                            tac_list = list(set(tac_list))

                            combined_stage = row['combined_stage']
                            combined_stage = combined_stage.split(";")
                            stage_list = []
                            for stage in combined_stage:
                                stage_list += json.loads(stage)
                            stage_list = list(set(stage_list))

                            metrics[row['cluster_id']] = {
                                "tech": {"count": len(tech_list)},
                                "tac": {"count": len(tac_list)},
                                "stage": {"count": len(stage_list)}
                            }

                            # if cluster id is present in table. Update fields for existing row.
                            params = {
                                "cluster_starttime": row['min_time'],
                                "cluster_endtime": row['max_time'],
                                "cluster_srcips": json.dumps(src_ips),
                                "cluster_dstips": json.dumps(dst_ips),
                                "cluster_techs": json.dumps(tech_list),
                                "cluster_tacs": json.dumps(tac_list),
                                "cluster_stages": json.dumps(stage_list),
                                "cluster_id": row['cluster_id']
                            }

                            update_cluster_output_params.append(params)

                        if source_cluster_id not in metrics:
                            SOURCE_EMPTY = True
                            params = {
                                "cluster_starttime": 0.0,
                                "cluster_endtime": 0.0,
                                "cluster_srcips": json.dumps([]),
                                "cluster_dstips": json.dumps([]),
                                "cluster_techs": json.dumps([]),
                                "cluster_tacs": json.dumps([]),
                                "cluster_stages": json.dumps([]),
                                "cluster_id": source_cluster_id
                            }

                            update_cluster_output_params.append(params)

                        cursor.executemany(update_cluster_query, update_cluster_output_params)

                    conn.commit()
                return metrics
            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id, metrics):
            '''
            Update cluster ticket output metrics
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
                metrics: list of metric for cluster id (contains count of unique tech, tac, stage per cluster)
            '''

            func = "update_cluster_ticket_output_sql"

            batch_cluster_id_list = [source_cluster_id, destination_cluster_id]

            metrics_cols = ["tech", "tac", "stage"]

            update_cluster_ticket_output_query = """
            UPDATE cluster_ticket_output
            SET metrics = %(metrics)s
            WHERE cluster_id = %(cluster_id)s
            ;
            """

            try:
                with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                    with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                        cluster_id_list = ",".join(map(str, batch_cluster_id_list))
                        cluster_id_list = f"({cluster_id_list})"
                        select_event_query = f"""
                        SELECT *
                        FROM event
                        WHERE cluster_id IN {cluster_id_list}
                        ORDER BY cluster_id
                        ;
                        """
                        cursor.execute(select_event_query)
                        results = cursor.fetchall()
                        results = [dict(row) for row in results]
                        events_df = pd.DataFrame(results)

                        update_cluster_ticket_output_params = []
                        for cluster_id in batch_cluster_id_list:
                            if cluster_id in metrics:
                                filtered_events = events_df[events_df["cluster_id"] == cluster_id]
                                cluster_metrics = per_cluster_metrics_events(filtered_events)

                                for metric_col in metrics_cols:
                                    cluster_metrics[metric_col]["count"] = float(metrics[cluster_id][metric_col]["count"])

                                params = {
                                    "cluster_id": cluster_id,
                                    "metrics": json.dumps(cluster_metrics)
                                }
                                update_cluster_ticket_output_params.append(params)
                            else:
                                params = {
                                    "cluster_id": cluster_id,
                                    "metrics": json.dumps({})
                                }
                                update_cluster_ticket_output_params.append(params)
                        cursor.executemany(update_cluster_ticket_output_query, update_cluster_ticket_output_params)

                    conn.commit()

            except Exception as e:
                print(f"{func}: Failed to update cluster ticket output table.")
                raise e


        def per_cluster_metrics_events(involved_events_df):
            '''
            Get metrics per cluster
            Args:
                row_cluster_ticket_output: one record of cluster ticket output
            Returns:
                metrics of one record of cluster ticket output
            '''

            metrics_cols = ["tech", "tac", "stage"]

            metrics = {}

            for metric_col in metrics_cols:
                involved_events_df[f'{metric_col}'] = involved_events_df[f'{metric_col}'].apply(json.loads)
                involved_events_df[f'{metric_col}_count'] = involved_events_df[f'{metric_col}'].apply(len)

                metrics[metric_col] = {
                    "avg": float(involved_events_df[f"{metric_col}_count"].mean()),
                    "min": float(involved_events_df[f"{metric_col}_count"].min()),
                    "max": float(involved_events_df[f"{metric_col}_count"].max()),
                    "median": float(involved_events_df[f"{metric_col}_count"].median())
                }

            return metrics


        def create_current_cluster_ticket_output_sql(source_cluster_id, destination_cluster_id):
            '''
            Create cluster ticket output for current operation clusters
            Args:
                source_cluster_id: source cluster id
                destination_cluster_id: destination cluster id
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME
            global SOURCE_EMPTY

            func = "create_current_cluster_ticket_output_sql"

            input_cluster_id_list = [source_cluster_id, destination_cluster_id]

            try:
                with open(TEMP_CURRENT_CLUSTER_TICKET_OUTPUT_FILENAME, "w") as f:
                    with closing(psycopg2.connect(DB_CONNECTION_STRING)) as conn:
                        with closing(conn.cursor(cursor_factory=DictCursor)) as cursor:
                            cluster_id_list = ",".join(map(str, input_cluster_id_list))
                            cluster_id_list = f"({cluster_id_list})"
                            select_query = f"""
                            SELECT
                                co.cluster_id,
                                co.cluster_starttime,
                                co.cluster_endtime,
                                co.cluster_srcips,
                                co.cluster_dstips,
                                co.cluster_techs,
                                co.cluster_tacs,
                                co.cluster_stages,
                                cto.ticket_id,
                                cto.metrics,
                                e.alert_id,
                                e.src AS event_src,
                                e.dst AS event_dst,
                                e.time AS event_time,
                                e.name AS event_name,
                                e.tech AS event_tech,
                                e.tac AS event_tac,
                                e.stage AS event_stage,
                                e.other_attributes AS event_other_attributes
                            FROM cluster_ticket_output cto
                            INNER JOIN cluster_output co ON co.cluster_id = cto.cluster_id
                            INNER JOIN event e ON e.cluster_id = cto.cluster_id
                            WHERE cto.cluster_id IN {cluster_id_list}
                            ORDER BY cto.cluster_id
                            ;
                            """
                            cursor.execute(select_query)

                            row = cursor.fetchone()
                            prev_cluster = None

                            involved_events = []

                            while row:
                                row = dict(row)

                                curr_cluster = row
                                if prev_cluster is None:
                                    prev_cluster = curr_cluster

                                if curr_cluster["cluster_id"] != prev_cluster["cluster_id"]:
                                    row_cluster_ticket_output = {
                                        "ticket_id": prev_cluster["ticket_id"],
                                        "cluster_id": prev_cluster["cluster_id"],
                                        "involved_events": involved_events,
                                        "start_time": prev_cluster["cluster_starttime"],
                                        "end_time": prev_cluster["cluster_endtime"],
                                        "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                        "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                        "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                        "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                        "metrics": json.loads(prev_cluster["metrics"]),
                                    }
                                    f.write(json.dumps(row_cluster_ticket_output) + "\n")
                                    involved_events = []

                                events = {
                                    "id": row["alert_id"],
                                    "src": row["event_src"],
                                    "dst": row["event_dst"],
                                    "time": row["event_time"],
                                    "name": row["event_name"],
                                    "tech": json.loads(row["event_tech"]),
                                    "tac": json.loads(row["event_tac"]),
                                    "stage": json.loads(row["event_stage"]),
                                    "other_attributes_dict": json.loads(row["event_other_attributes"]),
                                }
                                involved_events.append(events)

                                prev_cluster = curr_cluster
                                row = cursor.fetchone()

                            row_cluster_ticket_output = {
                                "ticket_id": prev_cluster["ticket_id"],
                                "cluster_id": prev_cluster["cluster_id"],
                                "involved_events": involved_events,
                                "start_time": prev_cluster["cluster_starttime"],
                                "end_time": prev_cluster["cluster_endtime"],
                                "involved_entities": list(set(json.loads(prev_cluster["cluster_dstips"]) + json.loads(prev_cluster["cluster_srcips"]))),
                                "involved_techs": json.loads(prev_cluster["cluster_techs"]),
                                "involved_tacs": json.loads(prev_cluster["cluster_tacs"]),
                                "involved_stages": json.loads(prev_cluster["cluster_stages"]),
                                "metrics": json.loads(prev_cluster["metrics"]),
                            }
                            f.write(json.dumps(row_cluster_ticket_output) + "\n")

                            if SOURCE_EMPTY:
                                select_query = f"""
                                SELECT
                                    co.cluster_id,
                                    co.cluster_starttime,
                                    co.cluster_endtime,
                                    co.cluster_srcips,
                                    co.cluster_dstips,
                                    co.cluster_techs,
                                    co.cluster_tacs,
                                    co.cluster_stages,
                                    cto.ticket_id,
                                    cto.metrics
                                FROM cluster_ticket_output cto
                                INNER JOIN cluster_output co ON co.cluster_id = cto.cluster_id
                                WHERE cto.cluster_id = {source_cluster_id}
                                ORDER BY cto.cluster_id
                                ;
                                """
                                cursor.execute(select_query)

                                row = cursor.fetchone()
                                row_cluster_ticket_output = {
                                    "ticket_id": row["ticket_id"],
                                    "cluster_id": row["cluster_id"],
                                    "involved_events": [],
                                    "start_time": row["cluster_starttime"],
                                    "end_time": row["cluster_endtime"],
                                    "involved_entities": list(set(json.loads(row["cluster_dstips"]) + json.loads(row["cluster_srcips"]))),
                                    "involved_techs": json.loads(row["cluster_techs"]),
                                    "involved_tacs": json.loads(row["cluster_tacs"]),
                                    "involved_stages": json.loads(row["cluster_stages"]),
                                    "metrics": json.loads(row["metrics"]),
                                }
                            f.write(json.dumps(row_cluster_ticket_output) + "\n")

            except Exception as e:
                print(f"{func}: Failed to update event table.")
                raise e


        def update_global_attributes_weights(action_attributes):
            '''
            Update global attributes weights file
            Args:
                action_attributes: Record of current action performed on which the function is triggered
            '''
            global S3_CLIENT
            global BUCKET
            global TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME
            global INITIAL_WEIGHTS_VALUE

            func = "update_global_attributes_weights"

            path_to_global_attribute_weights = f"{SCRATCH_DIR}/attribute_weights.json"
            fetch_or_create_global_attribute_weights(path_to_global_attribute_weights)

            global_attribute_weights_json = json.load(open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME))

            print(f"{func}: Current weights: {global_attribute_weights_json}")

            attributes_list = ["node", "event"]

            map_attributes = {
                "tech": "technique"
            }

            for attr in attributes_list:
                for key, value in action_attributes["attributes"][attr].items():
                    if key in map_attributes:
                        key = map_attributes[key]
                    if key not in global_attribute_weights_json[attr]:
                        global_attribute_weights_json[attr][key] = INITIAL_WEIGHTS_VALUE

                    global_attribute_weights_json[attr][key] += value

                    if global_attribute_weights_json[attr][key] < MIN_WEIGHTS_VALUE:
                        global_attribute_weights_json[attr][key] = MIN_WEIGHTS_VALUE

                    if global_attribute_weights_json[attr][key] > MAX_WEIGHTS_VALUE:
                        global_attribute_weights_json[attr][key] = MAX_WEIGHTS_VALUE
            
            print(f"{func}: After update weights: {global_attribute_weights_json}")
            json.dump(global_attribute_weights_json, open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, "w"))

            try:
                print(f"{func}: Upload file to S3: {BUCKET}/{path_to_global_attribute_weights}")
                S3_CLIENT.upload_file(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, BUCKET, path_to_global_attribute_weights)
            except Exception as e:
                print(f"{func}: Failed to upload file to S3: {BUCKET}/{path_to_global_attribute_weights}")
                raise e


        def fetch_or_create_global_attribute_weights(path_to_global_attribute_weights):
            '''
            Get or initialize the global attribute weights file
            Args:
                path_to_global_attribute_weights: global attribute weights object key
            '''
            global SCRATCH_DIR
            global S3_CLIENT
            global BUCKET
            global TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME
            global INITIAL_WEIGHTS_VALUE
            func = "fetch_or_create_global_attribute_weights"

            try:
                print(
                    f"{func}: Checking if file exists on S3 path {BUCKET}/{path_to_global_attribute_weights}")

                S3_CLIENT.head_object(Bucket=BUCKET, Key=path_to_global_attribute_weights)

            except ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise e

                print(f"{func}: File does not exist on S3. Initialize file")

                global_attribute_weights_json = {
                    "event": {
                        "technique": INITIAL_WEIGHTS_VALUE,
                        "tactic": INITIAL_WEIGHTS_VALUE,
                        "stage": INITIAL_WEIGHTS_VALUE,
                        "count": INITIAL_WEIGHTS_VALUE,
                        "priority": INITIAL_WEIGHTS_VALUE,
                        "port": INITIAL_WEIGHTS_VALUE,
                        "url": INITIAL_WEIGHTS_VALUE,
                        "user_agent": INITIAL_WEIGHTS_VALUE,
                        "cert": INITIAL_WEIGHTS_VALUE
                    },
                    "node": {
                        "os": INITIAL_WEIGHTS_VALUE,
                        "risk": INITIAL_WEIGHTS_VALUE,
                        "user": INITIAL_WEIGHTS_VALUE,
                        "domain": INITIAL_WEIGHTS_VALUE,
                        "subnet": INITIAL_WEIGHTS_VALUE,
                        "usergroup": INITIAL_WEIGHTS_VALUE,
                        "geolocation": INITIAL_WEIGHTS_VALUE
                    }
                }
                json.dump(global_attribute_weights_json, open(TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME, "w"))

                print(f"{func}: Initiated file")
            else:
                try:
                    print(f"{func}: Download available file from S3")

                    S3_CLIENT.download_file(BUCKET, path_to_global_attribute_weights, TEMP_GLOBAL_ATTRIBUTE_WEIGHTS_FILENAME)

                    print(f"{func}: Download completed")
                except Exception as e:
                    print(f"{func}: Failed to download lookup file from S3: {BUCKET}/{path_to_global_attribute_weights}")
                    raise e


        def get_campaign(ui_cookies, ui_headers, ui_session, campaign_id):
            '''
            Get campaign details by sending a request to the UI
            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                campaign_id: ID of the campaign
            Returns:
                JSON response containing the campaign
            '''
            global HOST

            func = "get_campaign"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/campaign/{campaign_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get campaign details for campaign id: {campaign_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response for campaign_id id: {campaign_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_raw_event(ui_cookies, ui_headers, ui_session, campaign_id, event_id):
            '''
            Get raw event details by event ID within a campaign.

            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                campaign_id: ID of the campaign
                event_id: ID of the event
            Returns:
                JSON response containing the raw event details
            '''
            global HOST

            func = "get_raw_event"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/campaign/{campaign_id}/event/{event_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get event details for event id: {event_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response for event id: {event_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_events(ui_cookies, ui_headers, ui_session, campaign_id):
            '''
            Get event details by campaign ID within a campaign.

            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                campaign_id: ID of the campaign
            Returns:
                JSON response containing the event details
            '''
            global HOST

            func = "get_events"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/campaign/{campaign_id}/event/bulk", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get event details")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response. Response: {response_from_ui.content}")
                raise json_err


        def get_action_attributes(ui_cookies, ui_headers, ui_session, action_id):
            '''
            Get action attributes by action ID.

            Args:
                ui_cookies: Cookies for the UI session
                ui_headers: Headers for the UI session
                ui_session: Session object for the UI
                action_id: ID of the action

            Returns:
                JSON response containing the action attributes
            '''
            global HOST

            func = "get_action_attributes"

            try:
                response_from_ui = ui_session.get(f"{HOST}api/v2/action/{action_id}", cookies=ui_cookies, headers=ui_headers)
                print(f"{func}: Got response", response_from_ui)

                response_from_ui.raise_for_status()

                return response_from_ui.json()
            except RequestException as req_err:
                print(f"{func}: Unable to get attributes from action id: {action_id}")
                raise req_err
            except ValueError as json_err:
                print(f"{func}: Unable to parse JSON response from action id: {action_id}. Response: {response_from_ui.content}")
                raise json_err


        def get_ui_session():
            '''
            Get UI session and cookies
            Returns:
                cookies: Cookies for the connection to UI
                headers: Headers for the connection to UI
                s: session for the connection to UI
            '''
            global HOST
            global UI_PASSWORD
            global UI_USERNAME

            try:
                s = requests.Session()
                response = s.get(HOST + "login")
                response.raise_for_status()

                cookies = s.cookies.get_dict()
                csrf = cookies.get("csrftoken")
                if not csrf:
                    raise ValueError("CSRF token not found in cookies")

                headers = {"X-CSRFToken": csrf, "Accept": "application/json"}

                response = s.post(f"{HOST}login/?next",
                                  data={"username": UI_USERNAME, "password": UI_PASSWORD, "csrfmiddlewaretoken": csrf})

                if response.status_code != 404 and not response.url.endswith("/accounts/profile/"):
                    response.raise_for_status()

                print("login successful")
                return cookies, headers, s

            except requests.exceptions.RequestException as e:
                print("Request error occurred.")
                raise e
            except Exception as e:
                print("An unexpected error occurred.")
                raise e


        def clear_temp_dir():
            rm_list = glob.glob("/tmp/**", recursive=True)
            for f in rm_list:
                if os.path.isfile(f):
                    os.remove(f)

      Handler: index.lambda_handler
      Runtime: python3.11
      MemorySize: 3008
      EphemeralStorage:
        Size: 10240
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Tracing: Active
      Layers:
      - !FindInMap
        - RegionMap
        - !Ref AWS::Region
        - lambdaLayer
      Environment:
        Variables:
          bucket: !Ref BucketName
          ui_lb_url: !GetAtt UILoadBalancer.DNSName
          ui_password: !Ref SuperuserPassword
          ui_username: !Ref SuperuserUsername
          db_connection_string: !Sub postgresql://${DbUsername}:${DbPassword}@${DbLoadBalancer.DNSName}:5432/postgres
      Events:
        MyLogGroupTrigger:
          Type: CloudWatchLogs
          Properties:
            LogGroupName: !Ref ECSClusterLogGroup
            FilterPattern: Save campaign json
  saveFeedbackLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      LogGroupName: !Sub /aws/lambda/${saveFeedback}
  saveFeedbackLambdaEventInvokeConfig:
    Type: AWS::Lambda::EventInvokeConfig
    Properties:
      FunctionName: !Ref saveFeedback
      MaximumRetryAttempts: 0
      Qualifier: $LATEST

  stepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub ${AWS::StackName}_step_function
      RoleArn: !GetAtt StepFunctionRole.Arn
      LoggingConfiguration:
        Level: ALL
        IncludeExecutionData: true
        Destinations:
        - CloudWatchLogsLogGroup:
            LogGroupArn: !GetAtt StepFunctionLogGroup.Arn
      DefinitionString: !Sub |
        {
          "Comment": "A description of my state machine",
          "StartAt": "enrich_with_technique",
          "States": {
            "enrich_with_technique": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${enrichWithTechnique.Arn}",
                "Payload.$": "$"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "Next": "tech_or_cluster"
            },
            "map tech": {
              "Type": "Map",
              "ItemProcessor": {
                "ProcessorConfig": {
                  "Mode": "DISTRIBUTED",
                  "ExecutionType": "STANDARD"
                },
                "StartAt": "process_enriched_with_technique",
                "States": {
                  "process_enriched_with_technique": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "OutputPath": "$.Payload",
                    "Parameters": {
                      "FunctionName": "${processEnrichedWithTechnique.Arn}",
                      "Payload.$": "$"
                    },
                    "Retry": [
                      {
                        "ErrorEquals": [
                          "Lambda.ServiceException",
                          "Lambda.AWSLambdaException",
                          "Lambda.SdkClientException",
                          "Lambda.TooManyRequestsException",
                          "States.TaskFailed"
                        ],
                        "MaxAttempts": 3,
                        "BackoffRate": 2,
                        "IntervalSeconds": 5
                      }
                    ],
                    "End": true
                  }
                }
              },
              "Next": "create_embedding",
              "Label": "mapembeddingecs",
              "ItemReader": {
                "Resource": "arn:aws:states:::s3:listObjectsV2",
                "Parameters": {
                  "Bucket.$": "$.bucket",
                  "Prefix.$": "$.prefix"
                }
              },
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "OutputPath": "$.[0]"
            },
            "create_embedding": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$",
                "FunctionName": "${createEmbedding.Arn}"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "Next": "Choice"
            },
            "Choice": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.TransformJobName",
                  "StringEquals": "skipped_input",
                  "Next": "Success"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-embedding*",
                  "Next": "map embedding ecs"
                }
              ],
              "Default": "Fail"
            },
            "tech_or_cluster": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-tech*",
                  "Next": "map tech ecs"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-cluster*",
                  "Next": "create_embedding"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringEquals": "skipped_input",
                  "Next": "Success"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringEquals": "user_override_cluster",
                  "Next": "process_cluster"
                }
              ],
              "Default": "Fail"
            },
            "map embedding ecs": {
              "Type": "Map",
              "ItemProcessor": {
                "ProcessorConfig": {
                  "Mode": "DISTRIBUTED",
                  "ExecutionType": "STANDARD"
                },
                "StartAt": "wait embedding task",
                "States": {
                  "wait embedding task": {
                    "Type": "Wait",
                    "Seconds": 15,
                    "Next": "start_embedding_task"
                  },
                  "start_embedding_task": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "OutputPath": "$.Payload",
                    "Parameters": {
                      "FunctionName": "${startEmbeddingTask.Arn}",
                      "Payload.$": "$"
                    },
                    "Retry": [
                      {
                        "ErrorEquals": [
                          "Lambda.ServiceException",
                          "Lambda.AWSLambdaException",
                          "Lambda.SdkClientException",
                          "Lambda.TooManyRequestsException",
                          "States.TaskFailed"
                        ],
                        "MaxAttempts": 3,
                        "BackoffRate": 2,
                        "IntervalSeconds": 5
                      }
                    ],
                    "Next": "embedding ecs task"
                  },
                  "embedding ecs task": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::ecs:runTask.waitForTaskToken",
                    "Parameters": {
                      "Cluster.$": "$.Cluster",
                      "TaskDefinition.$": "$.TaskDefinition",
                      "CapacityProviderStrategy": [
                        {
                          "CapacityProvider": "${ClusterPart1EC2CapacityProvider}",
                          "Weight": 1
                        }
                      ],
                      "Overrides": {
                        "ContainerOverrides": [
                          {
                            "Name": "model",
                            "Environment": [
                              {
                                "Name": "TASK_TOKEN",
                                "Value.$": "$$.Task.Token"
                              },
                              {
                                "Name": "S3InputPath",
                                "Value.$": "$.S3InputPath"
                              },
                              {
                                "Name": "S3OutputPath",
                                "Value.$": "$.S3OutputPath"
                              }
                            ]
                          }
                        ]
                      }
                    },
                    "TimeoutSeconds": 3600,
                    "ResultPath": "$.response",
                    "End": true
                  }
                }
              },
              "Next": "map embedding",
              "Label": "mapembedding",
              "MaxConcurrency": 1,
              "ItemReader": {
                "Resource": "arn:aws:states:::s3:listObjectsV2",
                "Parameters": {
                  "Bucket.$": "$.bucket",
                  "Prefix.$": "$.prefix"
                }
              },
              "OutputPath": "$.[0]",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.HeartbeatTimeout",
                    "States.Timeout",
                    "States.ResultPathMatchFailure",
                    "States.ParameterPathFailure",
                    "States.BranchFailed",
                    "States.NoChoiceMatched",
                    "States.IntrinsicFailure"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "map tech ecs": {
              "Type": "Map",
              "ItemProcessor": {
                "ProcessorConfig": {
                  "Mode": "DISTRIBUTED",
                  "ExecutionType": "STANDARD"
                },
                "StartAt": "wait tech task",
                "States": {
                  "wait tech task": {
                    "Type": "Wait",
                    "Seconds": 15,
                    "Next": "start_tech_task"
                  },
                  "start_tech_task": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "OutputPath": "$.Payload",
                    "Parameters": {
                      "FunctionName": "${startTechTask.Arn}",
                      "Payload.$": "$"
                    },
                    "Retry": [
                      {
                        "ErrorEquals": [
                          "Lambda.ServiceException",
                          "Lambda.AWSLambdaException",
                          "Lambda.SdkClientException",
                          "Lambda.TooManyRequestsException",
                          "States.TaskFailed"
                        ],
                        "MaxAttempts": 3,
                        "BackoffRate": 2,
                        "IntervalSeconds": 5
                      }
                    ],
                    "Next": "tech ecs task"
                  },
                  "tech ecs task": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::ecs:runTask.waitForTaskToken",
                    "Parameters": {
                      "Cluster.$": "$.Cluster",
                      "TaskDefinition.$": "$.TaskDefinition",
                      "CapacityProviderStrategy": [
                        {
                          "CapacityProvider": "${TechEC2CapacityProvider}",
                          "Weight": 1
                        }
                      ],
                      "Overrides": {
                        "ContainerOverrides": [
                          {
                            "Name": "model",
                            "Environment": [
                              {
                                "Name": "TASK_TOKEN",
                                "Value.$": "$$.Task.Token"
                              },
                              {
                                "Name": "S3InputPath",
                                "Value.$": "$.S3InputPath"
                              },
                              {
                                "Name": "S3OutputPath",
                                "Value.$": "$.S3OutputPath"
                              }
                            ]
                          }
                        ]
                      }
                    },
                    "TimeoutSeconds": 3600,
                    "End": true,
                    "ResultPath": "$.response"
                  }
                }
              },
              "Next": "map tech",
              "Label": "maptechecs",
              "MaxConcurrency": 1,
              "ItemReader": {
                "Resource": "arn:aws:states:::s3:listObjectsV2",
                "Parameters": {
                  "Bucket.$": "$.bucket",
                  "Prefix.$": "$.prefix"
                }
              },
              "OutputPath": "$.[0]",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.Timeout",
                    "States.HeartbeatTimeout",
                    "States.ResultPathMatchFailure",
                    "States.ParameterPathFailure",
                    "States.BranchFailed",
                    "States.IntrinsicFailure",
                    "States.NoChoiceMatched"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "map embedding": {
              "Type": "Map",
              "ItemProcessor": {
                "ProcessorConfig": {
                  "Mode": "DISTRIBUTED",
                  "ExecutionType": "STANDARD"
                },
                "StartAt": "process_embedding",
                "States": {
                  "process_embedding": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "OutputPath": "$.Payload",
                    "Parameters": {
                      "FunctionName": "${processEmbedding.Arn}",
                      "Payload.$": "$"
                    },
                    "Retry": [
                      {
                        "ErrorEquals": [
                          "Lambda.ServiceException",
                          "Lambda.AWSLambdaException",
                          "Lambda.SdkClientException",
                          "Lambda.TooManyRequestsException",
                          "States.TaskFailed"
                        ],
                        "MaxAttempts": 3,
                        "BackoffRate": 2,
                        "IntervalSeconds": 5
                      }
                    ],
                    "End": true
                  }
                }
              },
              "Next": "create_cluster",
              "Label": "Map",
              "ItemReader": {
                "Resource": "arn:aws:states:::s3:listObjectsV2",
                "Parameters": {
                  "Bucket.$": "$.bucket",
                  "Prefix.$": "$.prefix"
                }
              },
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "OutputPath": "$.[0]"
            },
            "create_cluster": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$",
                "FunctionName": "${createCluster.Arn}"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Next": "cluster ecs task",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "cluster ecs task": {
              "Type": "Task",
              "Resource": "arn:aws:states:::ecs:runTask.waitForTaskToken",
              "Parameters": {
                "Cluster.$": "$.Cluster",
                "TaskDefinition.$": "$.TaskDefinition",
                "CapacityProviderStrategy": [
                  {
                    "CapacityProvider": "${ClusterPart2EC2CapacityProvider}",
                    "Weight": 1
                  }
                ],
                "Overrides": {
                  "ContainerOverrides": [
                    {
                      "Name": "model",
                      "Environment": [
                        {
                          "Name": "TASK_TOKEN",
                          "Value.$": "$$.Task.Token"
                        },
                        {
                          "Name": "S3InputPath",
                          "Value.$": "$.S3InputPath"
                        },
                        {
                          "Name": "S3OutputPath",
                          "Value.$": "$.S3OutputPath"
                        }
                      ]
                    }
                  ]
                }
              },
              "TimeoutSeconds": 3600,
              "ResultPath": "$.response",
              "Next": "process_cluster"
            },
            "process_cluster": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${processCluster.Arn}",
                "Payload": {
                  "TransformJobName.$": "$.TransformJobName",
                  "inputPath.$": "$.S3InputPath",
                  "outputPath.$": "$.S3OutputPath"
                }
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException",
                    "States.TaskFailed"
                  ],
                  "IntervalSeconds": 5,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Next": "cluster_or_flow",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "cluster_or_flow": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-cluster*",
                  "Next": "cluster ecs task"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringMatches": "*job-flow*",
                  "Next": "flow ecs task"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringEquals": "user_override_cluster",
                  "Next": "process_cluster"
                },
                {
                  "Variable": "$.TransformJobName",
                  "StringEquals": "user_override_flow",
                  "Next": "process_flow"
                }
              ],
              "Default": "Fail"
            },
            "flow ecs task": {
              "Type": "Task",
              "Resource": "arn:aws:states:::ecs:runTask.waitForTaskToken",
              "Parameters": {
                "Cluster.$": "$.Cluster",
                "TaskDefinition.$": "$.TaskDefinition",
                "CapacityProviderStrategy": [
                  {
                    "CapacityProvider": "${FlowEC2CapacityProvider}",
                    "Weight": 1
                  }
                ],
                "Overrides": {
                  "ContainerOverrides": [
                    {
                      "Name": "model",
                      "Environment": [
                        {
                          "Name": "TASK_TOKEN",
                          "Value.$": "$$.Task.Token"
                        },
                        {
                          "Name": "S3InputPath",
                          "Value.$": "$.S3InputPath"
                        },
                        {
                          "Name": "S3OutputPath",
                          "Value.$": "$.S3OutputPath"
                        }
                      ]
                    }
                  ]
                }
              },
              "TimeoutSeconds": 3600,
              "ResultPath": "$.response",
              "Next": "process_flow"
            },
            "process_flow": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${processFlow.Arn}",
                "Payload": {
                  "TransformJobName.$": "$.TransformJobName",
                  "inputPath.$": "$.S3InputPath",
                  "outputPath.$": "$.S3OutputPath"
                }
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Next": "create_campaign",
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ]
            },
            "create_campaign": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$",
                "FunctionName": "${createCampaign.Arn}"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "Next": "delete_event"
            },
            "delete_event": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$",
                "FunctionName": "${deleteEvent.Arn}"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException",
                    "Lambda.TooManyRequestsException"
                  ],
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "BackoffRate": 2
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Next": "Fail"
                }
              ],
              "Next": "Success"
            },
            "Success": {
              "Type": "Succeed"
            },
            "Fail": {
              "Type": "Fail"
            }
          }
        }

  StepFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/vendedlogs/states/${AWS::StackName}_step_function

  ExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ecs-tasks.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy
      - arn:aws:iam::aws:policy/AWSMarketplaceMeteringRegisterUsage

  EcsModelExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ecs-tasks.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-ecsModelPolicy"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:*
            - states:*
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy
      - arn:aws:iam::aws:policy/AWSMarketplaceMeteringRegisterUsage

  EcsInstanceExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: !Sub "${AWS::StackName}-RexrayPolicy"
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ec2:AttachVolume
            - ec2:CreateVolume
            - ec2:CreateSnapshot
            - ec2:CreateTags
            - ec2:DeleteVolume
            - ec2:DeleteSnapshot
            - ec2:DescribeAvailabilityZones
            - ec2:DescribeInstances
            - ec2:DescribeVolumes
            - ec2:DescribeVolumeAttribute
            - ec2:DescribeVolumeStatus
            - ec2:DescribeSnapshots
            - ec2:CopySnapshot
            - ec2:DescribeSnapshotAttribute
            - ec2:DetachVolume
            - ec2:ModifySnapshotAttribute
            - ec2:ModifyVolumeAttribute
            - ec2:DescribeTag
            Resource: '*'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role

  EcsInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
      - !Ref EcsInstanceExecutionRole

  # UI ECS
  UITaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - UILoadBalancer
    Properties:
      Family: !Sub ui-task-${AWS::StackName}
      Cpu: !Ref Cpu
      Memory: !Ref Memory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt ExecutionRole.Arn
      TaskRoleArn: !GetAtt ExecutionRole.Arn
      ContainerDefinitions:
      - Name: bastet_db
        Image: postgres:13-bullseye
        Cpu: 0
        PortMappings: []
        Essential: false
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_DB
          Value: bastet
        MountPoints:
        - SourceVolume: postgres_data
          ContainerPath: /var/lib/postgresql/data/
          ReadOnly: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_redis
        Image: redis:6.2.6-alpine
        Cpu: 0
        PortMappings: []
        Essential: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_rabbitmq
        Image: rabbitmq:alpine
        Cpu: 0
        PortMappings:
        - Name: bastet_rabbitmq-5672-tcp
          ContainerPort: 5672
          HostPort: 5672
          Protocol: tcp
        Essential: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_web
        Image: !Ref WebContainerImage
        Cpu: 0
        PortMappings:
        - Name: bastet_web-8000-tcp
          ContainerPort: 8000
          HostPort: 8000
          Protocol: tcp
        Essential: false
        Command:
        - ./entry.sh
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: bastet.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_EMAIL
          Value: !Ref SuperuserEmail
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_USERNAME
          Value: !Ref SuperuserUsername
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: DJANGO_SUPERUSER_PASSWORD
          Value: !Ref SuperuserPassword
        - Name: POSTGRES_DB_NAME
          Value: bastet
        - Name: AWS_REGION
          Value: !Ref AWS::Region
        MountPoints:
        - SourceVolume: static_volume
          ContainerPath: /code/static
          ReadOnly: false
        - SourceVolume: media_volume
          ContainerPath: /code/media
          ReadOnly: false
        - SourceVolume: shared_temp
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_db
          Condition: START
        - ContainerName: bastet_redis
          Condition: START
        - ContainerName: bastet_rabbitmq
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_celery
        Image: !Ref WebContainerImage
        Cpu: 0
        Essential: true
        Command:
        - celery
        - -A
        - bastet
        - worker
        - -l
        - info
        Environment:
        - Name: POSTGRES_USER
          Value: bastet
        - Name: DJANGO_SETTINGS_MODULE
          Value: bastet.settings.local
        - Name: REDIS_HOST
          Value: localhost
        - Name: POSTGRES_HOST
          Value: localhost
        - Name: POSTGRES_PASSWORD
          Value: bastet
        - Name: POSTGRES_PORT
          Value: '5432'
        - Name: RABBITMQ_HOST
          Value: localhost
        - Name: POSTGRES_DB_NAME
          Value: bastet
        MountPoints:
        - SourceVolume: shared_temp
          ContainerPath: /tmp/shared
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_db
          Condition: START
        - ContainerName: bastet_redis
          Condition: START
        - ContainerName: bastet_rabbitmq
          Condition: START
        - ContainerName: bastet_web
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      - Name: bastet_nginx
        Image: !Ref NginxContainerImage
        Cpu: 0
        PortMappings:
        - Name: bastet_nginx-80-tcp
          ContainerPort: 80
          HostPort: 80
          Protocol: tcp
        Essential: true
        MountPoints:
        - SourceVolume: static_volume
          ContainerPath: /home/app/web/staticfiles
          ReadOnly: false
        - SourceVolume: media_volume
          ContainerPath: /home/app/web/mediafiles
          ReadOnly: false
        DependsOn:
        - ContainerName: bastet_celery
          Condition: START
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      Volumes:
      - Name: postgres_data
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: static_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: media_volume
        DockerVolumeConfiguration:
          Scope: task
          Driver: local
      - Name: shared_temp
        DockerVolumeConfiguration:
          Scope: task
          Driver: local

  UIEcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - UITaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Sub ui-service-${AWS::StackName}
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt UITaskDefinition.TaskDefinitionArn
      CapacityProviderStrategy:
      - CapacityProvider: !Ref UIEC2CapacityProvider
        Base: 0
        Weight: 1
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: bastet_nginx
        ContainerPort: 80
        TargetGroupArn: !GetAtt UITargetGroup.TargetGroupArn

  UILoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - UITargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${UILoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      Subnets:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      SecurityGroups:
      - !Ref SecurityGroup

  UITargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${UILoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      VpcId: !Ref VPC
      Port: 80
      Protocol: HTTP
      HealthCheckProtocol: HTTP
      HealthCheckPort: 80
      HealthCheckPath: /
      Matcher:
        HttpCode: 200-399
      TargetType: instance

  UIListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt UILoadBalancer.LoadBalancerArn
      Port: 8000
      Protocol: HTTP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt UITargetGroup.TargetGroupArn

  UIECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        InstanceType: !Ref UIECSClusterInstanceType
        IamInstanceProfile:
          Arn: !GetAtt EcsInstanceProfile.Arn
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  UIECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref UIECSLaunchTemplate
        Version: !GetAtt UIECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-ui-instance

  UIEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref UIECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # DB ECS
  DbTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub db-task-${AWS::StackName}
      Cpu: !Ref Cpu
      Memory: !Ref Memory
      NetworkMode: host
      ExecutionRoleArn: !GetAtt ExecutionRole.Arn
      TaskRoleArn: !GetAtt ExecutionRole.Arn
      RequiresCompatibilities:
      - EC2
      PlacementConstraints:
      - Type: memberOf
        Expression: !Sub
        - attribute:ecs.availability-zone==${AvailabilityZone}
        - AvailabilityZone: !Select
          - '0'
          - Fn::GetAZs: !Ref AWS::Region
      ContainerDefinitions:
      - Name: lambda_db
        Image: postgres:13-bullseye
        Cpu: 0
        PortMappings:
        - Name: lambda_db-5432-tcp
          ContainerPort: 5432
          HostPort: 5432
          Protocol: tcp
        Essential: true
        Environment:
        - Name: POSTGRES_USER
          Value: !Ref DbUsername
        - Name: POSTGRES_PASSWORD
          Value: !Ref DbPassword
        - Name: POSTGRES_DB
          Value: lambda
        MountPoints:
        - SourceVolume: !Sub "${AWS::StackName}_postgres_data"
          ContainerPath: /var/lib/postgresql/data/
          ReadOnly: false
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs
      Volumes:
      - Name: !Sub "${AWS::StackName}_postgres_data"
        DockerVolumeConfiguration:
          Scope: shared
          Driver: rexray/ebs
          Autoprovision: true
          DriverOpts:
            size: '10'
            volumetype: gp2

  DbEcsService:
    Type: AWS::ECS::Service
    DependsOn:
    - DbTaskDefinition
    - ClusterCPAssociation
    - ECSCluster
    Properties:
      ServiceName: !Sub db-service-${AWS::StackName}
      Cluster: !Ref ECSCluster
      TaskDefinition: !GetAtt DbTaskDefinition.TaskDefinitionArn
      CapacityProviderStrategy:
      - CapacityProvider: !Ref DbEC2CapacityProvider
        Base: 0
        Weight: 1
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: '20'
      DeploymentConfiguration:
        DeploymentCircuitBreaker:
          Enable: true
          Rollback: false
      LoadBalancers:
      - ContainerName: lambda_db
        ContainerPort: 5432
        TargetGroupArn: !GetAtt DbTargetGroup.TargetGroupArn

  DbLoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    DependsOn:
    - DbTargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${DbLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      Type: network
      Scheme: internet-facing
      Subnets:
      - !Ref PublicSubnet1
      SecurityGroups:
      - !Ref SecurityGroup

  DbTargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name:
        Fn::Sub:
        - ${DbLoadBalancerName}-${StackSuffix}
        - StackSuffix:
            Fn::Select:
            - 1
            - Fn::Split:
              - '-'
              - Fn::Select:
                - 2
                - Fn::Split:
                  - /
                  - !Ref AWS::StackId
      VpcId: !Ref VPC
      Port: 5432
      Protocol: TCP
      HealthCheckProtocol: TCP
      HealthCheckPort: 5432
      TargetType: instance

  DbListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !GetAtt DbLoadBalancer.LoadBalancerArn
      Port: 5432
      Protocol: TCP
      DefaultActions:
      - Type: forward
        TargetGroupArn: !GetAtt DbTargetGroup.TargetGroupArn

  DbECSLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    DependsOn:
    - ECSCluster
    Properties:
      ImageId: !Ref DbECSOptimizedAMI
      InstanceType: !Ref DbECSClusterInstanceType
      IamInstanceProfile: !Ref EcsInstanceProfile
      AssociatePublicIpAddress: true
      SecurityGroups:
      - !Ref SecurityGroup
      UserData: !Base64
        Fn::Sub: |
          #!/bin/bash
          yum install -y aws-cfn-bootstrap
          /opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSLaunchConfiguration
          /opt/aws/bin/cfn-signal -e $? --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSAutoScalingGroup
          exec 2>>/var/log/ecs/ecs-agent-install.log
          set -x
          until curl -s http://localhost:51678/v1/metadata
          do
             sleep 1
          done
          docker plugin install rexray/ebs REXRAY_PREEMPT=true EBS_REGION=${AWS::Region} --grant-all-permissions
          stop ecs
          start ecs
    Metadata:
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              aws-cli: []
              jq: []
              ecs-init: []
          commands:
            01_add_instance_to_cluster:
              command: !Sub echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config
            02_start_ecs_agent:
              command: start ecs
          files:
            /etc/cfn/cfn-hup.conf:
              mode: 256
              owner: root
              group: root
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.DbECSLaunchConfiguration.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource DbECSLaunchConfiguration
          services:
            sysvinit:
              cfn-hup:
                enabled: 'true'
                ensureRunning: 'true'
                files:
                - /etc/cfn/cfn-hup.conf
                - /etc/cfn/hooks.d/cfn-auto-reloader.conf

  DbECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 1
      # LaunchTemplate:
      #   LaunchTemplateId: !Ref DbECSLaunchTemplate
      #   Version: !GetAtt DbECSLaunchTemplate.LatestVersionNumber
      LaunchConfigurationName: !Ref DbECSLaunchConfiguration
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      AvailabilityZones:
      - !Select
        - '0'
        - Fn::GetAZs: !Ref AWS::Region
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-db-instance

  DbEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref DbECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Tech Model ECS
  TechTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub tech-model-task-${AWS::StackName}
      Memory: !Ref TechTaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !Ref TechModelContainerImage
        Cpu: 0
        Essential: true
        EntryPoint:
        - main
        ResourceRequirements:
        - Type: GPU
          Value: '1'
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: NVIDIA_DRIVER_CAPABILITIES
          Value: all
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  TechECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref GpuECSOptimizedAMI
        InstanceType: !Ref TechECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '45'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
            echo ECS_ENABLE_GPU_SUPPORT=true >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  TechECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref TechECSLaunchTemplate
        Version: !GetAtt TechECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-tech-instance

  TechEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref TechECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Cluster part 1 Model ECS
  ClusterPart1TaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub cluster-model-part-1-task-${AWS::StackName}
      Memory: !Ref ClusterPart1TaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !Ref ClusterModelPart1ContainerImage
        Cpu: 0
        Essential: true
        EntryPoint:
        - main
        ResourceRequirements:
        - Type: GPU
          Value: '1'
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        - Name: NVIDIA_DRIVER_CAPABILITIES
          Value: all
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  ClusterPart1ECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref GpuECSOptimizedAMI
        InstanceType: !Ref ClusterPart1ECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '45'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
            echo ECS_ENABLE_GPU_SUPPORT=true >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  ClusterPart1ECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref ClusterPart1ECSLaunchTemplate
        Version: !GetAtt ClusterPart1ECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-cluster-1-instance

  ClusterPart1EC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref ClusterPart1ECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Cluster part 2 Model ECS
  ClusterPart2TaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub cluster-model-part-2-task-${AWS::StackName}
      Memory: !Ref ClusterPart2TaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !Ref ClusterModelPart2ContainerImage
        Cpu: 0
        Essential: true
        EntryPoint:
        - main
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  ClusterPart2ECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        InstanceType: !Ref ClusterPart2ECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  ClusterPart2ECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref ClusterPart2ECSLaunchTemplate
        Version: !GetAtt ClusterPart2ECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-cluster-2-instance

  ClusterPart2EC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref ClusterPart2ECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  # Flow Model ECS
  FlowTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    DependsOn:
    - DbLoadBalancer
    Properties:
      Family: !Sub flow-model-task-${AWS::StackName}
      Memory: !Ref FlowTaskMemory
      NetworkMode: host
      RequiresCompatibilities:
      - EC2
      ExecutionRoleArn: !GetAtt EcsModelExecutionRole.Arn
      TaskRoleArn: !GetAtt EcsModelExecutionRole.Arn
      ContainerDefinitions:
      - Name: model
        Image: !Ref FlowModelContainerImage
        Cpu: 0
        Essential: true
        EntryPoint:
        - main
        Environment:
        - Name: AWS_DEFAULT_REGION
          Value: !Ref AWS::Region
        LogConfiguration:
          LogDriver: awslogs
          Options:
            awslogs-group: !Sub /ecs/${AWS::StackName}-cluster
            awslogs-region: !Ref AWS::Region
            awslogs-stream-prefix: ecs

  FlowECSLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
    - ECSCluster
    Properties:
      LaunchTemplateData:
        ImageId: !Ref CpuECSOptimizedAMI
        InstanceType: !Ref FlowECSClusterInstanceType
        IamInstanceProfile:
          Name: !Ref EcsInstanceProfile
        NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          DeleteOnTermination: true
          Groups:
          - !Ref SecurityGroup
        BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: '30'
        UserData: !Base64
          Fn::Sub:
          - |-
            #!/bin/bash
            echo ECS_CLUSTER=${ClusterName} >> /etc/ecs/ecs.config;
          - ClusterName: !Sub ${AWS::StackName}-cluster

  FlowECSAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn:
    - ECSCluster
    Properties:
      MinSize: !Ref ClusterAutoScalingMinSize
      MaxSize: !Ref ClusterAutoScalingMaxSize
      DesiredCapacity: 0
      LaunchTemplate:
        LaunchTemplateId: !Ref FlowECSLaunchTemplate
        Version: !GetAtt FlowECSLaunchTemplate.LatestVersionNumber
      VPCZoneIdentifier:
      - !Ref PublicSubnet1
      - !Ref PublicSubnet2
      Tags:
      - Key: Name
        PropagateAtLaunch: true
        Value: !Join
        - ' - '
        - - ECS Instance
          - !Sub ${AWS::StackName}-flow-instance

  FlowEC2CapacityProvider:
    Type: AWS::ECS::CapacityProvider
    Properties:
      AutoScalingGroupProvider:
        AutoScalingGroupArn: !Ref FlowECSAutoScalingGroup
        ManagedScaling:
          Status: ENABLED
          TargetCapacity: 100
        ManagedTerminationProtection: DISABLED

  ECSCluster:
    Type: AWS::ECS::Cluster
    DependsOn:
    - EcsInstanceProfile
    Properties:
      ClusterName: !Sub ${AWS::StackName}-cluster
      ClusterSettings:
      - Name: containerInsights
        Value: disabled
      Configuration:
        ExecuteCommandConfiguration:
          Logging: DEFAULT
      ServiceConnectDefaults:
        Namespace: !Sub ${AWS::StackName}-cluster
      Tags: []
  ECSClusterLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /ecs/${AWS::StackName}-cluster

  ClusterCPAssociation:
    Type: AWS::ECS::ClusterCapacityProviderAssociations
    DependsOn:
    - ECSCluster
    Properties:
      Cluster: !Sub ${AWS::StackName}-cluster
      CapacityProviders:
      - !Ref DbEC2CapacityProvider
      - !Ref UIEC2CapacityProvider
      - !Ref TechEC2CapacityProvider
      - !Ref ClusterPart1EC2CapacityProvider
      - !Ref ClusterPart2EC2CapacityProvider
      - !Ref FlowEC2CapacityProvider
      DefaultCapacityProviderStrategy:
      - Base: 0
        Weight: 1
        CapacityProvider: !Ref DbEC2CapacityProvider

  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCidr
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-InternetGateway"

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    DependsOn:
    - VPC
    - InternetGateway
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet1Cidr
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [0, !GetAZs ""]
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSubnet1"

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet2Cidr
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [1, !GetAZs ""]
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSubnet2"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicRouteTable"

  Route:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation1:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref RouteTable

  SubnetRouteTableAssociation2:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref RouteTable

  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Allow HTTP
      VpcId: !Ref VPC
      SecurityGroupIngress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      SecurityGroupEgress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-PublicSecurityGroup"

Outputs:
  LoadBalancerDNSName:
    Description: The DNS name of the load balancer
    Value: !Sub "http://${UILoadBalancer.DNSName}:8000"
